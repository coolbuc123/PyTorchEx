{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "original-diagram",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I feel hungry.\\t나는 배가 고프다.',\n",
       " 'Pytorch is very easy.\\t파이토치는 매우 쉽다.',\n",
       " 'Pytorch is a framework for deep learning.\\t파이토치는 딥러닝을 위한 프레임워크이다.',\n",
       " 'Pytorch is very clear to use.\\t파이토치는 사용하기 매우 직관적이다.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main reference\n",
    "# https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "raw = [\"I feel hungry.\t나는 배가 고프다.\",\n",
    "       \"Pytorch is very easy.\t파이토치는 매우 쉽다.\",\n",
    "       \"Pytorch is a framework for deep learning.\t파이토치는 딥러닝을 위한 프레임워크이다.\",\n",
    "       \"Pytorch is very clear to use.\t파이토치는 사용하기 매우 직관적이다.\"]\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-twins",
   "metadata": {},
   "source": [
    "#### 단어사전 Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "insured-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):                                                                                             # 'hello seq to seq model '\n",
    "        self.vocab2index = {\"<SOS>\": SOS_token, \"<EOS>\": EOS_token}     # {'<SOS>': 0, '<EOS>': 1, 'hello': 2, 'seq': 3, 'to': 4, 'model': 5, '': 6}\n",
    "        self.index2vocab = {SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}     # {0: '<SOS>', 1: '<EOS>', 2: 'hello', 3: 'seq', 4: 'to', 5: 'model', 6: ''}\n",
    "        self.vocab_count = {}                                                                                  # {'hello': 1, 'seq': 2, 'to': 1, 'model': 1, '': 1}\n",
    "        self.n_vocab = len(self.vocab2index)                                                       # 7\n",
    "\n",
    "    def add_vocab(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in self.vocab2index:\n",
    "                self.vocab2index[word] = self.n_vocab\n",
    "                self.vocab_count[word] = 1\n",
    "                self.index2vocab[self.n_vocab] = word\n",
    "                self.n_vocab += 1\n",
    "            else:\n",
    "                self.vocab_count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-intersection",
   "metadata": {},
   "source": [
    "##### 단어사전 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "statutory-tattoo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab2index: \t {'<SOS>': 0, '<EOS>': 1, 'hello': 2, 'seq': 3, 'to': 4, 'model': 5, '': 6}\n",
      "index2vocab: \t {0: '<SOS>', 1: '<EOS>', 2: 'hello', 3: 'seq', 4: 'to', 5: 'model', 6: ''}\n",
      "vocab_count: \t {'hello': 1, 'seq': 2, 'to': 1, 'model': 1, '': 1}\n",
      "n_vocab: \t\t 7\n"
     ]
    }
   ],
   "source": [
    "test_sentence = 'hello seq to seq model '\n",
    "\n",
    "test_vocab = Vocab()\n",
    "test_vocab.add_vocab(test_sentence)\n",
    "print(\"vocab2index: \\t\", test_vocab.vocab2index)\n",
    "print(\"index2vocab: \\t\", test_vocab.index2vocab)\n",
    "print(\"vocab_count: \\t\", test_vocab.vocab_count)\n",
    "print(\"n_vocab: \\t\\t\", test_vocab.n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-acrylic",
   "metadata": {},
   "source": [
    "#### filter_pair : pair(src, tgt)가 모두 Max Length 이하인지 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "raised-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pair(pair, source_max_length, target_max_length):\n",
    "    return len(pair[0].split(\" \")) < source_max_length and len(pair[1].split(\" \")) < target_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "wired-plumbing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 4 sentence pairs\n",
      "['i feel hungry.', '나는 배가 고프다.']\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "pairs = []\n",
    "for line in raw:\n",
    "    pairs.append([s for s in line.strip().lower().split(\"\\t\")])\n",
    "print(\"Read {} sentence pairs\".format(len(pairs)))\n",
    "print(pairs[0])\n",
    "\n",
    "print(filter_pair(pairs[0], 10, 12))\n",
    "print(filter_pair(pairs[0], 3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-malpractice",
   "metadata": {},
   "source": [
    "### 전처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "regulation-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(corpus, source_max_length, target_max_length):\n",
    "\n",
    "    # 문장별로 pair 간단히 만들어줌\n",
    "    print(\"[preprocess] reading corpus...\")\n",
    "    pairs = []\n",
    "    for line in corpus:\n",
    "        pairs.append([s for s in line.strip().lower().split(\"\\t\")])\n",
    "    print(\"[preprocess] Read {} sentence pairs\".format(len(pairs)))\n",
    "\n",
    "    # (src, tgt) 둘다 max length 이하인 경우만 통과\n",
    "    pairs = [pair for pair in pairs if filter_pair(pair, source_max_length, target_max_length)]\n",
    "    print(\"[preprocess] Trimmed to {} sentence pairs\".format(len(pairs)))\n",
    "\n",
    "    # vocab 2개 생성\n",
    "    source_vocab = Vocab() \n",
    "    target_vocab = Vocab()\n",
    "\n",
    "    # src들 → src_vocab에 추가, tgt들 → tgt_vocab에 추가\n",
    "    print(\"[preprocess] Counting words...\")\n",
    "    for pair in pairs:\n",
    "        source_vocab.add_vocab(pair[0])\n",
    "        target_vocab.add_vocab(pair[1])\n",
    "    print(\"[preprocess] source vocab size =\", source_vocab.n_vocab)\n",
    "    print(\"[preprocess] target vocab size =\", target_vocab.n_vocab)\n",
    "\n",
    "    # 리턴, pairs원본, 소스모음vocab, 타겟모음vocab\n",
    "    return pairs, source_vocab, target_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "grave-flashing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] reading corpus...\n",
      "[preprocess] Read 4 sentence pairs\n",
      "[preprocess] Trimmed to 4 sentence pairs\n",
      "[preprocess] Counting words...\n",
      "[preprocess] source vocab size = 17\n",
      "[preprocess] target vocab size = 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([['i feel hungry.', '나는 배가 고프다.'],\n",
       "  ['pytorch is very easy.', '파이토치는 매우 쉽다.'],\n",
       "  ['pytorch is a framework for deep learning.', '파이토치는 딥러닝을 위한 프레임워크이다.'],\n",
       "  ['pytorch is very clear to use.', '파이토치는 사용하기 매우 직관적이다.']],\n",
       " <__main__.Vocab at 0x7f2995468438>,\n",
       " <__main__.Vocab at 0x7f2995468198>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(raw, 10, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "special-avatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        zero_init = Variable(torch.zeros(1, 1, self.hidden_size)).to(device)    \n",
    "        return zero_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "passing-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))                        # 3D → 2D  # tensor([[[-0.1669, ... ,  0.4492]]])  → tensor([[-0.1669, ... ,  0.4492]]) ※ 요소 : 16개(hidden_size)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initSoS(self):\n",
    "        sos_init = Variable(torch.Tensor([[SOS_token]]).long().to(device))\n",
    "        return sos_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "acceptable-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size  # 추가\n",
    "        self.dropout_p   = dropout_p    # 추가\n",
    "        self.max_length = max_length # 추가\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)                   # attn (Linear) Layer추가\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size) # attn_combine (Linear) Layer추가\n",
    "        self.dropout = nn.Dropout(self.dropout_p)                                               # dropout Layer추가\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)                                                   # 주석 처리\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):  # encoder_outputs 추가\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        \n",
    "        ### Attention Decoder 구조 추가\n",
    "        embedded = self.dropout(embedded)\n",
    "        attn_weights = F.softmax( self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1 )\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        ### \n",
    "        \n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))                        # 3D → 2D ( 한겹 벗기기 ) ※ 요소 : 16개(hidden_size)\n",
    "        return output, hidden, attn_weights                                  # attn_weights  리턴 추가\n",
    "    \n",
    "    def initSoS(self):\n",
    "        sos_init = Variable(torch.Tensor([[SOS_token]]).long().to(device))\n",
    "        return sos_init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-transition",
   "metadata": {},
   "source": [
    "#### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "respiratory-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize(vocab, sentence):\n",
    "    indexes = [vocab.vocab2index[word] for word in sentence.split(\" \")]\n",
    "    indexes.append(vocab.vocab2index[\"<EOS>\"])\n",
    "    return torch.Tensor(indexes).long().to(device).view(-1, 1) # 특성1개 모양으로 : [2, 3, 4, 3, 5, 6, 1] → [ [2], [3], [4], [3], [5], [6], [1] ] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-seeker",
   "metadata": {},
   "source": [
    "##### 토큰화 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "alpha-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello seq to seq model \n",
      "{0: '<SOS>', 1: '<EOS>', 2: 'hello', 3: 'seq', 4: 'to', 5: 'model', 6: ''}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [3],\n",
       "        [5],\n",
       "        [6],\n",
       "        [1]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(test_sentence)\n",
    "print(test_vocab.index2vocab)\n",
    "tensorize(test_vocab, test_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-union",
   "metadata": {},
   "source": [
    "##### n_iter만큼 랜덤 묶음 batch만큼 Pair 구성 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "recovered-dubai",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['pytorch is very clear to use.', '파이토치는 사용하기 매우 직관적이다.'],\n",
       " ['i feel hungry.', '나는 배가 고프다.'],\n",
       " ['pytorch is very easy.', '파이토치는 매우 쉽다.'],\n",
       " ['i feel hungry.', '나는 배가 고프다.'],\n",
       " ['pytorch is very clear to use.', '파이토치는 사용하기 매우 직관적이다.'],\n",
       " ['pytorch is very easy.', '파이토치는 매우 쉽다.'],\n",
       " ['pytorch is very clear to use.', '파이토치는 사용하기 매우 직관적이다.'],\n",
       " ['pytorch is very clear to use.', '파이토치는 사용하기 매우 직관적이다.'],\n",
       " ['i feel hungry.', '나는 배가 고프다.'],\n",
       " ['i feel hungry.', '나는 배가 고프다.']]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(pairs) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-affiliation",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "gentle-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(pairs, source_vocab, target_vocab, encoder, decoder, n_iter, print_every=1000, learning_rate=0.01, max_length=MAX_LENGTH):\n",
    "    loss_total = 0\n",
    "\n",
    "    # opt, loss  설정\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # 배치단위로 src, tgt를 토큰화\n",
    "    training_batch = [random.choice(pairs) for _ in range(n_iter)] # n_iter만큼 랜덤묶음 batch 구성해줌\n",
    "    training_source = [tensorize(source_vocab, pair[0]) for pair in training_batch]\n",
    "    training_target = [tensorize(target_vocab, pair[1]) for pair in training_batch]\n",
    "\n",
    "    # batch 단위로 보면 될 듯(?)\n",
    "    for i in range(1, n_iter + 1):\n",
    "        source_tensor = training_source[i - 1] # idx를 1부터 시작해서 1씩 빼준 것 뿐\n",
    "        target_tensor = training_target[i - 1]   \n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        source_length = source_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        loss = 0\n",
    "        \n",
    "        # 최초 hidden은 0으로 채워진 모양으로 준비\n",
    "        encoder_hidden=encoder.initHidden()        \n",
    "        \n",
    "        #attn 추가\n",
    "        encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size).to(device))\n",
    "        \n",
    "        # source문장 돌면서 토큰단위로 Encoding\n",
    "        for ei in range(source_length):                                 \n",
    "            encoder_output, encoder_hidden = encoder(source_tensor[ei], encoder_hidden) # encoder_output 활용 시작         \n",
    "            encoder_outputs[ei] = encoder_output[0][0]             \n",
    "    \n",
    "        # Decoding 준비\n",
    "        decoder_input = decoder.initSoS()\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        # target문장 돌면서 Decoder Loss →  훈련(back, step)\n",
    "        for di in range(target_length):                                                          # 시작 [ output의 마지막 hidden →연결→ decoder의 hidden ]                                                \n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)             # enc_out 입력, weight 출력 부분 추가\n",
    "            loss += criterion(decoder_output, target_tensor[di])              # 이후 결과로 나온 decoder_hidden→ 다음 hidden으로 계속 활용                \n",
    "            decoder_input = target_tensor[di]  # teacher forcing\n",
    "            \n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        loss_iter = loss.item() / target_length\n",
    "        loss_total += loss_iter\n",
    "\n",
    "        if i % print_every == 0:\n",
    "            loss_avg = loss_total / print_every\n",
    "            loss_total = 0\n",
    "            print(\"[{} - {}%] loss = {:05.4f}\".format(i, i / n_iter * 100, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "israeli-hello",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  # RuntimeError: legacy constructor expects device type: cpubut device type: cuda was passed\n",
    "#  torch.Tensor([0], device=device)\n",
    "\n",
    "torch.Tensor([0]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-feeling",
   "metadata": {},
   "source": [
    "### 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "excellent-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "polished-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(pairs, source_vocab, target_vocab, encoder, decoder, target_max_length, max_length=MAX_LENGTH):\n",
    "    for pair in pairs:\n",
    "        print(\">\", pair[0])\n",
    "        print(\"=\", pair[1])\n",
    "\n",
    "        source_tensor = tensorize(source_vocab, pair[0])\n",
    "        source_length = source_tensor.size()[0]\n",
    "        \n",
    "        # Encoding 준비\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        #attn 추가\n",
    "        encoder_outputs = Variable(torch.zeros(max_length, encoder.hidden_size).to(device))\n",
    "\n",
    "        # source문장 돌면서 Encoding     \n",
    "        for ei in range(source_length):       \n",
    "            encoder_output, encoder_hidden = encoder(source_tensor[ei], encoder_hidden)             #  encoder_output 이제 attnDecoder에 활용 \n",
    "            encoder_outputs[ei] = encoder_outputs[ei] + encoder_output[0][0]                \n",
    "\n",
    "        # Decoding 준비\n",
    "        decoder_input = decoder.initSoS()   \n",
    "        decoder_hidden = encoder_hidden                                                      \n",
    "        \n",
    "        # Decoding\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "            \n",
    "        for di in range(target_max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)             # enc_out 입력, weight 출력 부분 추가\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            _, top_index = decoder_output.data.topk(1)\n",
    "            \n",
    "            # 디코더 끝이면(<EOS>) 종료\n",
    "            if top_index.item() == EOS_token:\n",
    "                decoded_words.append(\"<EOS>\")\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(target_vocab.index2vocab[top_index.item()])\n",
    "\n",
    "            decoder_input = top_index.squeeze().detach()\n",
    "\n",
    "        predict_words = decoded_words\n",
    "        predict_sentence = \" \".join(predict_words)\n",
    "        print(\"<\", predict_sentence)\n",
    "        print(\"\")\n",
    "        print(\"decoder_attention.shape : \", decoder_attention.shape)\n",
    "        print(\"decoder_attention : \", decoder_attention[:di+1].detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-gibraltar",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "solid-internet",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] reading corpus...\n",
      "[preprocess] Read 4 sentence pairs\n",
      "[preprocess] Trimmed to 4 sentence pairs\n",
      "[preprocess] Counting words...\n",
      "[preprocess] source vocab size = 17\n",
      "[preprocess] target vocab size = 13\n",
      "\n",
      "▶Pair모양 확인(random) :  ['pytorch is very clear to use.', '파이토치는 사용하기 매우 직관적이다.']\n",
      "\n",
      "▶인코더/디코더 선언\n",
      "\n",
      "▶훈련시작 >>>>>>>>>>> train()\n",
      "\n",
      "▶전체 평가 >>>>>>>>>>> evaluate()\n",
      "> i feel hungry.\n",
      "= 나는 배가 고프다.\n",
      "< 파이토치는 <EOS>\n",
      "\n",
      "decoder_attention.shape :  torch.Size([1, 50])\n",
      "decoder_attention :  [[0.0172065  0.01651089 0.02229207 0.03606767 0.0341313  0.01957956\n",
      "  0.0107967  0.02781163 0.02049082 0.01272375 0.02261607 0.02802904\n",
      "  0.02223894 0.04455445 0.01681549 0.01123761 0.01876548 0.01656967\n",
      "  0.00764475 0.01541826 0.02089866 0.00972778 0.00598188 0.03831386\n",
      "  0.01299775 0.02506229 0.01614982 0.02002547 0.01354413 0.03525115\n",
      "  0.00677523 0.02950615 0.0119466  0.01831561 0.01985232 0.00960566\n",
      "  0.02905442 0.01509971 0.0069574  0.01069265 0.02148312 0.02966379\n",
      "  0.02411496 0.0152854  0.01978196 0.03574554 0.03284624 0.00806342\n",
      "  0.01393347 0.0218228 ]]\n",
      "> pytorch is very easy.\n",
      "= 파이토치는 매우 쉽다.\n",
      "< 파이토치는 <EOS>\n",
      "\n",
      "decoder_attention.shape :  torch.Size([1, 50])\n",
      "decoder_attention :  [[0.01321474 0.01888276 0.02198259 0.02438833 0.0244161  0.02461106\n",
      "  0.00710611 0.03615949 0.01222471 0.0099989  0.01820353 0.01886658\n",
      "  0.01773805 0.05849556 0.01537154 0.0134824  0.0242552  0.01286901\n",
      "  0.0071059  0.01254988 0.01542978 0.01078539 0.00513758 0.03585482\n",
      "  0.01445784 0.02434882 0.01946006 0.02927219 0.01263232 0.03045425\n",
      "  0.00842914 0.03185335 0.01609403 0.01670158 0.02424071 0.00853359\n",
      "  0.03433841 0.01379584 0.00932595 0.00717365 0.02069438 0.03330021\n",
      "  0.02629761 0.01786195 0.01586548 0.03305425 0.04531413 0.00871353\n",
      "  0.01453749 0.02411915]]\n",
      "> pytorch is a framework for deep learning.\n",
      "= 파이토치는 딥러닝을 위한 프레임워크이다.\n",
      "< 파이토치는 <EOS>\n",
      "\n",
      "decoder_attention.shape :  torch.Size([1, 50])\n",
      "decoder_attention :  [[0.01322598 0.01595965 0.02182211 0.02389552 0.01970406 0.02466416\n",
      "  0.00779538 0.03573771 0.01144578 0.0095614  0.02037624 0.01608567\n",
      "  0.0212379  0.05539194 0.01529296 0.01468973 0.02643811 0.01626485\n",
      "  0.00831748 0.01200188 0.01803582 0.01068721 0.00572919 0.03249731\n",
      "  0.01317999 0.02275035 0.02284222 0.02474878 0.01150665 0.02806652\n",
      "  0.00923367 0.02631464 0.01283759 0.01950469 0.02024603 0.01000335\n",
      "  0.0360071  0.01425031 0.01003333 0.0088502  0.02221286 0.03071727\n",
      "  0.03172273 0.01620418 0.01784602 0.02705112 0.05290717 0.00887528\n",
      "  0.01528656 0.02994331]]\n",
      "> pytorch is very clear to use.\n",
      "= 파이토치는 사용하기 매우 직관적이다.\n",
      "< 파이토치는 <EOS>\n",
      "\n",
      "decoder_attention.shape :  torch.Size([1, 50])\n",
      "decoder_attention :  [[0.01434984 0.02516554 0.01788559 0.02191224 0.02325273 0.02354855\n",
      "  0.00843391 0.02283434 0.01104655 0.00837302 0.01828038 0.02148982\n",
      "  0.02092069 0.04103812 0.01746511 0.02046774 0.02607088 0.01370322\n",
      "  0.00682919 0.01587306 0.0192344  0.01011272 0.00665618 0.03845721\n",
      "  0.0143707  0.01892725 0.02823798 0.02127923 0.0096121  0.02909945\n",
      "  0.01483333 0.02721125 0.01869229 0.02187131 0.0291995  0.0107298\n",
      "  0.03267806 0.01426864 0.00983743 0.00959352 0.02076881 0.02817009\n",
      "  0.02826433 0.0252729  0.01569824 0.0281657  0.02814343 0.01044664\n",
      "  0.01866226 0.03256475]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SOURCE_MAX_LENGTH = 10\n",
    "TARGET_MAX_LENGTH = 12\n",
    "\n",
    "load_pairs, load_source_vocab, load_target_vocab = preprocess(raw, SOURCE_MAX_LENGTH, TARGET_MAX_LENGTH)\n",
    "print(\"\\n▶Pair모양 확인(random) : \", random.choice(load_pairs))\n",
    "\n",
    "print(\"\\n▶인코더/디코더 선언\")\n",
    "enc_hidden_size = 16\n",
    "dec_hidden_size = enc_hidden_size\n",
    "\n",
    "enc = EncoderRNN(load_source_vocab.n_vocab, enc_hidden_size).to(device)\n",
    "# dec = DecoderRNN(dec_hidden_size, load_target_vocab.n_vocab).to(device)\n",
    "attn_dec = AttnDecoderRNN(dec_hidden_size, load_target_vocab.n_vocab, dropout_p=0.1).to(device)\n",
    "\n",
    "print(\"\\n▶훈련시작 >>>>>>>>>>> train()\")\n",
    "train(load_pairs, load_source_vocab, load_target_vocab, enc, attn_dec, 100, print_every=1000)\n",
    "\n",
    "print(\"\\n▶전체 평가 >>>>>>>>>>> evaluate()\")\n",
    "evaluate(load_pairs, load_source_vocab, load_target_vocab, enc, attn_dec, TARGET_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-survival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-documentation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python My Env",
   "language": "python",
   "name": "python-my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
