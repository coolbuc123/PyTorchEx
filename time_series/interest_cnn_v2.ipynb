{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 낮음  \n",
    "제대로 학습이 되지 않음  \n",
    "학습을 지속할 수록 감소 쪽으로 더 몰리거나 왔다가다 하는 경향\n",
    "stride 커지면 관찰 기간 더 길어지면서 감소 쪽으로 더 몰리는 것도 같음\n",
    "dilation 도 그럴 것 같지만 특별히 더 몰리지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "#lib_s = '/home/adminuser/public/libs/pytorch_examples/basic_networks'\n",
    "lib_s = '/home/bwlee/work/codes/examples/basic_networks'\n",
    "sys.path.append(lib_s)\n",
    "import generate_model as nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make input data\n",
    "interest.csv has only closed price.   \n",
    "On the other hand, zipline needs OLHCV(?) format for each index  \n",
    "make dummy columns and put 'close' value to them  \n",
    "make csv file for each index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s0 = 'interest.csv'\n",
    "df0 = pd.read_csv(f_s0, index_col='date')\n",
    "df0 = df0.iloc[::-1] # 과거에서 현재로 정렬순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized input, norm val, tgt_date, tgt_val, tgt_diff_ratio, \n",
    "class Environ():\n",
    "    def __init__(self):\n",
    "        self.indices = ['trea3', 'trea5', 'trea10']\n",
    "        #self.tgt_index = 'trea10'\n",
    "        self.tgt_index = 'trea3'\n",
    "        # input window size\n",
    "        self.input_size = 15\n",
    "        # prediction is made after algo.pred_period from the present\n",
    "        self.pred_period = 5\n",
    "\n",
    "env = Environ() # setting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff0 = df0[:200] # test\n",
    "dff0 = df0\n",
    "dff = dff0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_periods = [int(env.input_size/4*x) for x in [1, 2, 3]]\n",
    "for diff_period in diff_periods:\n",
    "    diff = dff0.diff(periods=diff_period)\n",
    "    ii = diff_period\n",
    "    columns = ['d%d_diff_trea3'%(ii), 'd%d_diff_trea5'%(ii), 'd%d_diff_trea10'%(ii)]\n",
    "    diff.columns = columns\n",
    "    dff = dff.merge(diff, how='inner', on='date')\n",
    "dff['diff35'] = dff['trea3'] - dff['trea5']\n",
    "dff['diff310'] = dff['trea3'] - dff['trea10']\n",
    "dff['diff510'] = dff['trea5'] - dff['trea10']\n",
    "\n",
    "dff = dff.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data without time confusion\n",
    "Seperate two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_data(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read past data and push them to mkt_data\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        if ii_days+1 < env.input_size:\n",
    "            continue\n",
    "        elif ii_days+1 == env.input_size:\n",
    "            env.start_date = data.iloc[ii_days].name\n",
    "        \n",
    "        predicting_date = data.iloc[ii_days].name\n",
    "        # historical data from today to env.input_size behind days\n",
    "        history = data.iloc[ii_days+1-env.input_size:ii_days+1]\n",
    "        \n",
    "        ## normalize\n",
    "        \"\"\"\n",
    "        #  : look back 1 year\n",
    "        lookback_step = 250\n",
    "        lookback_step = env.input_size\n",
    "        if ii_days+1 < lookback_step:\n",
    "            cumul = data.iloc[:ii_days+1]\n",
    "        else:\n",
    "            cumul = data.iloc[ii_days+1-lookback_step:ii_days+1]\n",
    "        \n",
    "        mean_val = cumul.mean() \n",
    "        range_val = 2*cumul.std()\n",
    "        \"\"\"\n",
    "        \n",
    "        mean_val = history.mean() \n",
    "        range_val = history.max() - history.min()\n",
    "        \n",
    "        norm_history = (history-mean_val)/range_val\n",
    "        mkt_data[predicting_date] = [norm_history, mean_val, range_val, \n",
    "                                     None, None, None, None, None]\n",
    "        \n",
    "            \n",
    "def push_tgt(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read future data and push them to mkt_data\n",
    "    future_prediction_date is a future date when model want to predict\n",
    "    past_predicting_date is a past date when model made a prediction with given information\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        ii_predicting = ii_days - env.pred_period\n",
    "        predicting_date = data.iloc[ii_predicting].name\n",
    "        if ii_predicting < 0 or predicting_date < env.start_date:\n",
    "            continue\n",
    "        \n",
    "        past_history = mkt_data[predicting_date][0]\n",
    "        mean_val = mkt_data[predicting_date][1]\n",
    "        range_val = mkt_data[predicting_date][2]\n",
    "        \n",
    "        past_price = (past_history.iloc[-1]*range_val+mean_val)[env.tgt_index] # 49.6\n",
    "        \n",
    "        #past_price = (past_history.iloc[-5]*range_val+mean_val)[env.tgt_index] # 56.3\n",
    "        \n",
    "        #mkt_data[predicting_date][0] = past_history[:-4]\n",
    "        \n",
    "        #past_price = (past_history.iloc[-3]*range_val+mean_val)[env.tgt_index] # 53.0, # 53.0\n",
    "        #past_price = (past_history.iloc[-13]*range_val+mean_val)[env.tgt_index] # 67.4\n",
    "        #past_price = (past_history*range_val+mean_val).mean()[env.tgt_index] # 58.4\n",
    "        #past_price = (past_history*range_val+mean_val)[:-3].mean()[env.tgt_index] # 63.2\n",
    "        #past_price = (past_history*range_val+mean_val)[:-5].mean()[env.tgt_index] # 63.7\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10]*range_val+mean_val)[env.tgt_index] # 66.4\n",
    "\n",
    "        #past_price = (past_history.iloc[-5:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        #past_price = (past_history.iloc[-3:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10:].mean()*range_val+mean_val)[env.tgt_index] # 55.9\n",
    "        #past_price = (past_history.iloc[-10:-5].mean()*range_val+mean_val)[env.tgt_index] # 60.4\n",
    "        \n",
    "        \n",
    "        prediction_date = data.iloc[ii_days].name\n",
    "        tgt_price = data.iloc[ii_days][env.tgt_index]\n",
    "        #tgt_price = (data.iloc[ii_days-1:ii_days+2].mean())[env.tgt_index]\n",
    "        \n",
    "        tgt_ratio = (tgt_price - past_price)/past_price*10\n",
    "        \n",
    "        mkt_data[predicting_date][3] = prediction_date\n",
    "        mkt_data[predicting_date][4] = np.single(tgt_price)\n",
    "        mkt_data[predicting_date][5] = np.single(tgt_ratio)\n",
    "        \n",
    "        mkt_data[predicting_date][6] = np.single(past_price)\n",
    "        mkt_data[predicting_date][7] = predicting_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkt_data = {} # object to store data for learning\n",
    "push_data(env, dff, mkt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check data are pushed correctly\n",
    "* no dates should be ahead of key date\n",
    "* prediction info are not pushed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000-11-18', '2000-11-20', '2000-11-21']\n"
     ]
    }
   ],
   "source": [
    "print(list(mkt_data.keys())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[               trea3     trea5    trea10  d3_diff_trea3  d3_diff_trea5  \\\n",
       " date                                                                     \n",
       " 2000-11-02  0.455471  0.457619  0.468205       0.156444       0.139912   \n",
       " 2000-11-03  0.486005  0.493333  0.498974       0.129778       0.126754   \n",
       " 2000-11-04  0.486005  0.493333  0.498974       0.169778       0.172807   \n",
       " 2000-11-06  0.516539  0.529048  0.537436       0.176444       0.205702   \n",
       " 2000-11-07  0.516539  0.536190  0.537436       0.149778       0.179386   \n",
       " 2000-11-08  0.402036  0.407619  0.414359       0.049778       0.060965   \n",
       " 2000-11-09 -0.040712 -0.070952 -0.062564      -0.363556      -0.412719   \n",
       " 2000-11-10 -0.391858 -0.363810 -0.324103      -0.670222      -0.689035   \n",
       " 2000-11-11 -0.391858 -0.363810 -0.324103      -0.570222      -0.570614   \n",
       " 2000-11-13 -0.483461 -0.456667 -0.462564      -0.263556      -0.215351   \n",
       " 2000-11-14 -0.452926 -0.463810 -0.462564       0.069778       0.047807   \n",
       " 2000-11-15 -0.315522 -0.342381 -0.347179       0.189778       0.159649   \n",
       " 2000-11-16 -0.246819 -0.270952 -0.308718       0.329778       0.310965   \n",
       " 2000-11-17 -0.269720 -0.292381 -0.331795       0.283111       0.297807   \n",
       " 2000-11-18 -0.269720 -0.292381 -0.331795       0.163111       0.185965   \n",
       " \n",
       "             d3_diff_trea10  d7_diff_trea3  d7_diff_trea5  d7_diff_trea10  \\\n",
       " date                                                                       \n",
       " 2000-11-02        0.120707       0.152914       0.204889        0.121303   \n",
       " 2000-11-03        0.120707       0.229837       0.278222        0.279198   \n",
       " 2000-11-04        0.166162       0.411655       0.404889        0.414536   \n",
       " 2000-11-06        0.234343       0.439627       0.438222        0.452130   \n",
       " 2000-11-07        0.204040       0.453613       0.451556        0.452130   \n",
       " 2000-11-08        0.082828       0.292774       0.284889        0.301754   \n",
       " 2000-11-09       -0.424747      -0.070862      -0.115111       -0.119298   \n",
       " 2000-11-10       -0.682323      -0.413520      -0.388444       -0.344862   \n",
       " 2000-11-11       -0.561111      -0.441492      -0.421778       -0.374937   \n",
       " 2000-11-13       -0.227778      -0.525408      -0.508444       -0.510276   \n",
       " 2000-11-14        0.029798      -0.525408      -0.548444       -0.547870   \n",
       " 2000-11-15        0.143434      -0.399534      -0.441778       -0.435088   \n",
       " 2000-11-16        0.317677      -0.231702      -0.255111       -0.277193   \n",
       " 2000-11-17        0.294949       0.152914       0.171556        0.166416   \n",
       " 2000-11-18        0.181313       0.474592       0.444889        0.422055   \n",
       " \n",
       "             d11_diff_trea3  d11_diff_trea5  d11_diff_trea10    diff35  \\\n",
       " date                                                                    \n",
       " 2000-11-02        0.322436        0.352778             0.06 -0.314286   \n",
       " 2000-11-03        0.380128        0.394444             0.10 -0.385714   \n",
       " 2000-11-04        0.399359        0.427778             0.36 -0.385714   \n",
       " 2000-11-06        0.437821        0.444444             0.46 -0.457143   \n",
       " 2000-11-07        0.495513        0.527778             0.54 -0.528571   \n",
       " 2000-11-08        0.418590        0.427778             0.55 -0.314286   \n",
       " 2000-11-09        0.110897        0.027778             0.11  0.328571   \n",
       " 2000-11-10       -0.331410       -0.313889            -0.23 -0.028571   \n",
       " 2000-11-11       -0.312179       -0.305556            -0.23 -0.028571   \n",
       " 2000-11-13       -0.504487       -0.472222            -0.45  0.042857   \n",
       " 2000-11-14       -0.408333       -0.422222            -0.39  0.400000   \n",
       " 2000-11-15       -0.264103       -0.280556            -0.20  0.471429   \n",
       " 2000-11-16       -0.216026       -0.238889            -0.19  0.400000   \n",
       " 2000-11-17       -0.244872       -0.263889            -0.22  0.400000   \n",
       " 2000-11-18       -0.283333       -0.305556            -0.27  0.400000   \n",
       " \n",
       "              diff310   diff510  \n",
       " date                            \n",
       " 2000-11-02 -0.070588  0.228571  \n",
       " 2000-11-03 -0.070588  0.300000  \n",
       " 2000-11-04 -0.070588  0.300000  \n",
       " 2000-11-06 -0.129412  0.300000  \n",
       " 2000-11-07 -0.129412  0.371429  \n",
       " 2000-11-08 -0.070588  0.228571  \n",
       " 2000-11-09  0.164706 -0.128571  \n",
       " 2000-11-10 -0.541176 -0.628571  \n",
       " 2000-11-11 -0.541176 -0.628571  \n",
       " 2000-11-13 -0.188235 -0.271429  \n",
       " 2000-11-14  0.047059 -0.342857  \n",
       " 2000-11-15  0.223529 -0.200000  \n",
       " 2000-11-16  0.458824  0.157143  \n",
       " 2000-11-17  0.458824  0.157143  \n",
       " 2000-11-18  0.458824  0.157143  ,\n",
       " trea3              7.451667\n",
       " trea5              7.629667\n",
       " trea10             7.905667\n",
       " d3_diff_trea3     -0.092333\n",
       " d3_diff_trea5     -0.106333\n",
       " d3_diff_trea10    -0.109667\n",
       " d7_diff_trea3     -0.259333\n",
       " d7_diff_trea5     -0.283667\n",
       " d7_diff_trea10    -0.285667\n",
       " d11_diff_trea3    -0.367667\n",
       " d11_diff_trea5    -0.391667\n",
       " d11_diff_trea10   -0.430000\n",
       " diff35            -0.178000\n",
       " diff310           -0.454000\n",
       " diff510           -0.276000\n",
       " dtype: float64,\n",
       " trea3              0.655\n",
       " trea5              0.700\n",
       " trea10             0.650\n",
       " d3_diff_trea3      0.750\n",
       " d3_diff_trea5      0.760\n",
       " d3_diff_trea10     0.660\n",
       " d7_diff_trea3      0.715\n",
       " d7_diff_trea5      0.750\n",
       " d7_diff_trea10     0.665\n",
       " d11_diff_trea3     0.520\n",
       " d11_diff_trea5     0.600\n",
       " d11_diff_trea10    0.500\n",
       " diff35             0.070\n",
       " diff310            0.085\n",
       " diff510            0.070\n",
       " dtype: float64,\n",
       " '2000-11-24',\n",
       " 7.35,\n",
       " 0.10309278,\n",
       " 7.275,\n",
       " '2000-11-18']"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkt_data[list(mkt_data.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-11-06'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_tgt(env, dff, mkt_data)\n",
    "mkt_data = {key: val for key, val in mkt_data.items() if (val[-1] is not None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHECK tgt push is correctly made\n",
    "end value of '2000-11-23' * norm == prediction value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-10-26  0.300000  0.272099  0.293963\n",
      "2000-10-27  0.105970  0.131358  0.152231\n",
      "2000-10-28  0.105970  0.131358  0.152231\n",
      "2000-10-30  0.091045  0.123951  0.152231\n",
      "2000-10-31  0.150746  0.175802  0.183727\n",
      "2000-11-01  0.105970  0.123951  0.136483\n",
      "2000-11-02  0.128358  0.123951  0.104987\n",
      "2000-11-03  0.158209  0.160988  0.136483\n",
      "2000-11-04  0.158209  0.160988  0.136483\n",
      "2000-11-06  0.188060  0.198025  0.175853\n",
      "2000-11-07  0.188060  0.205432  0.175853\n",
      "2000-11-08  0.076119  0.072099  0.049869\n",
      "2000-11-09 -0.356716 -0.424198 -0.438320\n",
      "2000-11-10 -0.700000 -0.727901 -0.706037\n",
      "2000-11-11 -0.700000 -0.727901 -0.706037, trea3     7.664000\n",
      "trea5     7.866333\n",
      "trea10    8.143333\n",
      "dtype: float64, trea3     0.670\n",
      "trea5     0.675\n",
      "trea10    0.635\n",
      "dtype: float64, '2000-12-05', 7.07, -1.0108074]\n",
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-11-07  0.681934  0.687143  0.711795\n",
      "2000-11-08  0.567430  0.558571  0.588718\n",
      "2000-11-09  0.124682  0.080000  0.111795\n",
      "2000-11-10 -0.226463 -0.212857 -0.149744\n",
      "2000-11-11 -0.226463 -0.212857 -0.149744\n",
      "2000-11-13 -0.318066 -0.305714 -0.288205\n",
      "2000-11-14 -0.287532 -0.312857 -0.288205\n",
      "2000-11-15 -0.150127 -0.191429 -0.172821\n",
      "2000-11-16 -0.081425 -0.120000 -0.134359\n",
      "2000-11-17 -0.104326 -0.141429 -0.157436\n",
      "2000-11-18 -0.104326 -0.141429 -0.157436\n",
      "2000-11-20 -0.073791 -0.070000 -0.095897\n",
      "2000-11-21  0.117048  0.151429  0.104103\n",
      "2000-11-22  0.017812  0.072857  0.011795\n",
      "2000-11-23  0.063613  0.158571  0.065641, trea3     7.343333\n",
      "trea5     7.524000\n",
      "trea10    7.792333\n",
      "dtype: float64, trea3     0.655\n",
      "trea5     0.700\n",
      "trea10    0.650\n",
      "dtype: float64, '2000-12-16', 7.11, -0.9541985]\n"
     ]
    }
   ],
   "source": [
    "print(mkt_data['2000-11-11'])\n",
    "print(mkt_data['2000-11-23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for Conv1d\n",
    "In RNN, inputs are given as (time, indices)  \n",
    "In Conv1d, indices are given as channel, which need to (channel: indices, features: time)  \n",
    "(batch, channel=indices, data_size=seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_data(Dataset):\n",
    "    def __init__(self, in_data0, dates, addi=None):\n",
    "        \"\"\"\n",
    "        :param in_data: dict of (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        :param addi: (optional) additional non-sequential elements that can be added to in_data\n",
    "        \"\"\"\n",
    "        in_data = cp.deepcopy(in_data0)\n",
    "        if addi is not None:\n",
    "            assert list(in_data.keys()) == list(addi.keys())\n",
    "            \n",
    "        # change dataframe to float numpy array\n",
    "        for key, val in in_data.items():\n",
    "            # transform in_data into Tensor\n",
    "            if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "                #print('shape', (in_data[key][0].to_numpy()).shape, (in_data[key][1].to_numpy()).shape)\n",
    "                in_data[key][0] = torch.tensor(in_data[key][0].to_numpy(np.single))\n",
    "                in_data[key][1] = torch.tensor(in_data[key][1].to_numpy(np.single))\n",
    "                in_data[key][2] = torch.tensor(in_data[key][2].to_numpy(np.single))\n",
    "            \n",
    "            if addi is not None:\n",
    "                in_data[key].append(addi[key])\n",
    "            else:\n",
    "                # None is not accept in Dataloader, so give null value\n",
    "                in_data[key].append('NULL')\n",
    "            \n",
    "        self.in_data = in_data\n",
    "        self.i2dates = {}\n",
    "        for ii, date1 in enumerate(dates):\n",
    "             self.i2dates[ii] = date1    \n",
    "        \n",
    "    def __getitem__(self, ii_date):\n",
    "        \"\"\"\n",
    "        :return: (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        predicting_date = self.i2dates[ii_date]\n",
    "        return self.in_data[predicting_date]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.in_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        for key, val in self.in_data.items():\n",
    "            self.in_data[key][0] = torch.transpose(val[0], 0, 1)\n",
    "            self.in_data[key][1] = val[1].view(-1,1)\n",
    "            self.in_data[key][2] = val[2].view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase data that do not have prediction target\n",
    "traintest_dates = sorted(list(mkt_data.keys()))\n",
    "n_train = len(traintest_dates)*8//10\n",
    "n_test = len(traintest_dates) - n_train\n",
    "train_dates = traintest_dates[:n_train]\n",
    "test_dates = traintest_dates[n_train:]\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for key, val in mkt_data.items():\n",
    "    if key in train_dates:\n",
    "        train_data[key] = val\n",
    "    elif key in test_dates:\n",
    "        test_data[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            trea3  trea5    trea10\n",
       " date                              \n",
       " 2020-12-24 -0.296  0.180 -0.375758\n",
       " 2020-12-28 -0.196 -0.145 -0.648485\n",
       " 2020-12-29  0.264  0.230 -0.284848\n",
       " 2020-12-30  0.064  0.430  0.169697\n",
       " 2020-12-31  0.064  0.430  0.169697\n",
       " 2021-01-04 -0.236  0.180  0.351515\n",
       " 2021-01-05 -0.596 -0.570 -0.321212\n",
       " 2021-01-06 -0.136  0.105  0.351515\n",
       " 2021-01-07  0.064  0.105  0.133333\n",
       " 2021-01-08  0.204  0.080  0.133333\n",
       " 2021-01-11 -0.036 -0.570 -0.503030\n",
       " 2021-01-12  0.204 -0.170  0.078788\n",
       " 2021-01-13  0.164 -0.270  0.078788\n",
       " 2021-01-14  0.404  0.180  0.351515\n",
       " 2021-01-15  0.064 -0.195  0.315152,\n",
       " trea3     0.966800\n",
       " trea5     1.317800\n",
       " trea10    1.712667\n",
       " dtype: float64,\n",
       " trea3     0.050\n",
       " trea5     0.040\n",
       " trea10    0.055\n",
       " dtype: float64,\n",
       " '2021-01-22',\n",
       " 0.99,\n",
       " 0.20618556,\n",
       " 0.97,\n",
       " '2021-01-15']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['2021-01-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## this is an example, I already add differences\n",
    "def get_addi(in_data):\n",
    "    #print(in_data)\n",
    "    ret = {}\n",
    "    for key, val in in_data.items():\n",
    "        if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "            history_val = torch.tensor(val[0][['trea3','trea5', 'trea10']].to_numpy(np.single))\n",
    "        else:\n",
    "            history_val = val[0][:,:3] # only use three elements : ['trea3','trea5', 'trea10']\n",
    "        addi1 = history_val[-1] - history_val[-5]\n",
    "        addi2 = history_val[-1] - history_val[-7]\n",
    "        addi3 = history_val[-1] - history_val[-10]\n",
    "        addi4 = history_val[-1] - history_val[-3]\n",
    "        addi5 = history_val[-3] - history_val[-7]\n",
    "        addi6 = history_val[-3] - history_val[-10]\n",
    "        addi7 = history_val[-5] - history_val[-7]\n",
    "        addi8 = history_val[-5] - history_val[-10]\n",
    "        addi = torch.cat((addi1, addi2, addi3, addi4, addi5, addi6, addi7, addi8), dim=-1)    \n",
    "        ret[key] = addi\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CNN needs transposed data\n",
    "batch_size = 64\n",
    "\"\"\" # no additional\n",
    "train_dset = Index_data(train_data, train_dates)\n",
    "test_dset = Index_data(test_data, test_dates)\n",
    "\"\"\"\n",
    "train_dset = Index_data(train_data, train_dates, get_addi(train_data))\n",
    "test_dset = Index_data(test_data, test_dates, get_addi(test_data))\n",
    "train_dset.transpose()\n",
    "test_dset.transpose()\n",
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ii in range(len(train_dset)):\n",
    "    if type(train_dset[ii][-1]) is not torch.Tensor:\n",
    "        print(train_dset[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor(nets.Net):\n",
    "    \"\"\"\n",
    "    1D CNN is constructed.\n",
    "    You can constructed with 2D CNN\n",
    "    \"\"\"\n",
    "    def __init__(self, net=None, downnet=None, dropout=0.35,\n",
    "                 loss=None, optimizer=None, device='cuda:2',\n",
    "                 dim_hiddens=[100,200,300,50], len_seq = 15, size_kernels=[3,4,5], \n",
    "                 hidden_out=None, down_hiddens=[1, 1], add_down=-1,\n",
    "                 dilations=None, pads=None, strides=None):\n",
    "        \"\"\"\n",
    "        net is consist of [embed, net, downnet]\n",
    "        dilation will be treated in another class\n",
    "        initial seq_width, which corresponds to kernel_size, is 15(time window size)\n",
    "        :param downnet: define downstream job\n",
    "        :down_hiddens: multiply out_dimension of convolution\n",
    "        :dim_hiddens: size of features\n",
    "        \"\"\"\n",
    "        assert len(dim_hiddens)-1 == len(size_kernels)\n",
    "        if dilations is None:\n",
    "            dilations = [1 for _ in range(len(dim_hiddens))]\n",
    "        if pads is None:\n",
    "            pads = [0 for _ in range(len(dim_hiddens))]\n",
    "        if strides is None:\n",
    "            strides = [1 for _ in range(len(dim_hiddens))]\n",
    "        \n",
    "        super(Predictor, self).__init__(loss=loss, device=device)\n",
    "        \n",
    "        if net is None:\n",
    "            self.net = self._set_cnns(dim_hiddens, size_kernels, strides, pads, \n",
    "                                      dilations, dropout, len_seq)\n",
    "        else:\n",
    "            assert hidden_out is not None\n",
    "            self.hidden_out = dim_out\n",
    "            self.net = net\n",
    "            \n",
    "        if downnet is None:\n",
    "            down_dims = [self.hidden_out*x for x in down_hiddens] + [1]\n",
    "            if add_down>0:\n",
    "                down_dims[0] += add_down\n",
    "            print('down_dims', down_dims)\n",
    "            self.downnet = nets.get_MLP(down_dims, dropout=0.35, end=True)\n",
    "        else:\n",
    "            self.downnet = downnet\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.net = self.net.to(device)\n",
    "        self.downnet = self.downnet.to(device)\n",
    "        \n",
    "        parms = list(self.net.parameters())\n",
    "        parms += list(self.downnet.parameters())\n",
    "        self.optimizer = optimizer(parms)\n",
    "     \n",
    "    def _set_cnns(self, dim_hiddens, size_kernels, strides, pads, dilations,\n",
    "                 dropout, len_seq):\n",
    "        \"\"\"\n",
    "        in_channels, out_channels are dimension of features\n",
    "        \"\"\"\n",
    "        convs = []\n",
    "        bnorms = []\n",
    "        activation = nn.ReLU()\n",
    "        dropout = nn.Dropout(dropout)\n",
    "        # Setting should be changed by padding, stride, dilation\n",
    "        out_len_seq = len_seq\n",
    "        for ii in range(1, len(dim_hiddens)):\n",
    "            size_kernel = size_kernels[ii-1]\n",
    "            in_size_feat = dim_hiddens[ii-1]\n",
    "            out_size_feat = dim_hiddens[ii]\n",
    "            stride = strides[ii-1]\n",
    "            pad = pads[ii-1]\n",
    "            dilation = dilations[ii-1]\n",
    "            print('stride', stride, 'pad', pad, 'dilation', dilation, 'size_kernel', size_kernel)\n",
    "\n",
    "            conv1 = nn.Conv1d(in_channels=in_size_feat, \n",
    "                              out_channels=out_size_feat,\n",
    "                              kernel_size=size_kernel,\n",
    "                              stride=stride,\n",
    "                              padding=pad,\n",
    "                              dilation=dilation)\n",
    "            out_len_seq = int((out_len_seq + 2*pad - dilation*(size_kernel-1)-1)/stride) + 1\n",
    "            print('out_len_seq', out_len_seq)\n",
    "            convs.append(conv1)\n",
    "            if out_len_seq < 1:\n",
    "                raise Exception('Out dimension is less than 0, control parameters')\n",
    "            bnorms.append(nn.BatchNorm1d(num_features=out_size_feat))\n",
    "            \n",
    "        last_n_channel = dim_hiddens[-1]\n",
    "        # for data transforming to 1D\n",
    "        self.hidden_out =  last_n_channel * out_len_seq\n",
    "        \n",
    "        conv_nets = []\n",
    "        for conv1, bnorm in zip(convs, bnorms):\n",
    "            conv_nets.append(dropout)\n",
    "            conv_nets.append(conv1)\n",
    "            conv_nets.append(activation)\n",
    "            conv_nets.append(bnorm)\n",
    "            \n",
    "        return nn.Sequential(*conv_nets)\n",
    "        \n",
    "    def set_train(self):\n",
    "        self.net.train()\n",
    "        self.downnet.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        self.net.eval()\n",
    "        self.downnet.eval()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for ii, layer in enumerate(self.net):\n",
    "            if 'Conv' in str(layer):\n",
    "                torch.nn.init.xavier_uniform_(self.net[ii].weight)\n",
    "        for ii, layer in enumerate(self.downnet):\n",
    "            if 'Linear' in str(layer):\n",
    "                torch.nn.init.xavier_uniform_(self.downnet[ii].weight)\n",
    "                #torch.nn.init.xavier_normal_(self.downnet[ii].weight)\n",
    "\n",
    "    def forward(self, x, addi=('NULL')):\n",
    "        out = self.net(x)\n",
    "        dim_batch = out.shape[0]\n",
    "        # concatenate all data into 1D data\n",
    "        # This transforms CNN filtered data to be used for downnet which is MLP\n",
    "        out = out.reshape(dim_batch, -1) \n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            out = torch.cat((out, addi),dim=1)\n",
    "        out = self.downnet(out) \n",
    "        return out.view(-1)\n",
    "\n",
    "    def run_eval(self, data):\n",
    "        self.set_eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        tgts = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in = data_batch[0]\n",
    "                mean_val = data_batch[1]\n",
    "                range_val = data_batch[2]\n",
    "                pred_date = data_batch[3]\n",
    "                tgt = data_batch[5]\n",
    "                addi = data_batch[-1]\n",
    "                \n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                if 'Tensor' in str(type(addi)):\n",
    "                    addi = addi.to(self.device)\n",
    "                    \n",
    "                out = self.forward(data_in, addi)\n",
    "                #print('loss', loss, self.loss)\n",
    "                loss += self.loss(out, tgt).cpu().numpy()\n",
    "                out = out.cpu().numpy()\n",
    "                #print('loss===', loss)\n",
    "                tgt = tgt.cpu().numpy()\n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                    tgts = tgt\n",
    "                else:\n",
    "                    outs = np.concatenate((outs, out), axis=0)\n",
    "                    tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "        loss /= 1.0*(i_batch+1)\n",
    "        print('evaluate', 'loss', loss, 'accuracy : define function')\n",
    "        return outs, tgts, loss\n",
    "    \n",
    "    def run_batch(self, i_batch, data_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in = data_batch[0]\n",
    "        mean_val = data_batch[1]\n",
    "        range_val = data_batch[2]\n",
    "        pred_date = data_batch[3]\n",
    "        tgt = data_batch[5]\n",
    "        addi = data_batch[-1]\n",
    "        \n",
    "        data_in = data_in.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            addi = addi.to(self.device)\n",
    "        out = self.forward(data_in, addi)\n",
    "        \n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1072,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_seq = train_dset[0][0].shape[-1]\n",
    "# Add difference variables\n",
    "n_features = train_dset[0][0].shape[0]\n",
    "down_hiddens = [1,2,2] # multiples of input size\n",
    "size_kernels=[3,3,3,3]\n",
    "dilations=[1,2,3]\n",
    "dilations=None\n",
    "dilations=[1,1,1,2]\n",
    "#strides=[1,2,3]\n",
    "#strides=[1,1,1]\n",
    "strides=None\n",
    "# actual feature size is dim_hidden * out_len_seq\n",
    "#dim_hiddens = [n_features] + [10, 10, 20]\n",
    "dim_hiddens = [n_features] + [n_features, n_features, n_features, n_features]\n",
    "loss = nn.MSELoss() # which combines logsoftmax and nll loss\n",
    "optimizer = optim.Adam\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stride 1 pad 0 dilation 1 size_kernel 3\n",
      "out_len_seq 13\n",
      "stride 1 pad 0 dilation 1 size_kernel 3\n",
      "out_len_seq 11\n",
      "stride 1 pad 0 dilation 1 size_kernel 3\n",
      "out_len_seq 9\n",
      "stride 1 pad 0 dilation 2 size_kernel 3\n",
      "out_len_seq 5\n",
      "down_dims [99, 150, 150, 1]\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(loss=loss, optimizer=optimizer, \n",
    "                      device=device, len_seq= len_seq,\n",
    "                      dim_hiddens=dim_hiddens, down_hiddens=down_hiddens, \n",
    "                      size_kernels=size_kernels, add_down=3*8,\n",
    "                      strides=strides,\n",
    "                      dropout=0.1,\n",
    "                      dilations=dilations,\n",
    "                     )\n",
    "                      #dilations=dilations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.08031472558183457\n",
      "eval_train evaluate loss 0.06857562576656911 accuracy : define function\n",
      "epoch 1 loss 0.08039418674671828\n",
      "eval_train evaluate loss 0.06909842639049488 accuracy : define function\n",
      "epoch 2 loss 0.07980629162334685\n",
      "eval_train evaluate loss 0.06832085939040825 accuracy : define function\n",
      "epoch 3 loss 0.07947979939740095\n",
      "eval_train evaluate loss 0.0658079196712864 accuracy : define function\n",
      "epoch 4 loss 0.07956971283724058\n",
      "eval_train evaluate loss 0.06639901220576087 accuracy : define function\n",
      "epoch 5 loss 0.07974817420342076\n",
      "eval_train evaluate loss 0.06665357141130006 accuracy : define function\n",
      "epoch 6 loss 0.07915768622240024\n",
      "eval_train evaluate loss 0.07384842567479433 accuracy : define function\n",
      "epoch 7 loss 0.07898845345671497\n",
      "eval_train evaluate loss 0.06544970415198981 accuracy : define function\n",
      "epoch 8 loss 0.08075423160595681\n",
      "eval_train evaluate loss 0.06615514658502679 accuracy : define function\n",
      "epoch 9 loss 0.07853573277147848\n",
      "eval_train evaluate loss 0.06700103713282898 accuracy : define function\n"
     ]
    }
   ],
   "source": [
    "predictor.run_train(n_epoch=10, data=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = {}\n",
    "tgts = None\n",
    "preds = None\n",
    "predictor.set_eval()\n",
    "with torch.no_grad():\n",
    "    for data_batch in test_loader:\n",
    "        data_in = data_batch[0]\n",
    "        tgt = data_batch[5]\n",
    "        \n",
    "        data_in = data_in.to(device)\n",
    "        addi = data_batch[-1]\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            addi = addi.to(device)\n",
    "        tgt = tgt.numpy()\n",
    "        #out = predictor.forward(data_in).cpu().numpy()\n",
    "        out = predictor.forward(data_in, addi).cpu().numpy()\n",
    "        if preds is None:\n",
    "            preds = out\n",
    "            tgts = tgt\n",
    "        else:\n",
    "            preds = np.concatenate((preds, out), axis=0)\n",
    "            tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "            \n",
    "        prediction_dates = data_batch[3]\n",
    "        past_prices = data_batch[6]\n",
    "        predicting_dates = data_batch[7]\n",
    "        for ii, date1 in enumerate(prediction_dates):\n",
    "            out1 = out[ii]\n",
    "            out_orig = out1/10*past_prices[ii] + past_prices[ii]\n",
    "            rets[date1] = {'predicting_date': predicting_dates[ii],\n",
    "                          'pred': out_orig,\n",
    "                           'past': past_prices[ii],\n",
    "                           'tgt': data_batch[4][ii]}\n",
    "            \n",
    "preds = preds.reshape(-1)\n",
    "tgts = tgts.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147 482\n",
      "403 568\n",
      "1068 550 0.5149812734082397\n"
     ]
    }
   ],
   "source": [
    "sum1 = np.sum((preds > 0) & (tgts > 0))\n",
    "sum2 = np.sum((preds < 0) & (tgts < 0))\n",
    "print(sum1, np.sum(tgts>0))\n",
    "print(sum2, np.sum(tgts<0))\n",
    "print(len(preds), sum1+sum2, 1.0*(sum1+sum2)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 44\n",
      "19 24\n",
      "71 36 0.5070422535211268\n"
     ]
    }
   ],
   "source": [
    "limit = list(rets.keys()).index('2021-01-04')\n",
    "sum1 = np.sum((preds[limit:] > 0) & (tgts[limit:] > 0))\n",
    "sum2 = np.sum((preds[limit:] < 0) & (tgts[limit:] < 0))\n",
    "print(sum1, np.sum(tgts[limit:]>0))\n",
    "print(sum2, np.sum(tgts[limit:]<0))\n",
    "print(len(preds[limit:]), sum1+sum2, 1.0*(sum1+sum2)/len(preds[limit:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012098761,\n",
       " 0.06398107,\n",
       " 0.034371212,\n",
       " -0.014316494,\n",
       " -0.026728772,\n",
       " -0.008717187,\n",
       " 0.00914385,\n",
       " 0.0051481742,\n",
       " -0.015788577,\n",
       " -0.033726607,\n",
       " -0.02699127,\n",
       " -0.025021302,\n",
       " -0.017667959,\n",
       " -0.017090008,\n",
       " -0.025116883,\n",
       " -0.010800019,\n",
       " -0.0011157971,\n",
       " -0.025244031,\n",
       " -0.02163313,\n",
       " -0.0034951027]"
      ]
     },
     "execution_count": 1022,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(preds)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* high dropout(0.35) is not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021 = {key: val for key, val in rets.items() if key>'2021-01-01'}\n",
    "rows = list(rets_2021['2021-01-04'].keys())\n",
    "cols = list(rets_2021.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021b = {}\n",
    "for col in cols:\n",
    "    for row in rows:\n",
    "        rets_2021b.setdefault(row, {})\n",
    "        val = rets_2021[col][row]\n",
    "        if type(val) is not str:\n",
    "            rets_2021b[row][col] = rets_2021[col][row].item()        \n",
    "        else:\n",
    "            rets_2021b[row][col] = rets_2021[col][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rets_2021b).to_csv('./rets.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021-01-04': {'predicting_date': '2020-12-24',\n",
       "  'pred': tensor(0.9607),\n",
       "  'past': tensor(0.9403),\n",
       "  'tgt': tensor(0.9550)},\n",
       " '2021-01-05': {'predicting_date': '2020-12-28',\n",
       "  'pred': tensor(0.9542),\n",
       "  'past': tensor(0.9453),\n",
       "  'tgt': tensor(0.9370)},\n",
       " '2021-01-06': {'predicting_date': '2020-12-29',\n",
       "  'pred': tensor(0.9781),\n",
       "  'past': tensor(0.9630),\n",
       "  'tgt': tensor(0.9600)},\n",
       " '2021-01-07': {'predicting_date': '2020-12-30',\n",
       "  'pred': tensor(0.9640),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-08': {'predicting_date': '2020-12-31',\n",
       "  'pred': tensor(0.9614),\n",
       "  'past': tensor(0.9733),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-11': {'predicting_date': '2021-01-04',\n",
       "  'pred': tensor(0.9480),\n",
       "  'past': tensor(0.9650),\n",
       "  'tgt': tensor(0.9650)},\n",
       " '2021-01-12': {'predicting_date': '2021-01-05',\n",
       "  'pred': tensor(0.9365),\n",
       "  'past': tensor(0.9540),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-13': {'predicting_date': '2021-01-06',\n",
       "  'pred': tensor(0.9564),\n",
       "  'past': tensor(0.9507),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-14': {'predicting_date': '2021-01-07',\n",
       "  'pred': tensor(0.9678),\n",
       "  'past': tensor(0.9557),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-15': {'predicting_date': '2021-01-08',\n",
       "  'pred': tensor(0.9769),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-18': {'predicting_date': '2021-01-11',\n",
       "  'pred': tensor(0.9591),\n",
       "  'past': tensor(0.9707),\n",
       "  'tgt': tensor(0.9670)},\n",
       " '2021-01-19': {'predicting_date': '2021-01-12',\n",
       "  'pred': tensor(0.9671),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-20': {'predicting_date': '2021-01-13',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9723),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-21': {'predicting_date': '2021-01-14',\n",
       "  'pred': tensor(0.9775),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-22': {'predicting_date': '2021-01-15',\n",
       "  'pred': tensor(0.9616),\n",
       "  'past': tensor(0.9773),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-01-25': {'predicting_date': '2021-01-18',\n",
       "  'pred': tensor(0.9577),\n",
       "  'past': tensor(0.9747),\n",
       "  'tgt': tensor(1.0050)},\n",
       " '2021-01-26': {'predicting_date': '2021-01-19',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9713),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-01-27': {'predicting_date': '2021-01-20',\n",
       "  'pred': tensor(0.9637),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-28': {'predicting_date': '2021-01-21',\n",
       "  'pred': tensor(0.9625),\n",
       "  'past': tensor(0.9740),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-29': {'predicting_date': '2021-01-22',\n",
       "  'pred': tensor(0.9944),\n",
       "  'past': tensor(0.9783),\n",
       "  'tgt': tensor(0.9720)},\n",
       " '2021-02-01': {'predicting_date': '2021-01-25',\n",
       "  'pred': tensor(1.0058),\n",
       "  'past': tensor(0.9883),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-02-02': {'predicting_date': '2021-01-26',\n",
       "  'pred': tensor(1.0003),\n",
       "  'past': tensor(0.9990),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-03': {'predicting_date': '2021-01-27',\n",
       "  'pred': tensor(0.9802),\n",
       "  'past': tensor(0.9980),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-02-04': {'predicting_date': '2021-01-28',\n",
       "  'pred': tensor(0.9724),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-05': {'predicting_date': '2021-01-29',\n",
       "  'pred': tensor(0.9692),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-08': {'predicting_date': '2021-02-01',\n",
       "  'pred': tensor(0.9851),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-09': {'predicting_date': '2021-02-02',\n",
       "  'pred': tensor(0.9823),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-02-10': {'predicting_date': '2021-02-03',\n",
       "  'pred': tensor(0.9800),\n",
       "  'past': tensor(0.9813),\n",
       "  'tgt': tensor(0.9920)},\n",
       " '2021-02-15': {'predicting_date': '2021-02-04',\n",
       "  'pred': tensor(0.9856),\n",
       "  'past': tensor(0.9790),\n",
       "  'tgt': tensor(0.9940)},\n",
       " '2021-02-16': {'predicting_date': '2021-02-05',\n",
       "  'pred': tensor(0.9774),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-17': {'predicting_date': '2021-02-08',\n",
       "  'pred': tensor(1.0049),\n",
       "  'past': tensor(0.9857),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-18': {'predicting_date': '2021-02-09',\n",
       "  'pred': tensor(0.9871),\n",
       "  'past': tensor(0.9890),\n",
       "  'tgt': tensor(0.9850)},\n",
       " '2021-02-19': {'predicting_date': '2021-02-10',\n",
       "  'pred': tensor(0.9882),\n",
       "  'past': tensor(0.9923),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-22': {'predicting_date': '2021-02-15',\n",
       "  'pred': tensor(0.9979),\n",
       "  'past': tensor(0.9920),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-02-23': {'predicting_date': '2021-02-16',\n",
       "  'pred': tensor(0.9768),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(1.0150)},\n",
       " '2021-02-24': {'predicting_date': '2021-02-17',\n",
       "  'pred': tensor(0.9893),\n",
       "  'past': tensor(0.9853),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-02-25': {'predicting_date': '2021-02-18',\n",
       "  'pred': tensor(0.9911),\n",
       "  'past': tensor(0.9823),\n",
       "  'tgt': tensor(0.9960)},\n",
       " '2021-02-26': {'predicting_date': '2021-02-19',\n",
       "  'pred': tensor(1.0203),\n",
       "  'past': tensor(0.9873),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-03-02': {'predicting_date': '2021-02-22',\n",
       "  'pred': tensor(1.0568),\n",
       "  'past': tensor(1.),\n",
       "  'tgt': tensor(1.0220)},\n",
       " '2021-03-03': {'predicting_date': '2021-02-23',\n",
       "  'pred': tensor(1.0410),\n",
       "  'past': tensor(1.0100),\n",
       "  'tgt': tensor(1.0170)},\n",
       " '2021-03-04': {'predicting_date': '2021-02-24',\n",
       "  'pred': tensor(1.0213),\n",
       "  'past': tensor(1.0123),\n",
       "  'tgt': tensor(1.0350)},\n",
       " '2021-03-05': {'predicting_date': '2021-02-25',\n",
       "  'pred': tensor(0.9976),\n",
       "  'past': tensor(1.0043),\n",
       "  'tgt': tensor(1.0670)},\n",
       " '2021-03-08': {'predicting_date': '2021-02-26',\n",
       "  'pred': tensor(1.0095),\n",
       "  'past': tensor(1.0060),\n",
       "  'tgt': tensor(1.1500)},\n",
       " '2021-03-09': {'predicting_date': '2021-03-02',\n",
       "  'pred': tensor(1.0161),\n",
       "  'past': tensor(1.0127),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-10': {'predicting_date': '2021-03-03',\n",
       "  'pred': tensor(1.0121),\n",
       "  'past': tensor(1.0197),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-11': {'predicting_date': '2021-03-04',\n",
       "  'pred': tensor(1.0291),\n",
       "  'past': tensor(1.0247),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-12': {'predicting_date': '2021-03-05',\n",
       "  'pred': tensor(1.0829),\n",
       "  'past': tensor(1.0397),\n",
       "  'tgt': tensor(1.2300)},\n",
       " '2021-03-15': {'predicting_date': '2021-03-08',\n",
       "  'pred': tensor(1.1399),\n",
       "  'past': tensor(1.0840),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-16': {'predicting_date': '2021-03-09',\n",
       "  'pred': tensor(1.1995),\n",
       "  'past': tensor(1.1457),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-17': {'predicting_date': '2021-03-10',\n",
       "  'pred': tensor(1.2035),\n",
       "  'past': tensor(1.1817),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-03-18': {'predicting_date': '2021-03-11',\n",
       "  'pred': tensor(1.1807),\n",
       "  'past': tensor(1.1900),\n",
       "  'tgt': tensor(1.1370)},\n",
       " '2021-03-19': {'predicting_date': '2021-03-12',\n",
       "  'pred': tensor(1.1893),\n",
       "  'past': tensor(1.1933),\n",
       "  'tgt': tensor(1.1420)},\n",
       " '2021-03-22': {'predicting_date': '2021-03-15',\n",
       "  'pred': tensor(1.2049),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1300)},\n",
       " '2021-03-23': {'predicting_date': '2021-03-16',\n",
       "  'pred': tensor(1.1990),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-03-24': {'predicting_date': '2021-03-17',\n",
       "  'pred': tensor(1.1872),\n",
       "  'past': tensor(1.1907),\n",
       "  'tgt': tensor(1.1200)},\n",
       " '2021-03-25': {'predicting_date': '2021-03-18',\n",
       "  'pred': tensor(1.1615),\n",
       "  'past': tensor(1.1630),\n",
       "  'tgt': tensor(1.0950)},\n",
       " '2021-03-26': {'predicting_date': '2021-03-19',\n",
       "  'pred': tensor(1.1602),\n",
       "  'past': tensor(1.1520),\n",
       "  'tgt': tensor(1.1250)},\n",
       " '2021-03-29': {'predicting_date': '2021-03-22',\n",
       "  'pred': tensor(1.1531),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1150)},\n",
       " '2021-03-30': {'predicting_date': '2021-03-23',\n",
       "  'pred': tensor(1.1640),\n",
       "  'past': tensor(1.1397),\n",
       "  'tgt': tensor(1.1510)},\n",
       " '2021-03-31': {'predicting_date': '2021-03-24',\n",
       "  'pred': tensor(1.1460),\n",
       "  'past': tensor(1.1323),\n",
       "  'tgt': tensor(1.1270)},\n",
       " '2021-04-01': {'predicting_date': '2021-03-25',\n",
       "  'pred': tensor(1.1230),\n",
       "  'past': tensor(1.1207),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-02': {'predicting_date': '2021-03-26',\n",
       "  'pred': tensor(1.1178),\n",
       "  'past': tensor(1.1133),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-04-05': {'predicting_date': '2021-03-29',\n",
       "  'pred': tensor(1.1063),\n",
       "  'past': tensor(1.1117),\n",
       "  'tgt': tensor(1.2020)},\n",
       " '2021-04-06': {'predicting_date': '2021-03-30',\n",
       "  'pred': tensor(1.1355),\n",
       "  'past': tensor(1.1303),\n",
       "  'tgt': tensor(1.1850)},\n",
       " '2021-04-07': {'predicting_date': '2021-03-31',\n",
       "  'pred': tensor(1.1263),\n",
       "  'past': tensor(1.1310),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-04-08': {'predicting_date': '2021-04-01',\n",
       "  'pred': tensor(1.1260),\n",
       "  'past': tensor(1.1377),\n",
       "  'tgt': tensor(1.1600)},\n",
       " '2021-04-09': {'predicting_date': '2021-04-02',\n",
       "  'pred': tensor(1.1282),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1670)},\n",
       " '2021-04-12': {'predicting_date': '2021-04-05',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1613),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-13': {'predicting_date': '2021-04-06',\n",
       "  'pred': tensor(1.1763),\n",
       "  'past': tensor(1.1780),\n",
       "  'tgt': tensor(1.1320)},\n",
       " '2021-04-14': {'predicting_date': '2021-04-07',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1880),\n",
       "  'tgt': tensor(1.1000)},\n",
       " '2021-04-15': {'predicting_date': '2021-04-08',\n",
       "  'pred': tensor(1.1563),\n",
       "  'past': tensor(1.1740),\n",
       "  'tgt': tensor(1.1400)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
