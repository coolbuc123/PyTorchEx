{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "try: # ip: 153\n",
    "    lib_s = '/home/adminuser/public/libs/pytorch_examples/basic_networks'\n",
    "    sys.path.append(lib_s)\n",
    "    import generate_model as nets\n",
    "except: # ip : 103\n",
    "    lib_s = '/home/bwlee/work/codes/examples/basic_networks'\n",
    "    sys.path.append(lib_s)\n",
    "    import generate_model as nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make input data\n",
    "interest.csv has only closed price.   \n",
    "On the other hand, zipline needs OLHCV(?) format for each index  \n",
    "make dummy columns and put 'close' value to them  \n",
    "make csv file for each index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s0 = 'interest.csv'\n",
    "df0 = pd.read_csv(f_s0, index_col='date')\n",
    "df0 = df0.iloc[::-1] # 과거에서 현재로 정렬순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environ():\n",
    "    def __init__(self):\n",
    "        self.indices = ['trea3', 'trea5', 'trea10']\n",
    "        #self.tgt_index = 'trea10'\n",
    "        self.tgt_index = 'trea3'\n",
    "        # input window size\n",
    "        self.input_size = 15\n",
    "        # prediction is made after algo.pred_period from the present\n",
    "        self.pred_period = 5 \n",
    "env = Environ() # setting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dff0 = df0[:200] # test\n",
    "dff0 = df0\n",
    "dff = dff0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_periods = [int(env.input_size/4*x) for x in [1, 2, 3]]\n",
    "for diff_period in diff_periods:\n",
    "    diff = dff0.diff(periods=diff_period)\n",
    "    ii = diff_period\n",
    "    columns = ['d%d_diff_trea3'%(ii), 'd%d_diff_trea5'%(ii), 'd%d_diff_trea10'%(ii)]\n",
    "    diff.columns = columns\n",
    "    dff = dff.merge(diff, how='inner', on='date')\n",
    "dff['diff35'] = dff['trea3'] - dff['trea5']\n",
    "dff['diff310'] = dff['trea3'] - dff['trea10']\n",
    "dff['diff510'] = dff['trea5'] - dff['trea10']\n",
    "\n",
    "dff = dff.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data without time confusion\n",
    "Seperate two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized input, norm val, tgt_date, tgt_val, tgt_diff_ratio, \n",
    "def push_data(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read past data and push them to mkt_data\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    #print(data)\n",
    "    #print('----------')\n",
    "    for ii_days in range(len(data)):\n",
    "        if ii_days+1 < env.input_size:\n",
    "            continue\n",
    "        elif ii_days+1 == env.input_size:\n",
    "            env.start_date = data.iloc[ii_days].name\n",
    "        \n",
    "        predicting_date = data.iloc[ii_days].name\n",
    "        # historical data from today to env.input_size behind days\n",
    "        history = data.iloc[ii_days+1-env.input_size:ii_days+1]\n",
    "                \n",
    "        ## normalize : look back 1 year\n",
    "        lookback_step = 250\n",
    "        lookback_step = env.input_size\n",
    "        if ii_days+1 < lookback_step:\n",
    "            cumul = data.iloc[:ii_days+1]\n",
    "        else:\n",
    "            cumul = data.iloc[ii_days+1-lookback_step:ii_days+1]\n",
    "        \"\"\"\n",
    "        mean_val = cumul.mean() \n",
    "        range_val = 2*cumul.std()\n",
    "        \"\"\"\n",
    "        mean_val = history.mean() \n",
    "        #range_val = history.max() - history.min()\n",
    "        range_val = 2*history.std()\n",
    "        norm_history = (history-mean_val)/range_val\n",
    "        \n",
    "        mkt_data[predicting_date] = [norm_history, mean_val, range_val, \n",
    "                                     None, None, None, None, None]\n",
    "            \n",
    "def push_tgt(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read future data and push them to mkt_data\n",
    "    future_prediction_date is a future date when model want to predict\n",
    "    past_predicting_date is a past date when model made a prediction with given information\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        ii_predicting = ii_days - env.pred_period\n",
    "        predicting_date = data.iloc[ii_predicting].name\n",
    "        if ii_predicting < 0 or predicting_date < env.start_date:\n",
    "            continue\n",
    "        \n",
    "        past_history = mkt_data[predicting_date][0]\n",
    "        mean_val = mkt_data[predicting_date][1]\n",
    "        range_val = mkt_data[predicting_date][2]\n",
    "        \n",
    "        past_price = (past_history.iloc[-1]*range_val+mean_val)[env.tgt_index] # 49.6\n",
    "        \n",
    "        #past_price = (past_history.iloc[-5]*range_val+mean_val)[env.tgt_index] # 56.3\n",
    "        \n",
    "        #past_price = (past_history.iloc[-3]*range_val+mean_val)[env.tgt_index] # 53.0, # 53.0\n",
    "        #past_price = (past_history.iloc[-13]*range_val+mean_val)[env.tgt_index] # 67.4\n",
    "        #past_price = (past_history*range_val+mean_val).mean()[env.tgt_index] # 58.4\n",
    "        #past_price = (past_history*range_val+mean_val)[:-3].mean()[env.tgt_index] # 63.2\n",
    "        #past_price = (past_history*range_val+mean_val)[:-5].mean()[env.tgt_index] # 63.7\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10]*range_val+mean_val)[env.tgt_index] # 66.4\n",
    "\n",
    "        #past_price = (past_history.iloc[-5:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        #past_price = (past_history.iloc[-3:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10:].mean()*range_val+mean_val)[env.tgt_index] # 55.9\n",
    "        #past_price = (past_history.iloc[-10:-5].mean()*range_val+mean_val)[env.tgt_index] # 60.4\n",
    "        \n",
    "        \n",
    "        prediction_date = data.iloc[ii_days].name\n",
    "        tgt_price = data.iloc[ii_days][env.tgt_index]\n",
    "        #tgt_price = (data.iloc[ii_days-1:ii_days+2].mean())[env.tgt_index]\n",
    "        \n",
    "        tgt_ratio = (tgt_price - past_price)/past_price*10\n",
    "        \n",
    "        #mkt_data[predicting_date][0] = past_history[:-4]\n",
    "        mkt_data[predicting_date][3] = prediction_date\n",
    "        mkt_data[predicting_date][4] = np.single(tgt_price)\n",
    "        mkt_data[predicting_date][5] = np.single(tgt_ratio)\n",
    "        \n",
    "        mkt_data[predicting_date][6] = np.single(past_price)\n",
    "        mkt_data[predicting_date][7] = predicting_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkt_data = {} # object to store data for learning\n",
    "push_data(env, dff, mkt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check data are pushed correctly\n",
    "* no dates should be ahead of key date\n",
    "* prediction info are not pushed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000-11-06', '2000-11-07', '2000-11-08', '2000-11-09', '2000-11-10', '2000-11-11', '2000-11-13', '2000-11-14', '2000-11-15', '2000-11-16', '2000-11-17', '2000-11-18', '2000-11-20', '2000-11-21', '2000-11-22', '2000-11-23', '2000-11-24', '2000-11-25', '2000-11-27', '2000-11-28', '2000-11-29', '2000-11-30', '2000-12-01', '2000-12-02', '2000-12-04', '2000-12-05', '2000-12-06', '2000-12-07', '2000-12-08', '2000-12-09']\n"
     ]
    }
   ],
   "source": [
    "keys = list(mkt_data.keys())[:30]\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[               trea3     trea5    trea10\n",
       " date                                    \n",
       " 2000-11-02  0.234349  0.241046  0.162245\n",
       " 2000-11-03  0.269018  0.281901  0.192758\n",
       " 2000-11-04  0.269018  0.281901  0.192758\n",
       " 2000-11-06  0.303687  0.322756  0.230898\n",
       " 2000-11-07  0.303687  0.330927  0.230898\n",
       " 2000-11-08  0.173678  0.183849  0.108848\n",
       " 2000-11-09 -0.329022 -0.363612 -0.364098\n",
       " 2000-11-10 -0.727715 -0.698625 -0.623455\n",
       " 2000-11-11 -0.727715 -0.698625 -0.623455\n",
       " 2000-11-13 -0.831722 -0.804848 -0.760762\n",
       " 2000-11-14 -0.797053 -0.813019 -0.760762\n",
       " 2000-11-15 -0.641043 -0.674111 -0.646339\n",
       " 2000-11-16 -0.563038 -0.592401 -0.608199\n",
       " 2000-11-17 -0.589039 -0.616914 -0.631083\n",
       " 2000-11-18 -0.589039 -0.616914 -0.631083,\n",
       " trea3     7.614808\n",
       " trea5     7.802500\n",
       " trea10    8.103654\n",
       " dtype: float64,\n",
       " trea3     0.576885\n",
       " trea5     0.611917\n",
       " trea10    0.655467\n",
       " dtype: float64,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkt_data[keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-11-18'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_tgt(env, dff, mkt_data)\n",
    "mkt_data = {key: val for key, val in mkt_data.items() if (val[-1] is not None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHECK tgt push is correctly made\n",
    "end value of '2000-11-23' * norm == prediction value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-10-26  0.300000  0.272099  0.293963\n",
      "2000-10-27  0.105970  0.131358  0.152231\n",
      "2000-10-28  0.105970  0.131358  0.152231\n",
      "2000-10-30  0.091045  0.123951  0.152231\n",
      "2000-10-31  0.150746  0.175802  0.183727\n",
      "2000-11-01  0.105970  0.123951  0.136483\n",
      "2000-11-02  0.128358  0.123951  0.104987\n",
      "2000-11-03  0.158209  0.160988  0.136483\n",
      "2000-11-04  0.158209  0.160988  0.136483\n",
      "2000-11-06  0.188060  0.198025  0.175853\n",
      "2000-11-07  0.188060  0.205432  0.175853\n",
      "2000-11-08  0.076119  0.072099  0.049869\n",
      "2000-11-09 -0.356716 -0.424198 -0.438320\n",
      "2000-11-10 -0.700000 -0.727901 -0.706037\n",
      "2000-11-11 -0.700000 -0.727901 -0.706037, trea3     7.664000\n",
      "trea5     7.866333\n",
      "trea10    8.143333\n",
      "dtype: float64, trea3     0.670\n",
      "trea5     0.675\n",
      "trea10    0.635\n",
      "dtype: float64, '2000-11-17', 7.275, 0.11118832, 7.195, '2000-11-11']\n"
     ]
    }
   ],
   "source": [
    "#print(mkt_data['2000-11-18'])\n",
    "print(mkt_data['2000-11-11'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_data(Dataset):\n",
    "    def __init__(self, in_data0, dates, addi=None):\n",
    "        \"\"\"\n",
    "        :param in_data: dict of (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        :param addi: (optional) additional non-sequential elements that can be added to in_data\n",
    "        \"\"\"\n",
    "        in_data = cp.deepcopy(in_data0)\n",
    "        if addi is not None:\n",
    "            assert list(in_data.keys()) == list(addi.keys())\n",
    "             \n",
    "        # change dataframe to float numpy array\n",
    "        for key, val in in_data.items():\n",
    "            # transform in_data into Tensor\n",
    "            if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "                in_data[key][0] = torch.tensor(in_data[key][0].to_numpy(np.single))\n",
    "                in_data[key][1] = torch.tensor(in_data[key][1].to_numpy(np.single))\n",
    "                in_data[key][2] = torch.tensor(in_data[key][2].to_numpy(np.single))\n",
    "            \n",
    "            if addi is not None:\n",
    "                in_data[key].append(addi[key])\n",
    "            else:\n",
    "                in_data[key].append('NULL')\n",
    "            \n",
    "        self.in_data = in_data\n",
    "        self.i2dates = {}\n",
    "        for ii, date1 in enumerate(dates):\n",
    "             self.i2dates[ii] = date1\n",
    "        \n",
    "    def __getitem__(self, ii_date):\n",
    "        \"\"\"\n",
    "        :return: (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        predicting_date = self.i2dates[ii_date]\n",
    "        return self.in_data[predicting_date]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase data that do not have prediction target\n",
    "traintest_dates = sorted(list(mkt_data.keys()))\n",
    "n_train = len(traintest_dates)*8//10\n",
    "n_test = len(traintest_dates) - n_train\n",
    "train_dates = traintest_dates[:n_train]\n",
    "test_dates = traintest_dates[n_train:]\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for key, val in mkt_data.items():\n",
    "    if key in train_dates:\n",
    "        train_data[key] = val\n",
    "    elif key in test_dates:\n",
    "        test_data[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## this is an example, I already add differences\n",
    "def get_addi(in_data):\n",
    "    #print(in_data)\n",
    "    ret = {}\n",
    "    for key, val in in_data.items():\n",
    "        if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "            history_val = torch.tensor(val[0][['trea3','trea5', 'trea10']].to_numpy(np.single))\n",
    "        else:\n",
    "            history_val = val[0][:,:3] # only use three elements : ['trea3','trea5', 'trea10']\n",
    "        addi1 = history_val[-1] - history_val[-5]\n",
    "        addi2 = history_val[-1] - history_val[-7]\n",
    "        addi3 = history_val[-1] - history_val[-10]\n",
    "        addi4 = history_val[-1] - history_val[-3]\n",
    "        addi5 = history_val[-3] - history_val[-7]\n",
    "        addi6 = history_val[-3] - history_val[-10]\n",
    "        addi7 = history_val[-5] - history_val[-7]\n",
    "        addi8 = history_val[-5] - history_val[-10]\n",
    "        addi = torch.cat((addi1, addi2, addi3, addi4, addi5, addi6, addi7, addi8), dim=-1)    \n",
    "        ret[key] = addi\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "#batch_size = 512\n",
    "\n",
    "#train_dset = Index_data(train_data, train_dates)\n",
    "#test_dset = Index_data(test_data, test_dates)\n",
    "train_dset = Index_data(train_data, train_dates, get_addi(train_data))\n",
    "test_dset = Index_data(test_data, test_dates, get_addi(test_data))\n",
    "\n",
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor_rnn(nets.Net):\n",
    "    def __init__(self, rnn=None, downnet=None,\n",
    "                 loss=None, optimizer=None, device='cuda:2',\n",
    "                 dim_input=100, dim_hidden=200, bidirectional=True):\n",
    "        \"\"\"\n",
    "        net is consists of [embed, rnn, downnet]\n",
    "        :param downnet: define downstream job\n",
    "        \"\"\"\n",
    "        super(Predictor_rnn, self).__init__(loss=loss, device=device)\n",
    "        \n",
    "        if rnn is None:\n",
    "            self.rnn = nn.LSTM(input_size=dim_input, \n",
    "                               hidden_size=dim_hidden, \n",
    "                               num_layers= 2,\n",
    "                               batch_first=True,\n",
    "                               #bidirectional=False)\n",
    "                               bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = rnn\n",
    "            \n",
    "        if downnet is None:\n",
    "            self.downnet = nets.get_MLP([dim_hidden, dim_hidden*2, 1], \n",
    "                                        dropout=0.35, end=True)\n",
    "        else:\n",
    "            self.downnet = downnet\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.rnn = self.rnn.to(device)\n",
    "        self.downnet = self.downnet.to(device)\n",
    "        \n",
    "        parms = list(self.rnn.parameters())\n",
    "        parms += list(self.downnet.parameters())\n",
    "        self.optimizer = optimizer(parms)\n",
    "        \n",
    "    def set_train(self):\n",
    "        self.rnn.train()\n",
    "        self.downnet.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        self.rnn.eval()\n",
    "        self.downnet.eval()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for ii, layer in enumerate(self.downnet):\n",
    "            if 'Linear' in str(layer):\n",
    "                torch.nn.init.xavier_uniform_(self.downnet[ii].weight)\n",
    "                #torch.nn.init.xavier_normal_(self.downnet[ii].weight)\n",
    "\n",
    "    def forward(self, x, addi=('NULL')):\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = out[:,-1] # choose last output\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            out = torch.cat((out, addi),dim=1)\n",
    "        \n",
    "        out = self.downnet(out) \n",
    "        return out.view(-1)\n",
    "\n",
    "    def run_eval(self, data):\n",
    "        self.set_eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        tgts = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in = data_batch[0]\n",
    "                mean_val = data_batch[1]\n",
    "                range_val = data_batch[2]\n",
    "                pred_date = data_batch[3]\n",
    "                tgt = data_batch[5]\n",
    "                addi = data_batch[-1]\n",
    "                \n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                if 'Tensor' in str(type(addi)):\n",
    "                    addi = addi.to(self.device)\n",
    "                    \n",
    "                out = self.forward(data_in, addi)\n",
    "                #print('loss', loss, self.loss)\n",
    "                loss += self.loss(out, tgt).cpu().numpy()\n",
    "                out = out.cpu().numpy()\n",
    "                #print('loss===', loss)\n",
    "                tgt = tgt.cpu().numpy()\n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                    tgts = tgt\n",
    "                else:\n",
    "                    outs = np.concatenate((outs, out), axis=0)\n",
    "                    tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "        loss /= 1.0*(i_batch+1)\n",
    "        print('evaluate', 'loss', loss, 'accuracy : define function')\n",
    "        return outs, tgts, loss\n",
    "    \n",
    "    def run_batch(self, i_batch, data_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in = data_batch[0]\n",
    "        mean_val = data_batch[1]\n",
    "        range_val = data_batch[2]\n",
    "        pred_date = data_batch[3]\n",
    "        tgt = data_batch[5]\n",
    "        addi = data_batch[-1]\n",
    "        \n",
    "        data_in = data_in.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            addi = addi.to(self.device)\n",
    "        out = self.forward(data_in, addi)\n",
    "        \n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = train_dset[0][0].shape[-1]\n",
    "dim_hidden = 400 \n",
    "#downnet = nets.get_MLP([dim_hidden+3*3, dim_hidden, 1], \n",
    "\n",
    "# Add additional variables\n",
    "# no additional\n",
    "dim_hiddenb = dim_hidden \n",
    "# add 8*3(above examples) variables\n",
    "dim_hiddenb = dim_hidden+3*8 \n",
    "\n",
    "downnet = nets.get_MLP([dim_hiddenb, dim_hidden, 1], \n",
    "                        dropout=0.35, end=True)\n",
    "#bidownnet = nets.get_MLP([dim_hidden*2+3*8, dim_hidden*2, 1], \n",
    "#                        dropout=0.0, end=True)\n",
    "loss = nn.MSELoss() # which combines logsoftmax and nll loss\n",
    "optimizer = optim.Adam\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor_rnn(loss=loss, optimizer=optimizer, \n",
    "                          device=device, dim_input=dim_input, \n",
    "                          dim_hidden=dim_hidden, downnet=downnet, bidirectional=False)\n",
    "                          #dim_hidden=dim_hidden, downnet=bidownnet, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.10564924726512895\n",
      "evaluate loss 0.13288307430989602 accuracy : define function\n",
      "epoch 1 loss 0.10100839070197362\n",
      "evaluate loss 0.13254390350159476 accuracy : define function\n",
      "epoch 2 loss 0.09706368703228324\n",
      "evaluate loss 0.13254819525515332 accuracy : define function\n",
      "epoch 3 loss 0.09596463087112156\n",
      "evaluate loss 0.13333280296886668 accuracy : define function\n",
      "epoch 4 loss 0.09400750490934101\n",
      "evaluate loss 0.13271797733271823 accuracy : define function\n",
      "epoch 5 loss 0.09401697315164466\n",
      "evaluate loss 0.13312692195177078 accuracy : define function\n",
      "epoch 6 loss 0.09302833081403775\n",
      "evaluate loss 0.13196822803686648 accuracy : define function\n",
      "epoch 7 loss 0.09344892871024003\n",
      "evaluate loss 0.13216696701505604 accuracy : define function\n",
      "epoch 8 loss 0.09334946757377084\n",
      "evaluate loss 0.13126041148515308 accuracy : define function\n",
      "epoch 9 loss 0.09316687158017016\n",
      "evaluate loss 0.13249778090154424 accuracy : define function\n",
      "epoch 10 loss 0.0935307571025037\n",
      "evaluate loss 0.13245337828993797 accuracy : define function\n",
      "epoch 11 loss 0.09290251463874062\n",
      "evaluate loss 0.13193184838575475 accuracy : define function\n",
      "epoch 12 loss 0.0930153388363212\n",
      "evaluate loss 0.13301638097447507 accuracy : define function\n",
      "epoch 13 loss 0.09302117380855689\n",
      "evaluate loss 0.13130158711882198 accuracy : define function\n",
      "epoch 14 loss 0.09331401131713568\n",
      "evaluate loss 0.13301950649303548 accuracy : define function\n",
      "epoch 15 loss 0.09254051750498031\n",
      "evaluate loss 0.13242271257673993 accuracy : define function\n",
      "epoch 16 loss 0.09313267330402758\n",
      "evaluate loss 0.13168632085709012 accuracy : define function\n",
      "epoch 17 loss 0.09269328215228978\n",
      "evaluate loss 0.1316067788969068 accuracy : define function\n",
      "epoch 18 loss 0.09321142494011281\n",
      "evaluate loss 0.131725615438293 accuracy : define function\n",
      "epoch 19 loss 0.09261640200196807\n",
      "evaluate loss 0.13227591554031654 accuracy : define function\n",
      "epoch 20 loss 0.09253479390224414\n",
      "evaluate loss 0.13207621565636465 accuracy : define function\n",
      "epoch 21 loss 0.09252820803380724\n",
      "evaluate loss 0.13188819214701653 accuracy : define function\n",
      "epoch 22 loss 0.09255545514065829\n",
      "evaluate loss 0.13153298177263317 accuracy : define function\n",
      "epoch 23 loss 0.0921185152855382\n",
      "evaluate loss 0.13185281328418674 accuracy : define function\n",
      "epoch 24 loss 0.09226085690420065\n",
      "evaluate loss 0.1320529720362495 accuracy : define function\n",
      "epoch 25 loss 0.09242153162164475\n",
      "evaluate loss 0.13135268177617998 accuracy : define function\n",
      "epoch 26 loss 0.09174678224458624\n",
      "evaluate loss 0.1316402041736771 accuracy : define function\n",
      "epoch 27 loss 0.0911624086325738\n",
      "evaluate loss 0.13261326402425766 accuracy : define function\n",
      "epoch 28 loss 0.09224462687079586\n",
      "evaluate loss 0.13213739193537655 accuracy : define function\n",
      "epoch 29 loss 0.09178895614485243\n",
      "evaluate loss 0.13195607855039485 accuracy : define function\n"
     ]
    }
   ],
   "source": [
    "predictor.run_train(n_epoch=30, data=train_loader, test_data=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = {}\n",
    "tgts = None\n",
    "preds = None\n",
    "predictor.set_eval()\n",
    "with torch.no_grad():\n",
    "    for data_batch in test_loader:\n",
    "        data_in = data_batch[0]\n",
    "        tgt = data_batch[5]\n",
    "        addi = data_batch[-1]\n",
    "        \n",
    "        data_in = data_in.to(device)\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            addi = addi.to(device)\n",
    "            \n",
    "        tgt = tgt.numpy()\n",
    "        out = predictor.forward(data_in, addi).cpu().numpy()\n",
    "        if preds is None:\n",
    "            preds = out\n",
    "            tgts = tgt\n",
    "        else:\n",
    "            preds = np.concatenate((preds, out), axis=0)\n",
    "            tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "            \n",
    "        prediction_dates = data_batch[3]\n",
    "        past_prices = data_batch[6]\n",
    "        predicting_dates = data_batch[7]\n",
    "        for ii, date1 in enumerate(prediction_dates):\n",
    "            out1 = out[ii]\n",
    "            out_orig = out1/10*past_prices[ii] + past_prices[ii]\n",
    "            rets[date1] = {'predicting_date': predicting_dates[ii],\n",
    "                          'pred': out_orig,\n",
    "                           'past': past_prices[ii],\n",
    "                           'tgt': data_batch[4][ii]}\n",
    "            \n",
    "preds = preds.reshape(-1)\n",
    "tgts = tgts.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rets.keys()).index('2021-01-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 44\n",
      "8 24\n",
      "71 36 0.5070422535211268\n"
     ]
    }
   ],
   "source": [
    "limit = 999\n",
    "sum1 = np.sum((preds[limit:] > 0) & (tgts[limit:] > 0))\n",
    "sum2 = np.sum((preds[limit:] < 0) & (tgts[limit:] < 0))\n",
    "print(sum1, np.sum(tgts[limit:]>0))\n",
    "print(sum2, np.sum(tgts[limit:]<0))\n",
    "print(len(preds[limit:]), sum1+sum2, 1.0*(sum1+sum2)/len(preds[limit:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272 482\n",
      "313 570\n",
      "1070 585 0.5467289719626168\n"
     ]
    }
   ],
   "source": [
    "sum1 = np.sum((preds > 0) & (tgts > 0))\n",
    "sum2 = np.sum((preds < 0) & (tgts < 0))\n",
    "print(sum1, np.sum(tgts>0))\n",
    "print(sum2, np.sum(tgts<0))\n",
    "print(len(preds), sum1+sum2, 1.0*(sum1+sum2)/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## all includes 3*3=9 diff, predict 4 days avg, prediction_period=10\n",
    "\n",
    "lstm_layer=2  \n",
    "epoch 30, dropout=0, dim_hidden=100, mlp (dim_hidden, dim_hidden, 1) --> 55.5%  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, 1) --> 55.3%  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden*2, 1) --> 53.4%  \n",
    "\n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden//2, 1) --> 53.4%  \n",
    "epoch 20, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden//2, 1) --> 53.9%  \n",
    "\n",
    "epoch 20, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 48.8%  \n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 51.4%  \n",
    "epoch 40, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 53.7%  \n",
    "epoch 50, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 49.1%  \n",
    "epoch 60, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 48.7%  \n",
    "epoch 70, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 50.3%  \n",
    "\n",
    "\n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.2%  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.0%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 56.9%, 54.3%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.9%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.3%  \n",
    "epoch 60, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 48.6%, 49.6  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.2  \n",
    "epoch 40, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.8  \n",
    "epoch 50, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.8  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=500, mlp (dim_hidden, dim_hidden, 1) --> 55.8  \n",
    "epoch 40, dropout=0, dim_hidden=500, mlp (dim_hidden, dim_hidden, 1) --> 53.8  \n",
    "\n",
    "lstm_layer=1\n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.3%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.7%  \n",
    "\n",
    "lstm_layer=3\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.7%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.4%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.8%  \n",
    "\n",
    "## includes 3*8=24 diff\n",
    "layer=2\n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.5%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.9%  \n",
    "\n",
    "## includes 3*8=24 diff, 1day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.8%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below prediction days are (-2,-1,0)\n",
    "## includes 3*8=24 diff, 3day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.7%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.5%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=5\n",
    "layer=2\n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.8%  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 62.6%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.5%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.3%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=15\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=20\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.7%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.1%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.1%  \n",
    "epoch 60, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 51.4%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  includes 3*8=24 diff, last price, 3day prediction, layer=2\n",
    "epoch 5, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 51.7%  \n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 56.4%, 54.8, 54.6  \n",
    "epoch 10, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.7  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.5%, 53.9 \n",
    "epoch 20, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 49.3% \n",
    "epoch 30, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.2% \n",
    "epoch 40, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.5% \n",
    "\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.5  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.3  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9  \n",
    "\n",
    "## Bidirectional\n",
    "epoch 10, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.9 \n",
    "epoch 20, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.2 \n",
    "epoch 30, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.5 \n",
    "epoch 40, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.5 \n",
    "epoch 50, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.8 \n",
    "\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 56.7  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 54.1 \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.1 \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 56.4 \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.0 \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 54.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to last period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.0\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.2\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.1\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to last period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.6\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.1\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.5\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.1\n",
    "\n",
    "## last to last period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.3, 51.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.2, 52.8\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.8, 54.5\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.2, 52.1\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.5, 50.7\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.3, 52.2\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.1, 48.4\n",
    "\n",
    "## last to last period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.7\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.1\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 48.6\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 47.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 47.0  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.1  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.0  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.4  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4  \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.3  \n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.5  \n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.2  \n",
    "epoch 90, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.2\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.6\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.4\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.5\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.1\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.7\n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.9\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.0\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.0\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.6\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.4\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.8\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.4\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.3\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pred3 to last period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.0  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 60.1  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.9  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 60.5  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.0  \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.5  \n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.3  \n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.6  \n",
    "\n",
    "## pred3 to last period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.1\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.8\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.6\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.7\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.9\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.5\n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.1\n",
    "\n",
    "## pred3 to last period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.9\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.9\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.6\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.0\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.9\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.3\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.5\n",
    "\n",
    "## pred3 to last    period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.8\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.1\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.2\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021 = {key: val for key, val in rets.items() if key>'2021-01-01'}\n",
    "rows = list(rets_2021['2021-01-04'].keys())\n",
    "cols = list(rets_2021.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021b = {}\n",
    "for col in cols:\n",
    "    for row in rows:\n",
    "        rets_2021b.setdefault(row, {})\n",
    "        val = rets_2021[col][row]\n",
    "        if type(val) is not str:\n",
    "            rets_2021b[row][col] = rets_2021[col][row].item()        \n",
    "        else:\n",
    "            rets_2021b[row][col] = rets_2021[col][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rets_2021b).to_csv('./rets.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021-01-04': {'predicting_date': '2020-12-24',\n",
       "  'pred': tensor(0.9607),\n",
       "  'past': tensor(0.9403),\n",
       "  'tgt': tensor(0.9550)},\n",
       " '2021-01-05': {'predicting_date': '2020-12-28',\n",
       "  'pred': tensor(0.9542),\n",
       "  'past': tensor(0.9453),\n",
       "  'tgt': tensor(0.9370)},\n",
       " '2021-01-06': {'predicting_date': '2020-12-29',\n",
       "  'pred': tensor(0.9781),\n",
       "  'past': tensor(0.9630),\n",
       "  'tgt': tensor(0.9600)},\n",
       " '2021-01-07': {'predicting_date': '2020-12-30',\n",
       "  'pred': tensor(0.9640),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-08': {'predicting_date': '2020-12-31',\n",
       "  'pred': tensor(0.9614),\n",
       "  'past': tensor(0.9733),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-11': {'predicting_date': '2021-01-04',\n",
       "  'pred': tensor(0.9480),\n",
       "  'past': tensor(0.9650),\n",
       "  'tgt': tensor(0.9650)},\n",
       " '2021-01-12': {'predicting_date': '2021-01-05',\n",
       "  'pred': tensor(0.9365),\n",
       "  'past': tensor(0.9540),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-13': {'predicting_date': '2021-01-06',\n",
       "  'pred': tensor(0.9564),\n",
       "  'past': tensor(0.9507),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-14': {'predicting_date': '2021-01-07',\n",
       "  'pred': tensor(0.9678),\n",
       "  'past': tensor(0.9557),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-15': {'predicting_date': '2021-01-08',\n",
       "  'pred': tensor(0.9769),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-18': {'predicting_date': '2021-01-11',\n",
       "  'pred': tensor(0.9591),\n",
       "  'past': tensor(0.9707),\n",
       "  'tgt': tensor(0.9670)},\n",
       " '2021-01-19': {'predicting_date': '2021-01-12',\n",
       "  'pred': tensor(0.9671),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-20': {'predicting_date': '2021-01-13',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9723),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-21': {'predicting_date': '2021-01-14',\n",
       "  'pred': tensor(0.9775),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-22': {'predicting_date': '2021-01-15',\n",
       "  'pred': tensor(0.9616),\n",
       "  'past': tensor(0.9773),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-01-25': {'predicting_date': '2021-01-18',\n",
       "  'pred': tensor(0.9577),\n",
       "  'past': tensor(0.9747),\n",
       "  'tgt': tensor(1.0050)},\n",
       " '2021-01-26': {'predicting_date': '2021-01-19',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9713),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-01-27': {'predicting_date': '2021-01-20',\n",
       "  'pred': tensor(0.9637),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-28': {'predicting_date': '2021-01-21',\n",
       "  'pred': tensor(0.9625),\n",
       "  'past': tensor(0.9740),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-29': {'predicting_date': '2021-01-22',\n",
       "  'pred': tensor(0.9944),\n",
       "  'past': tensor(0.9783),\n",
       "  'tgt': tensor(0.9720)},\n",
       " '2021-02-01': {'predicting_date': '2021-01-25',\n",
       "  'pred': tensor(1.0058),\n",
       "  'past': tensor(0.9883),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-02-02': {'predicting_date': '2021-01-26',\n",
       "  'pred': tensor(1.0003),\n",
       "  'past': tensor(0.9990),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-03': {'predicting_date': '2021-01-27',\n",
       "  'pred': tensor(0.9802),\n",
       "  'past': tensor(0.9980),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-02-04': {'predicting_date': '2021-01-28',\n",
       "  'pred': tensor(0.9724),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-05': {'predicting_date': '2021-01-29',\n",
       "  'pred': tensor(0.9692),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-08': {'predicting_date': '2021-02-01',\n",
       "  'pred': tensor(0.9851),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-09': {'predicting_date': '2021-02-02',\n",
       "  'pred': tensor(0.9823),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-02-10': {'predicting_date': '2021-02-03',\n",
       "  'pred': tensor(0.9800),\n",
       "  'past': tensor(0.9813),\n",
       "  'tgt': tensor(0.9920)},\n",
       " '2021-02-15': {'predicting_date': '2021-02-04',\n",
       "  'pred': tensor(0.9856),\n",
       "  'past': tensor(0.9790),\n",
       "  'tgt': tensor(0.9940)},\n",
       " '2021-02-16': {'predicting_date': '2021-02-05',\n",
       "  'pred': tensor(0.9774),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-17': {'predicting_date': '2021-02-08',\n",
       "  'pred': tensor(1.0049),\n",
       "  'past': tensor(0.9857),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-18': {'predicting_date': '2021-02-09',\n",
       "  'pred': tensor(0.9871),\n",
       "  'past': tensor(0.9890),\n",
       "  'tgt': tensor(0.9850)},\n",
       " '2021-02-19': {'predicting_date': '2021-02-10',\n",
       "  'pred': tensor(0.9882),\n",
       "  'past': tensor(0.9923),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-22': {'predicting_date': '2021-02-15',\n",
       "  'pred': tensor(0.9979),\n",
       "  'past': tensor(0.9920),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-02-23': {'predicting_date': '2021-02-16',\n",
       "  'pred': tensor(0.9768),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(1.0150)},\n",
       " '2021-02-24': {'predicting_date': '2021-02-17',\n",
       "  'pred': tensor(0.9893),\n",
       "  'past': tensor(0.9853),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-02-25': {'predicting_date': '2021-02-18',\n",
       "  'pred': tensor(0.9911),\n",
       "  'past': tensor(0.9823),\n",
       "  'tgt': tensor(0.9960)},\n",
       " '2021-02-26': {'predicting_date': '2021-02-19',\n",
       "  'pred': tensor(1.0203),\n",
       "  'past': tensor(0.9873),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-03-02': {'predicting_date': '2021-02-22',\n",
       "  'pred': tensor(1.0568),\n",
       "  'past': tensor(1.),\n",
       "  'tgt': tensor(1.0220)},\n",
       " '2021-03-03': {'predicting_date': '2021-02-23',\n",
       "  'pred': tensor(1.0410),\n",
       "  'past': tensor(1.0100),\n",
       "  'tgt': tensor(1.0170)},\n",
       " '2021-03-04': {'predicting_date': '2021-02-24',\n",
       "  'pred': tensor(1.0213),\n",
       "  'past': tensor(1.0123),\n",
       "  'tgt': tensor(1.0350)},\n",
       " '2021-03-05': {'predicting_date': '2021-02-25',\n",
       "  'pred': tensor(0.9976),\n",
       "  'past': tensor(1.0043),\n",
       "  'tgt': tensor(1.0670)},\n",
       " '2021-03-08': {'predicting_date': '2021-02-26',\n",
       "  'pred': tensor(1.0095),\n",
       "  'past': tensor(1.0060),\n",
       "  'tgt': tensor(1.1500)},\n",
       " '2021-03-09': {'predicting_date': '2021-03-02',\n",
       "  'pred': tensor(1.0161),\n",
       "  'past': tensor(1.0127),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-10': {'predicting_date': '2021-03-03',\n",
       "  'pred': tensor(1.0121),\n",
       "  'past': tensor(1.0197),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-11': {'predicting_date': '2021-03-04',\n",
       "  'pred': tensor(1.0291),\n",
       "  'past': tensor(1.0247),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-12': {'predicting_date': '2021-03-05',\n",
       "  'pred': tensor(1.0829),\n",
       "  'past': tensor(1.0397),\n",
       "  'tgt': tensor(1.2300)},\n",
       " '2021-03-15': {'predicting_date': '2021-03-08',\n",
       "  'pred': tensor(1.1399),\n",
       "  'past': tensor(1.0840),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-16': {'predicting_date': '2021-03-09',\n",
       "  'pred': tensor(1.1995),\n",
       "  'past': tensor(1.1457),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-17': {'predicting_date': '2021-03-10',\n",
       "  'pred': tensor(1.2035),\n",
       "  'past': tensor(1.1817),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-03-18': {'predicting_date': '2021-03-11',\n",
       "  'pred': tensor(1.1807),\n",
       "  'past': tensor(1.1900),\n",
       "  'tgt': tensor(1.1370)},\n",
       " '2021-03-19': {'predicting_date': '2021-03-12',\n",
       "  'pred': tensor(1.1893),\n",
       "  'past': tensor(1.1933),\n",
       "  'tgt': tensor(1.1420)},\n",
       " '2021-03-22': {'predicting_date': '2021-03-15',\n",
       "  'pred': tensor(1.2049),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1300)},\n",
       " '2021-03-23': {'predicting_date': '2021-03-16',\n",
       "  'pred': tensor(1.1990),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-03-24': {'predicting_date': '2021-03-17',\n",
       "  'pred': tensor(1.1872),\n",
       "  'past': tensor(1.1907),\n",
       "  'tgt': tensor(1.1200)},\n",
       " '2021-03-25': {'predicting_date': '2021-03-18',\n",
       "  'pred': tensor(1.1615),\n",
       "  'past': tensor(1.1630),\n",
       "  'tgt': tensor(1.0950)},\n",
       " '2021-03-26': {'predicting_date': '2021-03-19',\n",
       "  'pred': tensor(1.1602),\n",
       "  'past': tensor(1.1520),\n",
       "  'tgt': tensor(1.1250)},\n",
       " '2021-03-29': {'predicting_date': '2021-03-22',\n",
       "  'pred': tensor(1.1531),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1150)},\n",
       " '2021-03-30': {'predicting_date': '2021-03-23',\n",
       "  'pred': tensor(1.1640),\n",
       "  'past': tensor(1.1397),\n",
       "  'tgt': tensor(1.1510)},\n",
       " '2021-03-31': {'predicting_date': '2021-03-24',\n",
       "  'pred': tensor(1.1460),\n",
       "  'past': tensor(1.1323),\n",
       "  'tgt': tensor(1.1270)},\n",
       " '2021-04-01': {'predicting_date': '2021-03-25',\n",
       "  'pred': tensor(1.1230),\n",
       "  'past': tensor(1.1207),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-02': {'predicting_date': '2021-03-26',\n",
       "  'pred': tensor(1.1178),\n",
       "  'past': tensor(1.1133),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-04-05': {'predicting_date': '2021-03-29',\n",
       "  'pred': tensor(1.1063),\n",
       "  'past': tensor(1.1117),\n",
       "  'tgt': tensor(1.2020)},\n",
       " '2021-04-06': {'predicting_date': '2021-03-30',\n",
       "  'pred': tensor(1.1355),\n",
       "  'past': tensor(1.1303),\n",
       "  'tgt': tensor(1.1850)},\n",
       " '2021-04-07': {'predicting_date': '2021-03-31',\n",
       "  'pred': tensor(1.1263),\n",
       "  'past': tensor(1.1310),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-04-08': {'predicting_date': '2021-04-01',\n",
       "  'pred': tensor(1.1260),\n",
       "  'past': tensor(1.1377),\n",
       "  'tgt': tensor(1.1600)},\n",
       " '2021-04-09': {'predicting_date': '2021-04-02',\n",
       "  'pred': tensor(1.1282),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1670)},\n",
       " '2021-04-12': {'predicting_date': '2021-04-05',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1613),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-13': {'predicting_date': '2021-04-06',\n",
       "  'pred': tensor(1.1763),\n",
       "  'past': tensor(1.1780),\n",
       "  'tgt': tensor(1.1320)},\n",
       " '2021-04-14': {'predicting_date': '2021-04-07',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1880),\n",
       "  'tgt': tensor(1.1000)},\n",
       " '2021-04-15': {'predicting_date': '2021-04-08',\n",
       "  'pred': tensor(1.1563),\n",
       "  'past': tensor(1.1740),\n",
       "  'tgt': tensor(1.1400)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
