{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도 낮음  \n",
    "제대로 학습이 되지 않음  \n",
    "학습을 지속할 수록 감소 쪽으로 더 몰리거나 왔다가다 하는 경향\n",
    "stride 커지면 관찰 기간 더 길어지면서 감소 쪽으로 더 몰리는 것도 같음\n",
    "dilation 도 그럴 것 같지만 특별히 더 몰리지는 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "#lib_s = '/home/adminuser/public/libs/pytorch_examples/basic_networks'\n",
    "lib_s = '/home/bwlee/work/codes/examples/basic_networks'\n",
    "sys.path.append(lib_s)\n",
    "import generate_model as nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make input data\n",
    "interest.csv has only closed price.   \n",
    "On the other hand, zipline needs OLHCV(?) format for each index  \n",
    "make dummy columns and put 'close' value to them  \n",
    "make csv file for each index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s0 = 'interest.csv'\n",
    "df0 = pd.read_csv(f_s0, index_col='date')\n",
    "df0 = df0.iloc[::-1] # 과거에서 현재로 정렬순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data without time confusion\n",
    "Seperate two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized input, norm val, tgt_date, tgt_val, tgt_diff_ratio, \n",
    "class Environ():\n",
    "    def __init__(self):\n",
    "        self.indices = ['trea3', 'trea5', 'trea10']\n",
    "        #self.tgt_index = 'trea10'\n",
    "        self.tgt_index = 'trea3'\n",
    "        # input window size\n",
    "        self.input_size = 15\n",
    "        # prediction is made after algo.pred_period from the present\n",
    "        self.pred_period = 5\n",
    "\n",
    "env = Environ() # setting values\n",
    "mkt_data = {} # object to store data for learning\n",
    "    \n",
    "def push_data(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read past data and push them to mkt_data\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        if ii_days+1 < env.input_size:\n",
    "            continue\n",
    "        elif ii_days+1 == env.input_size:\n",
    "            env.start_date = data.iloc[ii_days].name\n",
    "        \n",
    "        predicting_date = data.iloc[ii_days].name\n",
    "        # historical data from today to env.input_size behind days\n",
    "        history = data.iloc[ii_days+1-env.input_size:ii_days+1]\n",
    "        mean_val = history.mean() \n",
    "        max_val = history.max()\n",
    "        min_val = history.min()\n",
    "        \n",
    "        range_val = max_val - min_val\n",
    "        norm_history = (history-mean_val)/range_val\n",
    "        mkt_data[predicting_date] = [norm_history, mean_val, range_val, \n",
    "                                     None, None, None, None, None]\n",
    "        \n",
    "            \n",
    "def push_tgt(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read future data and push them to mkt_data\n",
    "    future_prediction_date is a future date when model want to predict\n",
    "    past_predicting_date is a past date when model made a prediction with given information\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        ii_predicting = ii_days - env.pred_period\n",
    "        predicting_date = data.iloc[ii_predicting].name\n",
    "        if ii_predicting < 0 or predicting_date < env.start_date:\n",
    "            continue\n",
    "        \n",
    "        past_history = mkt_data[predicting_date][0]\n",
    "        mean_val = mkt_data[predicting_date][1]\n",
    "        range_val = mkt_data[predicting_date][2]\n",
    "        \n",
    "        past_price = (past_history.iloc[-1]*range_val+mean_val)[env.tgt_index] # 49.6\n",
    "        \n",
    "        #past_price = (past_history.iloc[-5]*range_val+mean_val)[env.tgt_index] # 56.3\n",
    "        \n",
    "        #mkt_data[predicting_date][0] = past_history[:-4]\n",
    "        \n",
    "        #past_price = (past_history.iloc[-3]*range_val+mean_val)[env.tgt_index] # 53.0, # 53.0\n",
    "        #past_price = (past_history.iloc[-13]*range_val+mean_val)[env.tgt_index] # 67.4\n",
    "        #past_price = (past_history*range_val+mean_val).mean()[env.tgt_index] # 58.4\n",
    "        #past_price = (past_history*range_val+mean_val)[:-3].mean()[env.tgt_index] # 63.2\n",
    "        #past_price = (past_history*range_val+mean_val)[:-5].mean()[env.tgt_index] # 63.7\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10]*range_val+mean_val)[env.tgt_index] # 66.4\n",
    "\n",
    "        #past_price = (past_history.iloc[-5:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        #past_price = (past_history.iloc[-3:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10:].mean()*range_val+mean_val)[env.tgt_index] # 55.9\n",
    "        #past_price = (past_history.iloc[-10:-5].mean()*range_val+mean_val)[env.tgt_index] # 60.4\n",
    "        \n",
    "        \n",
    "        prediction_date = data.iloc[ii_days].name\n",
    "        tgt_price = data.iloc[ii_days][env.tgt_index]\n",
    "        #tgt_price = (data.iloc[ii_days-1:ii_days+2].mean())[env.tgt_index]\n",
    "        \n",
    "        tgt_ratio = (tgt_price - past_price)/past_price*10\n",
    "        \n",
    "        mkt_data[predicting_date][3] = prediction_date\n",
    "        mkt_data[predicting_date][4] = np.single(tgt_price)\n",
    "        mkt_data[predicting_date][5] = np.single(tgt_ratio)\n",
    "        \n",
    "        mkt_data[predicting_date][6] = np.single(past_price)\n",
    "        mkt_data[predicting_date][7] = predicting_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_data(env, df0, mkt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check data are pushed correctly\n",
    "* no dates should be ahead of key date\n",
    "* prediction info are not pushed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000-11-11', '2000-11-13', '2000-11-14']\n"
     ]
    }
   ],
   "source": [
    "print(list(mkt_data.keys())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.3000,  0.2721,  0.2940],\n",
       "         [ 0.1060,  0.1314,  0.1522],\n",
       "         [ 0.1060,  0.1314,  0.1522],\n",
       "         [ 0.0910,  0.1240,  0.1522],\n",
       "         [ 0.1507,  0.1758,  0.1837],\n",
       "         [ 0.1060,  0.1240,  0.1365],\n",
       "         [ 0.1284,  0.1240,  0.1050],\n",
       "         [ 0.1582,  0.1610,  0.1365],\n",
       "         [ 0.1582,  0.1610,  0.1365],\n",
       "         [ 0.1881,  0.1980,  0.1759],\n",
       "         [ 0.1881,  0.2054,  0.1759],\n",
       "         [ 0.0761,  0.0721,  0.0499],\n",
       "         [-0.3567, -0.4242, -0.4383],\n",
       "         [-0.7000, -0.7279, -0.7060],\n",
       "         [-0.7000, -0.7279, -0.7060]]),\n",
       " tensor([7.6640, 7.8663, 8.1433]),\n",
       " tensor([0.6700, 0.6750, 0.6350]),\n",
       " '2000-11-17',\n",
       " 7.275,\n",
       " 0.004584002,\n",
       " 7.2716665,\n",
       " '2000-11-11',\n",
       " tensor([-0.8881, -0.9333, -0.8819, -0.8582, -0.8889, -0.8425, -0.8060, -0.8519,\n",
       "         -0.8425, -0.3433, -0.3037, -0.2677, -0.5149, -0.5852, -0.5748, -0.4627,\n",
       "         -0.5481, -0.5748,  0.0299,  0.0444,  0.0394,  0.0821,  0.0815,  0.0394])]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkt_data['2000-11-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-11-06'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_tgt(env, df0, mkt_data)\n",
    "mkt_data = {key: val for key, val in mkt_data.items() if (val[-1] is not None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHECK tgt push is correctly made\n",
    "end value of '2000-11-23' * norm == prediction value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-10-26  0.300000  0.272099  0.293963\n",
      "2000-10-27  0.105970  0.131358  0.152231\n",
      "2000-10-28  0.105970  0.131358  0.152231\n",
      "2000-10-30  0.091045  0.123951  0.152231\n",
      "2000-10-31  0.150746  0.175802  0.183727\n",
      "2000-11-01  0.105970  0.123951  0.136483\n",
      "2000-11-02  0.128358  0.123951  0.104987\n",
      "2000-11-03  0.158209  0.160988  0.136483\n",
      "2000-11-04  0.158209  0.160988  0.136483\n",
      "2000-11-06  0.188060  0.198025  0.175853\n",
      "2000-11-07  0.188060  0.205432  0.175853\n",
      "2000-11-08  0.076119  0.072099  0.049869\n",
      "2000-11-09 -0.356716 -0.424198 -0.438320\n",
      "2000-11-10 -0.700000 -0.727901 -0.706037\n",
      "2000-11-11 -0.700000 -0.727901 -0.706037, trea3     7.664000\n",
      "trea5     7.866333\n",
      "trea10    8.143333\n",
      "dtype: float64, trea3     0.670\n",
      "trea5     0.675\n",
      "trea10    0.635\n",
      "dtype: float64, '2000-12-05', 7.07, -1.0108074]\n",
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-11-07  0.681934  0.687143  0.711795\n",
      "2000-11-08  0.567430  0.558571  0.588718\n",
      "2000-11-09  0.124682  0.080000  0.111795\n",
      "2000-11-10 -0.226463 -0.212857 -0.149744\n",
      "2000-11-11 -0.226463 -0.212857 -0.149744\n",
      "2000-11-13 -0.318066 -0.305714 -0.288205\n",
      "2000-11-14 -0.287532 -0.312857 -0.288205\n",
      "2000-11-15 -0.150127 -0.191429 -0.172821\n",
      "2000-11-16 -0.081425 -0.120000 -0.134359\n",
      "2000-11-17 -0.104326 -0.141429 -0.157436\n",
      "2000-11-18 -0.104326 -0.141429 -0.157436\n",
      "2000-11-20 -0.073791 -0.070000 -0.095897\n",
      "2000-11-21  0.117048  0.151429  0.104103\n",
      "2000-11-22  0.017812  0.072857  0.011795\n",
      "2000-11-23  0.063613  0.158571  0.065641, trea3     7.343333\n",
      "trea5     7.524000\n",
      "trea10    7.792333\n",
      "dtype: float64, trea3     0.655\n",
      "trea5     0.700\n",
      "trea10    0.650\n",
      "dtype: float64, '2000-12-16', 7.11, -0.9541985]\n"
     ]
    }
   ],
   "source": [
    "print(mkt_data['2000-11-11'])\n",
    "print(mkt_data['2000-11-23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for Conv1d\n",
    "In RNN, inputs are given as (time, indices)  \n",
    "In Conv1d, indices are given as channel, which need to (channel: indices, features: time)  \n",
    "(batch, channel=indices, data_size=seq_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_data(Dataset):\n",
    "    def __init__(self, in_data, dates):\n",
    "        \"\"\"\n",
    "        :param in_data: dict of (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        # change dataframe to float numpy array\n",
    "        for key, val in in_data.items():\n",
    "            if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "                in_data_temp = torch.tensor(in_data[key][0].to_numpy(np.single))\n",
    "                print('in_temp', in_data_temp.size())\n",
    "                in_data[key][0] = torch.tensor(np.transpose(in_data[key][0].to_numpy(np.single)))\n",
    "                in_data[key][1] = torch.tensor(np.transpose(in_data[key][1].to_numpy(np.single)))\n",
    "                in_data[key][2] = torch.tensor(np.transpose(in_data[key][2].to_numpy(np.single)))\n",
    "                ## add difference\n",
    "                diff = in_data_temp[-1] - in_data_temp[-5]\n",
    "                diff2 = in_data_temp[-1] - in_data_temp[-7]\n",
    "                diff3 = in_data_temp[-1] - in_data_temp[-10]\n",
    "                diff4 = in_data_temp[-1] - in_data_temp[-3]\n",
    "                diff5 = in_data_temp[-3] - in_data_temp[-7]\n",
    "                diff6 = in_data_temp[-3] - in_data_temp[-10]\n",
    "                diff7 = in_data_temp[-5] - in_data_temp[-7]\n",
    "                diff8 = in_data_temp[-5] - in_data_temp[-10]\n",
    "                #diff = torch.cat((diff, diff2, diff3), dim=-1)\n",
    "                diff = torch.cat((diff, diff2, diff3, diff4, diff5, diff6, diff7, diff8), dim=-1)\n",
    "                #print('in_temp2', in_data_temp.size(), 'diff', diff.size(), diff[:3])\n",
    "                in_data[key].append(diff)\n",
    "        self.in_data = in_data\n",
    "        self.i2dates = {}\n",
    "        for ii, date1 in enumerate(dates):\n",
    "             self.i2dates[ii] = date1\n",
    "        \n",
    "    def __getitem__(self, ii_date):\n",
    "        \"\"\"\n",
    "        :return: (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        predicting_date = self.i2dates[ii_date]\n",
    "        return self.in_data[predicting_date]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase data that do not have prediction target\n",
    "traintest_dates = sorted(list(mkt_data.keys()))\n",
    "n_train = len(traintest_dates)*8//10\n",
    "n_test = len(traintest_dates) - n_train\n",
    "train_dates = traintest_dates[:n_train]\n",
    "test_dates = traintest_dates[n_train:]\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for key, val in mkt_data.items():\n",
    "    if key in train_dates:\n",
    "        train_data[key] = val\n",
    "    elif key in test_dates:\n",
    "        test_data[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            trea3  trea5    trea10\n",
       " date                              \n",
       " 2020-12-24 -0.296  0.180 -0.375758\n",
       " 2020-12-28 -0.196 -0.145 -0.648485\n",
       " 2020-12-29  0.264  0.230 -0.284848\n",
       " 2020-12-30  0.064  0.430  0.169697\n",
       " 2020-12-31  0.064  0.430  0.169697\n",
       " 2021-01-04 -0.236  0.180  0.351515\n",
       " 2021-01-05 -0.596 -0.570 -0.321212\n",
       " 2021-01-06 -0.136  0.105  0.351515\n",
       " 2021-01-07  0.064  0.105  0.133333\n",
       " 2021-01-08  0.204  0.080  0.133333\n",
       " 2021-01-11 -0.036 -0.570 -0.503030\n",
       " 2021-01-12  0.204 -0.170  0.078788\n",
       " 2021-01-13  0.164 -0.270  0.078788\n",
       " 2021-01-14  0.404  0.180  0.351515\n",
       " 2021-01-15  0.064 -0.195  0.315152,\n",
       " trea3     0.966800\n",
       " trea5     1.317800\n",
       " trea10    1.712667\n",
       " dtype: float64,\n",
       " trea3     0.050\n",
       " trea5     0.040\n",
       " trea10    0.055\n",
       " dtype: float64,\n",
       " '2021-01-22',\n",
       " 0.99,\n",
       " 0.20618556,\n",
       " 0.97,\n",
       " '2021-01-15']"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['2021-01-15']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dset = Index_data(train_data, train_dates)\n",
    "test_dset = Index_data(test_data, test_dates)\n",
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_dset[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ii in range(len(train_dset)):\n",
    "    if type(train_dset[ii][-1]) is not torch.Tensor:\n",
    "        print(train_dset[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor(nets.Net):\n",
    "    def __init__(self, net=None, downnet=None, drop=0.35,\n",
    "                 loss=None, optimizer=None, device='cuda:2',\n",
    "                 dim_hiddens=[100,200,300,50], seq_size = 15, n_kernels=[3,4,5], \n",
    "                 hidden_out=None, down_hiddens=[1, 1], add_down=-1,\n",
    "                 dilations=None, pads=None, strides=None):\n",
    "        \"\"\"\n",
    "        net is consist of [embed, net, downnet]\n",
    "        dilation will be treated in another class\n",
    "        initial seq_width, which corresponds to kernel_size, is 15(time window size)\n",
    "        :param downnet: define downstream job\n",
    "        :down_hiddens: multiply out_dimension of convolution\n",
    "        \"\"\"\n",
    "        assert len(dim_hiddens)-1 == len(n_kernels)\n",
    "        if dilations is None:\n",
    "            dilations = [1 for _ in range(len(dim_hiddens))]\n",
    "        if pads is None:\n",
    "            pads = [0 for _ in range(len(dim_hiddens))]\n",
    "        if strides is None:\n",
    "            strides = [1 for _ in range(len(dim_hiddens))]\n",
    "        \n",
    "        super(Predictor, self).__init__(loss=loss, device=device)\n",
    "        \n",
    "        if net is None:\n",
    "            self.convs = []\n",
    "            relu = nn.ReLU()\n",
    "            dropout = nn.Dropout(drop)\n",
    "            # we do not care padding, stride, dilation\n",
    "            out_seq_size = seq_size\n",
    "            for ii, dim_hidden in enumerate(dim_hiddens):\n",
    "                if ii==0:\n",
    "                    continue\n",
    "                \n",
    "                n_kernel = n_kernels[ii-1]\n",
    "                in_channel = dim_hiddens[ii-1]\n",
    "                out_channel = dim_hiddens[ii]\n",
    "                stride = strides[ii-1]\n",
    "                pad = pads[ii-1]\n",
    "                dilation = dilations[ii-1]\n",
    "                \n",
    "                conv1 = nn.Conv1d(in_channels=in_channel, \n",
    "                                  out_channels=out_channel,\n",
    "                                  kernel_size=n_kernel,\n",
    "                                  stride=stride,\n",
    "                                  padding=pad,\n",
    "                                  dilation=dilation\n",
    "                                 )\n",
    "                       \n",
    "                out_seq_size0 = out_seq_size\n",
    "                out_seq_size = (out_seq_size + 2*pad - dilation*(n_kernel-1)-1)//stride + 1\n",
    "                #print(n_kernel, 'in', out_seq_size0, 'out', out_seq_size)\n",
    "                self.convs.append(conv1)\n",
    "            \n",
    "            last_n_channel = dim_hiddens[-1]\n",
    "            self.hidden_out =  last_n_channel * out_seq_size\n",
    "            conv_nets = []\n",
    "            for conv1 in self.convs:\n",
    "                conv_nets.append(dropout)\n",
    "                conv_nets.append(conv1)\n",
    "                conv_nets.append(relu)\n",
    "            self.net = nn.Sequential(*conv_nets)\n",
    "        else:\n",
    "            assert hidden_out is not None\n",
    "            self.hidden_out = dim_out\n",
    "            self.net = net\n",
    "            \n",
    "        if downnet is None:\n",
    "            down_dims = [self.hidden_out*x for x in down_hiddens] + [1]\n",
    "            if add_down>0:\n",
    "                down_dims[0] += add_down\n",
    "            self.downnet = nets.get_MLP(down_dims, dropout=0.35, end=True)\n",
    "        else:\n",
    "            self.downnet = downnet\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.net = self.net.to(device)\n",
    "        self.downnet = self.downnet.to(device)\n",
    "        \n",
    "        parms = list(self.net.parameters())\n",
    "        parms += list(self.downnet.parameters())\n",
    "        self.optimizer = optimizer(parms)\n",
    "        \n",
    "    def set_train(self):\n",
    "        self.net.train()\n",
    "        self.downnet.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        self.net.eval()\n",
    "        self.downnet.eval()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for ii, layer in enumerate(self.net):\n",
    "            if 'Conv' in str(layer):\n",
    "                torch.nn.init.xavier_uniform_(self.net[ii].weight)\n",
    "        for ii, layer in enumerate(self.downnet):\n",
    "            if 'Linear' in str(layer):\n",
    "                torch.nn.init.xavier_uniform_(self.downnet[ii].weight)\n",
    "                #torch.nn.init.xavier_normal_(self.downnet[ii].weight)\n",
    "\n",
    "    def forward(self, x, diff=None):\n",
    "        out = self.net(x)\n",
    "        dim_batch = out.shape[0]\n",
    "        out = out.reshape(dim_batch, -1) # concatenate all data into 1D data\n",
    "        #print('out size', out.size(), 'diff size', diff.size())\n",
    "        if diff is not None:\n",
    "            out = torch.cat((out, diff),dim=1)\n",
    "        out = self.downnet(out) \n",
    "        return out.view(-1)\n",
    "\n",
    "    def run_eval(self, data):\n",
    "        self.set_eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        tgts = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in = data_batch[0]\n",
    "                mean_val = data_batch[1]\n",
    "                range_val = data_batch[2]\n",
    "                pred_date = data_batch[3]\n",
    "                tgt = data_batch[5]\n",
    "                diff = data_batch[-1]\n",
    "                \n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                diff = diff.to(self.device)\n",
    "                out = self.forward(data_in, diff)\n",
    "                #print('loss', loss, self.loss)\n",
    "                loss += self.loss(out, tgt).cpu().numpy()\n",
    "                out = out.cpu().numpy()\n",
    "                #print('loss===', loss)\n",
    "                tgt = tgt.cpu().numpy()\n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                    tgts = tgt\n",
    "                else:\n",
    "                    outs = np.concatenate((outs, out), axis=0)\n",
    "                    tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "        loss /= 1.0*(i_batch+1)\n",
    "        print('evaluate', 'loss', loss, 'accuracy : define function')\n",
    "        return outs, tgts, loss\n",
    "    \n",
    "    def run_batch(self, i_batch, data_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in = data_batch[0]\n",
    "        mean_val = data_batch[1]\n",
    "        range_val = data_batch[2]\n",
    "        pred_date = data_batch[3]\n",
    "        tgt = data_batch[5]\n",
    "        diff = data_batch[-1]\n",
    "        #for val in diff:\n",
    "        #    if type(val) is not torch.Tensor:\n",
    "        #        print(diff)\n",
    "        data_in = data_in.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        #print('diff-----------', diff)\n",
    "        diff = diff.to(self.device)\n",
    "        out = self.forward(data_in, diff)\n",
    "        \n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_size = train_dset[0][0].shape[-1]\n",
    "# Add difference variables\n",
    "n_features = train_dset[0][0].shape[0]\n",
    "down_hiddens = [1,2,2]\n",
    "n_kernels=[3,3,3]\n",
    "#dilations=[1,2,2]\n",
    "#strides=[1,2,3]\n",
    "strides=[1,2,2]\n",
    "dim_hiddens = [n_features] + [50, 50, 50]\n",
    "loss = nn.MSELoss() # which combines logsoftmax and nll loss\n",
    "optimizer = optim.Adam\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(loss=loss, optimizer=optimizer, \n",
    "                      device=device, seq_size= seq_size,\n",
    "                      dim_hiddens=dim_hiddens, down_hiddens=down_hiddens, \n",
    "                      n_kernels=n_kernels, add_down=3*8,\n",
    "                      strides=strides,\n",
    "                      dilations=dilations,\n",
    "                     )\n",
    "                      #dilations=dilations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.09203722945122576\n",
      "eval_train evaluate loss 0.09066397496568623 accuracy : define function\n",
      "epoch 1 loss 0.09190750016427751\n",
      "eval_train evaluate loss 0.09029556427206566 accuracy : define function\n",
      "epoch 2 loss 0.09171775250292535\n",
      "eval_train evaluate loss 0.08994335914725687 accuracy : define function\n",
      "epoch 3 loss 0.09239515145100764\n",
      "eval_train evaluate loss 0.09006188353940622 accuracy : define function\n",
      "epoch 4 loss 0.09180346031242342\n",
      "eval_train evaluate loss 0.09027221443048164 accuracy : define function\n",
      "epoch 5 loss 0.09100315041506468\n",
      "eval_train evaluate loss 0.0896626860459349 accuracy : define function\n",
      "epoch 6 loss 0.09199910744357465\n",
      "eval_train evaluate loss 0.09026742220592143 accuracy : define function\n",
      "epoch 7 loss 0.09119554981589317\n",
      "eval_train evaluate loss 0.08967764119603741 accuracy : define function\n",
      "epoch 8 loss 0.09149711412280354\n",
      "eval_train evaluate loss 0.08981080553424892 accuracy : define function\n",
      "epoch 9 loss 0.09211227186580202\n",
      "eval_train evaluate loss 0.09019465950220379 accuracy : define function\n"
     ]
    }
   ],
   "source": [
    "predictor.run_train(n_epoch=10, data=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = {}\n",
    "tgts = None\n",
    "preds = None\n",
    "with torch.no_grad():\n",
    "    for data_batch in test_loader:\n",
    "        data_in = data_batch[0]\n",
    "        tgt = data_batch[5]\n",
    "        diff = data_batch[-1]\n",
    "        \n",
    "        data_in = data_in.to(device)\n",
    "        diff = diff.to(device)\n",
    "        tgt = tgt.numpy()\n",
    "        #out = predictor.forward(data_in).cpu().numpy()\n",
    "        out = predictor.forward(data_in, diff).cpu().numpy()\n",
    "        if preds is None:\n",
    "            preds = out\n",
    "            tgts = tgt\n",
    "        else:\n",
    "            preds = np.concatenate((preds, out), axis=0)\n",
    "            tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "            \n",
    "        prediction_dates = data_batch[3]\n",
    "        past_prices = data_batch[6]\n",
    "        predicting_dates = data_batch[7]\n",
    "        for ii, date1 in enumerate(prediction_dates):\n",
    "            out1 = out[ii]\n",
    "            out_orig = out1/10*past_prices[ii] + past_prices[ii]\n",
    "            rets[date1] = {'predicting_date': predicting_dates[ii],\n",
    "                          'pred': out_orig,\n",
    "                           'past': past_prices[ii],\n",
    "                           'tgt': data_batch[4][ii]}\n",
    "            \n",
    "preds = preds.reshape(-1)\n",
    "tgts = tgts.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 482\n",
      "495 570\n",
      "1070 573 0.5355140186915888\n"
     ]
    }
   ],
   "source": [
    "sum1 = np.sum((preds > 0) & (tgts > 0))\n",
    "sum2 = np.sum((preds < 0) & (tgts < 0))\n",
    "print(sum1, np.sum(tgts>0))\n",
    "print(sum2, np.sum(tgts<0))\n",
    "print(len(preds), sum1+sum2, 1.0*(sum1+sum2)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 44\n",
      "19 24\n",
      "71 30 0.4225352112676056\n"
     ]
    }
   ],
   "source": [
    "limit = list(rets.keys()).index('2021-01-04')\n",
    "sum1 = np.sum((preds[limit:] > 0) & (tgts[limit:] > 0))\n",
    "sum2 = np.sum((preds[limit:] < 0) & (tgts[limit:] < 0))\n",
    "print(sum1, np.sum(tgts[limit:]>0))\n",
    "print(sum2, np.sum(tgts[limit:]<0))\n",
    "print(len(preds[limit:]), sum1+sum2, 1.0*(sum1+sum2)/len(preds[limit:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## all includes 3*3=9 diff, predict 4 days avg, prediction_period=10\n",
    "\n",
    "lstm_layer=2  \n",
    "epoch 30, dropout=0, dim_hidden=100, mlp (dim_hidden, dim_hidden, 1) --> 55.5%  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, 1) --> 55.3%  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden*2, 1) --> 53.4%  \n",
    "\n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden//2, 1) --> 53.4%  \n",
    "epoch 20, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden//2, 1) --> 53.9%  \n",
    "\n",
    "epoch 20, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 48.8%  \n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 51.4%  \n",
    "epoch 40, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 53.7%  \n",
    "epoch 50, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 49.1%  \n",
    "epoch 60, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 48.7%  \n",
    "epoch 70, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 50.3%  \n",
    "\n",
    "\n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.2%  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.0%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 56.9%, 54.3%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.9%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.3%  \n",
    "epoch 60, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 48.6%, 49.6  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.2  \n",
    "epoch 40, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.8  \n",
    "epoch 50, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.8  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=500, mlp (dim_hidden, dim_hidden, 1) --> 55.8  \n",
    "epoch 40, dropout=0, dim_hidden=500, mlp (dim_hidden, dim_hidden, 1) --> 53.8  \n",
    "\n",
    "lstm_layer=1\n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.3%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.7%  \n",
    "\n",
    "lstm_layer=3\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.7%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.4%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.8%  \n",
    "\n",
    "## includes 3*8=24 diff\n",
    "layer=2\n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.5%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.9%  \n",
    "\n",
    "## includes 3*8=24 diff, 1day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.8%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below prediction days are (-2,-1,0)\n",
    "## includes 3*8=24 diff, 3day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.7%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.5%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=5\n",
    "layer=2\n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.8%  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 62.6%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.5%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.3%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=15\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=20\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.7%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.1%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.1%  \n",
    "epoch 60, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 51.4%  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  includes 3*8=24 diff, last price, 3day prediction, layer=2\n",
    "epoch 5, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 51.7%  \n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 56.4%, 54.8, 54.6  \n",
    "epoch 10, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.7  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.5%, 53.9 \n",
    "epoch 20, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 49.3% \n",
    "epoch 30, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.2% \n",
    "epoch 40, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.5% \n",
    "\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.5  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.3  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9  \n",
    "\n",
    "## Bidirectional\n",
    "epoch 10, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.9 \n",
    "epoch 20, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.2 \n",
    "epoch 30, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.5 \n",
    "epoch 40, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.5 \n",
    "epoch 50, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.8 \n",
    "\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 56.7  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 54.1 \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.1 \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 56.4 \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.0 \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 54.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to last period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.0\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.2\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.1\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to last period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.6\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.1\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.5\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.1\n",
    "\n",
    "## last to last period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.3, 51.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.2, 52.8\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.8, 54.5\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.2, 52.1\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.5, 50.7\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.3, 52.2\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.1, 48.4\n",
    "\n",
    "## last to last period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.7\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.1\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 48.6\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 47.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 47.0  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.1  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.0  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.4  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4  \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.3  \n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.5  \n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.2  \n",
    "epoch 90, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.2\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.6\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.4\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.5\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.1\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.7\n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.9\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.0\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.0\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.6\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last to pred3 period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.4\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.8\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.4\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.3\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pred3 to last period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.0  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 60.1  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.9  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 60.5  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.0  \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.5  \n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.3  \n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.6  \n",
    "\n",
    "## pred3 to last period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.1\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.8\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.6\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.7\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.9\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.5\n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.1\n",
    "\n",
    "## pred3 to last period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.9\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.9\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.6\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.0\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.9\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.3\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.5\n",
    "\n",
    "## pred3 to last    period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.8\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.1\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.2\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021 = {key: val for key, val in rets.items() if key>'2021-01-01'}\n",
    "rows = list(rets_2021['2021-01-04'].keys())\n",
    "cols = list(rets_2021.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021b = {}\n",
    "for col in cols:\n",
    "    for row in rows:\n",
    "        rets_2021b.setdefault(row, {})\n",
    "        val = rets_2021[col][row]\n",
    "        if type(val) is not str:\n",
    "            rets_2021b[row][col] = rets_2021[col][row].item()        \n",
    "        else:\n",
    "            rets_2021b[row][col] = rets_2021[col][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rets_2021b).to_csv('./rets.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021-01-04': {'predicting_date': '2020-12-24',\n",
       "  'pred': tensor(0.9607),\n",
       "  'past': tensor(0.9403),\n",
       "  'tgt': tensor(0.9550)},\n",
       " '2021-01-05': {'predicting_date': '2020-12-28',\n",
       "  'pred': tensor(0.9542),\n",
       "  'past': tensor(0.9453),\n",
       "  'tgt': tensor(0.9370)},\n",
       " '2021-01-06': {'predicting_date': '2020-12-29',\n",
       "  'pred': tensor(0.9781),\n",
       "  'past': tensor(0.9630),\n",
       "  'tgt': tensor(0.9600)},\n",
       " '2021-01-07': {'predicting_date': '2020-12-30',\n",
       "  'pred': tensor(0.9640),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-08': {'predicting_date': '2020-12-31',\n",
       "  'pred': tensor(0.9614),\n",
       "  'past': tensor(0.9733),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-11': {'predicting_date': '2021-01-04',\n",
       "  'pred': tensor(0.9480),\n",
       "  'past': tensor(0.9650),\n",
       "  'tgt': tensor(0.9650)},\n",
       " '2021-01-12': {'predicting_date': '2021-01-05',\n",
       "  'pred': tensor(0.9365),\n",
       "  'past': tensor(0.9540),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-13': {'predicting_date': '2021-01-06',\n",
       "  'pred': tensor(0.9564),\n",
       "  'past': tensor(0.9507),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-14': {'predicting_date': '2021-01-07',\n",
       "  'pred': tensor(0.9678),\n",
       "  'past': tensor(0.9557),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-15': {'predicting_date': '2021-01-08',\n",
       "  'pred': tensor(0.9769),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-18': {'predicting_date': '2021-01-11',\n",
       "  'pred': tensor(0.9591),\n",
       "  'past': tensor(0.9707),\n",
       "  'tgt': tensor(0.9670)},\n",
       " '2021-01-19': {'predicting_date': '2021-01-12',\n",
       "  'pred': tensor(0.9671),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-20': {'predicting_date': '2021-01-13',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9723),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-21': {'predicting_date': '2021-01-14',\n",
       "  'pred': tensor(0.9775),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-22': {'predicting_date': '2021-01-15',\n",
       "  'pred': tensor(0.9616),\n",
       "  'past': tensor(0.9773),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-01-25': {'predicting_date': '2021-01-18',\n",
       "  'pred': tensor(0.9577),\n",
       "  'past': tensor(0.9747),\n",
       "  'tgt': tensor(1.0050)},\n",
       " '2021-01-26': {'predicting_date': '2021-01-19',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9713),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-01-27': {'predicting_date': '2021-01-20',\n",
       "  'pred': tensor(0.9637),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-28': {'predicting_date': '2021-01-21',\n",
       "  'pred': tensor(0.9625),\n",
       "  'past': tensor(0.9740),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-29': {'predicting_date': '2021-01-22',\n",
       "  'pred': tensor(0.9944),\n",
       "  'past': tensor(0.9783),\n",
       "  'tgt': tensor(0.9720)},\n",
       " '2021-02-01': {'predicting_date': '2021-01-25',\n",
       "  'pred': tensor(1.0058),\n",
       "  'past': tensor(0.9883),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-02-02': {'predicting_date': '2021-01-26',\n",
       "  'pred': tensor(1.0003),\n",
       "  'past': tensor(0.9990),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-03': {'predicting_date': '2021-01-27',\n",
       "  'pred': tensor(0.9802),\n",
       "  'past': tensor(0.9980),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-02-04': {'predicting_date': '2021-01-28',\n",
       "  'pred': tensor(0.9724),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-05': {'predicting_date': '2021-01-29',\n",
       "  'pred': tensor(0.9692),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-08': {'predicting_date': '2021-02-01',\n",
       "  'pred': tensor(0.9851),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-09': {'predicting_date': '2021-02-02',\n",
       "  'pred': tensor(0.9823),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-02-10': {'predicting_date': '2021-02-03',\n",
       "  'pred': tensor(0.9800),\n",
       "  'past': tensor(0.9813),\n",
       "  'tgt': tensor(0.9920)},\n",
       " '2021-02-15': {'predicting_date': '2021-02-04',\n",
       "  'pred': tensor(0.9856),\n",
       "  'past': tensor(0.9790),\n",
       "  'tgt': tensor(0.9940)},\n",
       " '2021-02-16': {'predicting_date': '2021-02-05',\n",
       "  'pred': tensor(0.9774),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-17': {'predicting_date': '2021-02-08',\n",
       "  'pred': tensor(1.0049),\n",
       "  'past': tensor(0.9857),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-18': {'predicting_date': '2021-02-09',\n",
       "  'pred': tensor(0.9871),\n",
       "  'past': tensor(0.9890),\n",
       "  'tgt': tensor(0.9850)},\n",
       " '2021-02-19': {'predicting_date': '2021-02-10',\n",
       "  'pred': tensor(0.9882),\n",
       "  'past': tensor(0.9923),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-22': {'predicting_date': '2021-02-15',\n",
       "  'pred': tensor(0.9979),\n",
       "  'past': tensor(0.9920),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-02-23': {'predicting_date': '2021-02-16',\n",
       "  'pred': tensor(0.9768),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(1.0150)},\n",
       " '2021-02-24': {'predicting_date': '2021-02-17',\n",
       "  'pred': tensor(0.9893),\n",
       "  'past': tensor(0.9853),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-02-25': {'predicting_date': '2021-02-18',\n",
       "  'pred': tensor(0.9911),\n",
       "  'past': tensor(0.9823),\n",
       "  'tgt': tensor(0.9960)},\n",
       " '2021-02-26': {'predicting_date': '2021-02-19',\n",
       "  'pred': tensor(1.0203),\n",
       "  'past': tensor(0.9873),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-03-02': {'predicting_date': '2021-02-22',\n",
       "  'pred': tensor(1.0568),\n",
       "  'past': tensor(1.),\n",
       "  'tgt': tensor(1.0220)},\n",
       " '2021-03-03': {'predicting_date': '2021-02-23',\n",
       "  'pred': tensor(1.0410),\n",
       "  'past': tensor(1.0100),\n",
       "  'tgt': tensor(1.0170)},\n",
       " '2021-03-04': {'predicting_date': '2021-02-24',\n",
       "  'pred': tensor(1.0213),\n",
       "  'past': tensor(1.0123),\n",
       "  'tgt': tensor(1.0350)},\n",
       " '2021-03-05': {'predicting_date': '2021-02-25',\n",
       "  'pred': tensor(0.9976),\n",
       "  'past': tensor(1.0043),\n",
       "  'tgt': tensor(1.0670)},\n",
       " '2021-03-08': {'predicting_date': '2021-02-26',\n",
       "  'pred': tensor(1.0095),\n",
       "  'past': tensor(1.0060),\n",
       "  'tgt': tensor(1.1500)},\n",
       " '2021-03-09': {'predicting_date': '2021-03-02',\n",
       "  'pred': tensor(1.0161),\n",
       "  'past': tensor(1.0127),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-10': {'predicting_date': '2021-03-03',\n",
       "  'pred': tensor(1.0121),\n",
       "  'past': tensor(1.0197),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-11': {'predicting_date': '2021-03-04',\n",
       "  'pred': tensor(1.0291),\n",
       "  'past': tensor(1.0247),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-12': {'predicting_date': '2021-03-05',\n",
       "  'pred': tensor(1.0829),\n",
       "  'past': tensor(1.0397),\n",
       "  'tgt': tensor(1.2300)},\n",
       " '2021-03-15': {'predicting_date': '2021-03-08',\n",
       "  'pred': tensor(1.1399),\n",
       "  'past': tensor(1.0840),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-16': {'predicting_date': '2021-03-09',\n",
       "  'pred': tensor(1.1995),\n",
       "  'past': tensor(1.1457),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-17': {'predicting_date': '2021-03-10',\n",
       "  'pred': tensor(1.2035),\n",
       "  'past': tensor(1.1817),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-03-18': {'predicting_date': '2021-03-11',\n",
       "  'pred': tensor(1.1807),\n",
       "  'past': tensor(1.1900),\n",
       "  'tgt': tensor(1.1370)},\n",
       " '2021-03-19': {'predicting_date': '2021-03-12',\n",
       "  'pred': tensor(1.1893),\n",
       "  'past': tensor(1.1933),\n",
       "  'tgt': tensor(1.1420)},\n",
       " '2021-03-22': {'predicting_date': '2021-03-15',\n",
       "  'pred': tensor(1.2049),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1300)},\n",
       " '2021-03-23': {'predicting_date': '2021-03-16',\n",
       "  'pred': tensor(1.1990),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-03-24': {'predicting_date': '2021-03-17',\n",
       "  'pred': tensor(1.1872),\n",
       "  'past': tensor(1.1907),\n",
       "  'tgt': tensor(1.1200)},\n",
       " '2021-03-25': {'predicting_date': '2021-03-18',\n",
       "  'pred': tensor(1.1615),\n",
       "  'past': tensor(1.1630),\n",
       "  'tgt': tensor(1.0950)},\n",
       " '2021-03-26': {'predicting_date': '2021-03-19',\n",
       "  'pred': tensor(1.1602),\n",
       "  'past': tensor(1.1520),\n",
       "  'tgt': tensor(1.1250)},\n",
       " '2021-03-29': {'predicting_date': '2021-03-22',\n",
       "  'pred': tensor(1.1531),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1150)},\n",
       " '2021-03-30': {'predicting_date': '2021-03-23',\n",
       "  'pred': tensor(1.1640),\n",
       "  'past': tensor(1.1397),\n",
       "  'tgt': tensor(1.1510)},\n",
       " '2021-03-31': {'predicting_date': '2021-03-24',\n",
       "  'pred': tensor(1.1460),\n",
       "  'past': tensor(1.1323),\n",
       "  'tgt': tensor(1.1270)},\n",
       " '2021-04-01': {'predicting_date': '2021-03-25',\n",
       "  'pred': tensor(1.1230),\n",
       "  'past': tensor(1.1207),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-02': {'predicting_date': '2021-03-26',\n",
       "  'pred': tensor(1.1178),\n",
       "  'past': tensor(1.1133),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-04-05': {'predicting_date': '2021-03-29',\n",
       "  'pred': tensor(1.1063),\n",
       "  'past': tensor(1.1117),\n",
       "  'tgt': tensor(1.2020)},\n",
       " '2021-04-06': {'predicting_date': '2021-03-30',\n",
       "  'pred': tensor(1.1355),\n",
       "  'past': tensor(1.1303),\n",
       "  'tgt': tensor(1.1850)},\n",
       " '2021-04-07': {'predicting_date': '2021-03-31',\n",
       "  'pred': tensor(1.1263),\n",
       "  'past': tensor(1.1310),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-04-08': {'predicting_date': '2021-04-01',\n",
       "  'pred': tensor(1.1260),\n",
       "  'past': tensor(1.1377),\n",
       "  'tgt': tensor(1.1600)},\n",
       " '2021-04-09': {'predicting_date': '2021-04-02',\n",
       "  'pred': tensor(1.1282),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1670)},\n",
       " '2021-04-12': {'predicting_date': '2021-04-05',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1613),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-13': {'predicting_date': '2021-04-06',\n",
       "  'pred': tensor(1.1763),\n",
       "  'past': tensor(1.1780),\n",
       "  'tgt': tensor(1.1320)},\n",
       " '2021-04-14': {'predicting_date': '2021-04-07',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1880),\n",
       "  'tgt': tensor(1.1000)},\n",
       " '2021-04-15': {'predicting_date': '2021-04-08',\n",
       "  'pred': tensor(1.1563),\n",
       "  'past': tensor(1.1740),\n",
       "  'tgt': tensor(1.1400)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
