{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crucial-default",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "twenty-ballet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '/home/adminuser/.local/lib/python3.6/site-packages',\n",
       " '/opt/conda/envs/python/lib/python36.zip',\n",
       " '/opt/conda/envs/python/lib/python3.6',\n",
       " '/opt/conda/envs/python/lib/python3.6/lib-dynload',\n",
       " '/opt/conda/envs/python/lib/python3.6/site-packages',\n",
       " '/opt/conda/envs/python/lib/python3.6/site-packages/IPython/extensions',\n",
       " '/home/adminuser/.ipython']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "light-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "colored-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "lib_s = '/home/adminuser/public/libs/pytorch_examples/basic_networks'\n",
    "sys.path.append(lib_s)\n",
    "import generate_model as nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-cricket",
   "metadata": {},
   "source": [
    "## Make input data\n",
    "interest.csv has only closed price.   \n",
    "On the other hand, zipline needs OLHCV(?) format for each index  \n",
    "make dummy columns and put 'close' value to them  \n",
    "make csv file for each index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "irish-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s0 = 'interest.csv'\n",
    "df0 = pd.read_csv(f_s0, index_col='date')\n",
    "df0 = df0.iloc[::-1] # 과거에서 현재로 정렬순서 변경"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-stand",
   "metadata": {},
   "source": [
    "## prepare data without time confusion\n",
    "Seperate two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "disturbed-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized input, norm val, tgt_date, tgt_val, tgt_diff_ratio, \n",
    "class Environ():\n",
    "    def __init__(self):\n",
    "        self.indices = ['trea3', 'trea5', 'trea10']\n",
    "        #self.tgt_index = 'trea10'\n",
    "        self.tgt_index = 'trea3'\n",
    "        # input window size\n",
    "        self.input_size = 15\n",
    "        # prediction is made after algo.pred_period from the present\n",
    "        self.pred_period = 5\n",
    "\n",
    "env = Environ() # setting values\n",
    "mkt_data = {} # object to store data for learning\n",
    "    \n",
    "def push_data(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read past data and push them to mkt_data\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        if ii_days+1 < env.input_size:\n",
    "            continue\n",
    "        elif ii_days+1 == env.input_size:\n",
    "            env.start_date = data.iloc[ii_days].name\n",
    "        \n",
    "        predicting_date = data.iloc[ii_days].name\n",
    "        # historical data from today to env.input_size behind days\n",
    "        history = data.iloc[ii_days+1-env.input_size:ii_days+1]\n",
    "        mean_val = history.mean() \n",
    "        max_val = history.max()\n",
    "        min_val = history.min()\n",
    "        \n",
    "        range_val = max_val - min_val\n",
    "        norm_history = (history-mean_val)/range_val\n",
    "        mkt_data[predicting_date] = [norm_history, mean_val, range_val, \n",
    "                                     None, None, None, None, None]\n",
    "        \n",
    "            \n",
    "def push_tgt(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read future data and push them to mkt_data\n",
    "    future_prediction_date is a future date when model want to predict\n",
    "    past_predicting_date is a past date when model made a prediction with given information\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        ii_predicting = ii_days - env.pred_period\n",
    "        predicting_date = data.iloc[ii_predicting].name\n",
    "        if ii_predicting < 0 or predicting_date < env.start_date:\n",
    "            continue\n",
    "        \n",
    "        past_history = mkt_data[predicting_date][0]\n",
    "        mean_val = mkt_data[predicting_date][1]\n",
    "        range_val = mkt_data[predicting_date][2]\n",
    "        \n",
    "        #past_price = (past_history.iloc[-1]*range_val+mean_val)[env.tgt_index] # 49.6\n",
    "        \n",
    "        #past_price = (past_history.iloc[-5]*range_val+mean_val)[env.tgt_index] # 56.3\n",
    "        \n",
    "        #mkt_data[predicting_date][0] = past_history[:-4]\n",
    "        \n",
    "        #past_price = (past_history.iloc[-3]*range_val+mean_val)[env.tgt_index] # 53.0, # 53.0\n",
    "        #past_price = (past_history.iloc[-13]*range_val+mean_val)[env.tgt_index] # 67.4\n",
    "        #past_price = (past_history*range_val+mean_val).mean()[env.tgt_index] # 58.4\n",
    "        #past_price = (past_history*range_val+mean_val)[:-3].mean()[env.tgt_index] # 63.2\n",
    "        #past_price = (past_history*range_val+mean_val)[:-5].mean()[env.tgt_index] # 63.7\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10]*range_val+mean_val)[env.tgt_index] # 66.4\n",
    "\n",
    "        #past_price = (past_history.iloc[-5:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        past_price = (past_history.iloc[-3:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10:].mean()*range_val+mean_val)[env.tgt_index] # 55.9\n",
    "        #past_price = (past_history.iloc[-10:-5].mean()*range_val+mean_val)[env.tgt_index] # 60.4\n",
    "        \n",
    "        \n",
    "        prediction_date = data.iloc[ii_days].name\n",
    "        tgt_price = data.iloc[ii_days][env.tgt_index]\n",
    "        #tgt_price = (data.iloc[ii_days-1:ii_days+2].mean())[env.tgt_index]\n",
    "        \n",
    "        tgt_ratio = (tgt_price - past_price)/past_price*10\n",
    "        \n",
    "        mkt_data[predicting_date][3] = prediction_date\n",
    "        mkt_data[predicting_date][4] = np.single(tgt_price)\n",
    "        mkt_data[predicting_date][5] = np.single(tgt_ratio)\n",
    "        \n",
    "        mkt_data[predicting_date][6] = np.single(past_price)\n",
    "        mkt_data[predicting_date][7] = predicting_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-russia",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inappropriate-willow",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_data(env, df0, mkt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-question",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check data are pushed correctly\n",
    "* no dates should be ahead of key date\n",
    "* prediction info are not pushed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "contained-greensboro",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000-11-11', '2000-11-13', '2000-11-14']\n"
     ]
    }
   ],
   "source": [
    "print(list(mkt_data.keys())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "latin-beauty",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.3000,  0.2721,  0.2940],\n",
       "         [ 0.1060,  0.1314,  0.1522],\n",
       "         [ 0.1060,  0.1314,  0.1522],\n",
       "         [ 0.0910,  0.1240,  0.1522],\n",
       "         [ 0.1507,  0.1758,  0.1837],\n",
       "         [ 0.1060,  0.1240,  0.1365],\n",
       "         [ 0.1284,  0.1240,  0.1050],\n",
       "         [ 0.1582,  0.1610,  0.1365],\n",
       "         [ 0.1582,  0.1610,  0.1365],\n",
       "         [ 0.1881,  0.1980,  0.1759],\n",
       "         [ 0.1881,  0.2054,  0.1759],\n",
       "         [ 0.0761,  0.0721,  0.0499],\n",
       "         [-0.3567, -0.4242, -0.4383],\n",
       "         [-0.7000, -0.7279, -0.7060],\n",
       "         [-0.7000, -0.7279, -0.7060]]),\n",
       " tensor([7.6640, 7.8663, 8.1433]),\n",
       " tensor([0.6700, 0.6750, 0.6350]),\n",
       " '2000-11-17',\n",
       " 7.275,\n",
       " 0.004584002,\n",
       " 7.2716665,\n",
       " '2000-11-11',\n",
       " tensor([-0.8881, -0.9333, -0.8819, -0.8582, -0.8889, -0.8425, -0.8060, -0.8519,\n",
       "         -0.8425, -0.3433, -0.3037, -0.2677, -0.5149, -0.5852, -0.5748, -0.4627,\n",
       "         -0.5481, -0.5748,  0.0299,  0.0444,  0.0394,  0.0821,  0.0815,  0.0394])]"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkt_data['2000-11-11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "arranged-taylor",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-11-06'"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-carry",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "liquid-insert",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_tgt(env, df0, mkt_data)\n",
    "mkt_data = {key: val for key, val in mkt_data.items() if (val[-1] is not None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-exploration",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHECK tgt push is correctly made\n",
    "end value of '2000-11-23' * norm == prediction value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "broadband-ethnic",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-10-26  0.300000  0.272099  0.293963\n",
      "2000-10-27  0.105970  0.131358  0.152231\n",
      "2000-10-28  0.105970  0.131358  0.152231\n",
      "2000-10-30  0.091045  0.123951  0.152231\n",
      "2000-10-31  0.150746  0.175802  0.183727\n",
      "2000-11-01  0.105970  0.123951  0.136483\n",
      "2000-11-02  0.128358  0.123951  0.104987\n",
      "2000-11-03  0.158209  0.160988  0.136483\n",
      "2000-11-04  0.158209  0.160988  0.136483\n",
      "2000-11-06  0.188060  0.198025  0.175853\n",
      "2000-11-07  0.188060  0.205432  0.175853\n",
      "2000-11-08  0.076119  0.072099  0.049869\n",
      "2000-11-09 -0.356716 -0.424198 -0.438320\n",
      "2000-11-10 -0.700000 -0.727901 -0.706037\n",
      "2000-11-11 -0.700000 -0.727901 -0.706037, trea3     7.664000\n",
      "trea5     7.866333\n",
      "trea10    8.143333\n",
      "dtype: float64, trea3     0.670\n",
      "trea5     0.675\n",
      "trea10    0.635\n",
      "dtype: float64, '2000-12-05', 7.07, -1.0108074]\n",
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-11-07  0.681934  0.687143  0.711795\n",
      "2000-11-08  0.567430  0.558571  0.588718\n",
      "2000-11-09  0.124682  0.080000  0.111795\n",
      "2000-11-10 -0.226463 -0.212857 -0.149744\n",
      "2000-11-11 -0.226463 -0.212857 -0.149744\n",
      "2000-11-13 -0.318066 -0.305714 -0.288205\n",
      "2000-11-14 -0.287532 -0.312857 -0.288205\n",
      "2000-11-15 -0.150127 -0.191429 -0.172821\n",
      "2000-11-16 -0.081425 -0.120000 -0.134359\n",
      "2000-11-17 -0.104326 -0.141429 -0.157436\n",
      "2000-11-18 -0.104326 -0.141429 -0.157436\n",
      "2000-11-20 -0.073791 -0.070000 -0.095897\n",
      "2000-11-21  0.117048  0.151429  0.104103\n",
      "2000-11-22  0.017812  0.072857  0.011795\n",
      "2000-11-23  0.063613  0.158571  0.065641, trea3     7.343333\n",
      "trea5     7.524000\n",
      "trea10    7.792333\n",
      "dtype: float64, trea3     0.655\n",
      "trea5     0.700\n",
      "trea10    0.650\n",
      "dtype: float64, '2000-12-16', 7.11, -0.9541985]\n"
     ]
    }
   ],
   "source": [
    "print(mkt_data['2000-11-11'])\n",
    "print(mkt_data['2000-11-23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "severe-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vanilla-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_data(Dataset):\n",
    "    def __init__(self, in_data, dates):\n",
    "        \"\"\"\n",
    "        :param in_data: dict of (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        # change dataframe to float numpy array\n",
    "        for key, val in in_data.items():\n",
    "            if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "                in_data[key][0] = torch.tensor(in_data[key][0].to_numpy(np.single))\n",
    "                in_data[key][1] = torch.tensor(in_data[key][1].to_numpy(np.single))\n",
    "                in_data[key][2] = torch.tensor(in_data[key][2].to_numpy(np.single))\n",
    "                ## add difference\n",
    "                diff = in_data[key][0][-1] - in_data[key][0][-5]\n",
    "                diff2 = in_data[key][0][-1] - in_data[key][0][-7]\n",
    "                diff3 = in_data[key][0][-1] - in_data[key][0][-10]\n",
    "                diff4 = in_data[key][0][-1] - in_data[key][0][-3]\n",
    "                diff5 = in_data[key][0][-3] - in_data[key][0][-7]\n",
    "                diff6 = in_data[key][0][-3] - in_data[key][0][-10]\n",
    "                diff7 = in_data[key][0][-5] - in_data[key][0][-7]\n",
    "                diff8 = in_data[key][0][-5] - in_data[key][0][-10]\n",
    "                #diff = torch.cat((diff, diff2, diff3), dim=-1)\n",
    "                diff = torch.cat((diff, diff2, diff3, diff4, diff5, diff6, diff7, diff8), dim=-1)\n",
    "                in_data[key].append(diff)\n",
    "        self.in_data = in_data\n",
    "        self.i2dates = {}\n",
    "        for ii, date1 in enumerate(dates):\n",
    "             self.i2dates[ii] = date1\n",
    "        \n",
    "    def __getitem__(self, ii_date):\n",
    "        \"\"\"\n",
    "        :return: (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        predicting_date = self.i2dates[ii_date]\n",
    "        return self.in_data[predicting_date]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "difficult-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase data that do not have prediction target\n",
    "traintest_dates = sorted(list(mkt_data.keys()))\n",
    "n_train = len(traintest_dates)*8//10\n",
    "n_test = len(traintest_dates) - n_train\n",
    "train_dates = traintest_dates[:n_train]\n",
    "test_dates = traintest_dates[n_train:]\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for key, val in mkt_data.items():\n",
    "    if key in train_dates:\n",
    "        train_data[key] = val\n",
    "    elif key in test_dates:\n",
    "        test_data[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eleven-caribbean",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dset = Index_data(train_data, train_dates)\n",
    "test_dset = Index_data(test_data, test_dates)\n",
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dominant-fifth",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "productive-recognition",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0044, -0.0933, -0.0689],\n",
       "         [-0.4378, -0.5562, -0.5213],\n",
       "         [-0.0244, -0.1790, -0.1117],\n",
       "         [-0.0711, -0.1848, -0.1403],\n",
       "         [ 0.0289, -0.0819, -0.0451],\n",
       "         [ 0.0822, -0.1276, -0.1403],\n",
       "         [ 0.5622,  0.3010,  0.1692],\n",
       "         [ 0.5156,  0.3181,  0.1216],\n",
       "         [ 0.0156, -0.0019, -0.0927],\n",
       "         [-0.2378, -0.0305, -0.0975],\n",
       "         [-0.1578, -0.0133, -0.0451],\n",
       "         [-0.1378,  0.0895,  0.1930],\n",
       "         [ 0.0956,  0.4438,  0.4787],\n",
       "         [-0.1711,  0.0152,  0.1073],\n",
       "         [-0.0578,  0.1010,  0.1930]]),\n",
       " tensor([1.7457, 1.9123, 2.1595]),\n",
       " tensor([0.1500, 0.1750, 0.2100]),\n",
       " '2016-12-13',\n",
       " 1.7043333,\n",
       " -0.19934829,\n",
       " tensor([ 0.1000,  0.1143,  0.2381, -0.0733,  0.1029,  0.2857, -0.1400,  0.2286,\n",
       "          0.3333, -0.1533, -0.3429, -0.2857,  0.0800,  0.4457,  0.5714,  0.0133,\n",
       "          0.5714,  0.6190, -0.1733, -0.0114,  0.0476, -0.2400,  0.1143,  0.0952])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rental-aquatic",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor_rnn(nets.Net):\n",
    "    def __init__(self, rnn=None, downnet=None,\n",
    "                 loss=None, optimizer=None, device='cuda:2',\n",
    "                 dim_input=100, dim_hidden=200, bidirectional=True):\n",
    "        \"\"\"\n",
    "        net is consists of [embed, rnn, downnet]\n",
    "        :param downnet: define downstream job\n",
    "        \"\"\"\n",
    "        super(Predictor_rnn, self).__init__(loss=loss, device=device)\n",
    "        \n",
    "        if rnn is None:\n",
    "            self.rnn = nn.LSTM(input_size=dim_input, \n",
    "                               hidden_size=dim_hidden, \n",
    "                               num_layers= 2,\n",
    "                               batch_first=True,\n",
    "                               #bidirectional=False)\n",
    "                               bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.rnn = rnn\n",
    "            \n",
    "        if downnet is None:\n",
    "            self.downnet = nets.get_MLP([dim_hidden, dim_hidden*2, 1], \n",
    "                                        dropout=0.35, end=True)\n",
    "        else:\n",
    "            self.downnet = downnet\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.rnn = self.rnn.to(device)\n",
    "        self.downnet = self.downnet.to(device)\n",
    "        \n",
    "        parms = list(self.rnn.parameters())\n",
    "        parms += list(self.downnet.parameters())\n",
    "        self.optimizer = optimizer(parms)\n",
    "        \n",
    "    def set_train(self):\n",
    "        self.rnn.train()\n",
    "        self.downnet.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        self.rnn.eval()\n",
    "        self.downnet.eval()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for ii, layer in enumerate(self.downnet):\n",
    "            if 'Linear' in str(layer):\n",
    "                torch.nn.init.xavier_uniform_(self.downnet[ii].weight)\n",
    "                #torch.nn.init.xavier_normal_(self.downnet[ii].weight)\n",
    "\n",
    "    def forward(self, x, diff=None):\n",
    "        out, hidden = self.rnn(x)\n",
    "        out = out[:,-1] # choose last output\n",
    "        if diff is not None:\n",
    "            #print(out.size())\n",
    "            out = torch.cat((out, diff),dim=1)\n",
    "            #print(diff.size())\n",
    "        out = self.downnet(out) \n",
    "        return out.view(-1)\n",
    "\n",
    "    def run_eval(self, data):\n",
    "        self.set_eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        tgts = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in = data_batch[0]\n",
    "                mean_val = data_batch[1]\n",
    "                range_val = data_batch[2]\n",
    "                pred_date = data_batch[3]\n",
    "                tgt = data_batch[5]\n",
    "                diff = data_batch[-1]\n",
    "                \n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                diff = diff.to(self.device)\n",
    "                out = self.forward(data_in, diff)\n",
    "                #print('loss', loss, self.loss)\n",
    "                loss += self.loss(out, tgt).cpu().numpy()\n",
    "                out = out.cpu().numpy()\n",
    "                #print('loss===', loss)\n",
    "                tgt = tgt.cpu().numpy()\n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                    tgts = tgt\n",
    "                else:\n",
    "                    outs = np.concatenate((outs, out), axis=0)\n",
    "                    tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "        loss /= 1.0*(i_batch+1)\n",
    "        print('evaluate', 'loss', loss, 'accuracy : define function')\n",
    "        return outs, tgts, loss\n",
    "    \n",
    "    def run_batch(self, i_batch, data_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in = data_batch[0]\n",
    "        mean_val = data_batch[1]\n",
    "        range_val = data_batch[2]\n",
    "        pred_date = data_batch[3]\n",
    "        tgt = data_batch[5]\n",
    "        diff = data_batch[-1]\n",
    "        \n",
    "        data_in = data_in.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        diff = diff.to(self.device)\n",
    "        out = self.forward(data_in, diff)\n",
    "        \n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "annual-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = train_dset[0][0].shape[-1]\n",
    "dim_hidden = 400\n",
    "#downnet = nets.get_MLP([dim_hidden+3*3, dim_hidden, 1], \n",
    "# Add difference variables\n",
    "downnet = nets.get_MLP([dim_hidden+3*8, dim_hidden, 1], \n",
    "                        dropout=0.35, end=True)\n",
    "#bidownnet = nets.get_MLP([dim_hidden*2+3*8, dim_hidden*2, 1], \n",
    "#                        dropout=0.0, end=True)\n",
    "loss = nn.MSELoss() # which combines logsoftmax and nll loss\n",
    "optimizer = optim.Adam\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lasting-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor_rnn(loss=loss, optimizer=optimizer, \n",
    "                          device=device, dim_input=dim_input, \n",
    "                          dim_hidden=dim_hidden, downnet=downnet, bidirectional=False)\n",
    "                          #dim_hidden=dim_hidden, downnet=bidownnet, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hired-edmonton",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.11407846023342502\n",
      "eval_train evaluate loss 0.09677652897897052 accuracy : define function\n",
      "epoch 1 loss 0.10318686257102597\n",
      "eval_train evaluate loss 0.09629140026978593 accuracy : define function\n",
      "epoch 2 loss 0.1015465887291218\n",
      "eval_train evaluate loss 0.09651900038345536 accuracy : define function\n",
      "epoch 3 loss 0.10088799248880415\n",
      "eval_train evaluate loss 0.0981692595045958 accuracy : define function\n",
      "epoch 4 loss 0.09975654225963265\n",
      "eval_train evaluate loss 0.09543080344351369 accuracy : define function\n",
      "epoch 5 loss 0.09875169101713309\n",
      "eval_train evaluate loss 0.09770687452670354 accuracy : define function\n",
      "epoch 6 loss 0.09874016869423995\n",
      "eval_train evaluate loss 0.09639740373883675 accuracy : define function\n",
      "epoch 7 loss 0.098176098692773\n",
      "eval_train evaluate loss 0.09563348920487647 accuracy : define function\n",
      "epoch 8 loss 0.09803614659763094\n",
      "eval_train evaluate loss 0.0951554383804549 accuracy : define function\n",
      "epoch 9 loss 0.09732009665067516\n",
      "eval_train evaluate loss 0.09673290076985289 accuracy : define function\n",
      "epoch 10 loss 0.0977989924487783\n",
      "eval_train evaluate loss 0.09572194978149969 accuracy : define function\n",
      "epoch 11 loss 0.09762107580900192\n",
      "eval_train evaluate loss 0.09478186849337905 accuracy : define function\n",
      "epoch 12 loss 0.0972804997060726\n",
      "eval_train evaluate loss 0.09518923326881963 accuracy : define function\n",
      "epoch 13 loss 0.09618523044150266\n",
      "eval_train evaluate loss 0.09528867383279017 accuracy : define function\n",
      "epoch 14 loss 0.09675831725793098\n",
      "eval_train evaluate loss 0.09471642965478684 accuracy : define function\n",
      "epoch 15 loss 0.09747352951498174\n",
      "eval_train evaluate loss 0.09461134972412195 accuracy : define function\n",
      "epoch 16 loss 0.09687761189555054\n",
      "eval_train evaluate loss 0.094856827394731 accuracy : define function\n",
      "epoch 17 loss 0.09733669520981277\n",
      "eval_train evaluate loss 0.09549858243162952 accuracy : define function\n",
      "epoch 18 loss 0.09658229239841006\n",
      "eval_train evaluate loss 0.09370668238001083 accuracy : define function\n",
      "epoch 19 loss 0.09513397192332282\n",
      "eval_train evaluate loss 0.09561544662313674 accuracy : define function\n",
      "epoch 20 loss 0.0967566071272786\n",
      "eval_train evaluate loss 0.0933035557060989 accuracy : define function\n",
      "epoch 21 loss 0.09498441975508164\n",
      "eval_train evaluate loss 0.09287571134184723 accuracy : define function\n",
      "epoch 22 loss 0.09479553502664637\n",
      "eval_train evaluate loss 0.09371380202138602 accuracy : define function\n",
      "epoch 23 loss 0.0944422381137734\n",
      "eval_train evaluate loss 0.09202683555768497 accuracy : define function\n",
      "epoch 24 loss 0.09395898583887229\n",
      "eval_train evaluate loss 0.09156632840410987 accuracy : define function\n",
      "epoch 25 loss 0.09356489304953547\n",
      "eval_train evaluate loss 0.09185926549470247 accuracy : define function\n",
      "epoch 26 loss 0.0943428176218894\n",
      "eval_train evaluate loss 0.09148403707502493 accuracy : define function\n",
      "epoch 27 loss 0.09336295985241434\n",
      "eval_train evaluate loss 0.0909297837695079 accuracy : define function\n",
      "epoch 28 loss 0.09299291987250101\n",
      "eval_train evaluate loss 0.09109036702273497 accuracy : define function\n",
      "epoch 29 loss 0.09417349283597362\n",
      "eval_train evaluate loss 0.0898094417443916 accuracy : define function\n"
     ]
    }
   ],
   "source": [
    "predictor.run_train(n_epoch=30, data=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "photographic-rogers",
   "metadata": {},
   "outputs": [],
   "source": [
    "rets = {}\n",
    "tgts = None\n",
    "preds = None\n",
    "with torch.no_grad():\n",
    "    for data_batch in test_loader:\n",
    "        data_in = data_batch[0]\n",
    "        tgt = data_batch[5]\n",
    "        diff = data_batch[-1]\n",
    "        \n",
    "        data_in = data_in.to(device)\n",
    "        diff = diff.to(device)\n",
    "        tgt = tgt.numpy()\n",
    "        #out = predictor.forward(data_in).cpu().numpy()\n",
    "        out = predictor.forward(data_in, diff).cpu().numpy()\n",
    "        if preds is None:\n",
    "            preds = out\n",
    "            tgts = tgt\n",
    "        else:\n",
    "            preds = np.concatenate((preds, out), axis=0)\n",
    "            tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "            \n",
    "        prediction_dates = data_batch[3]\n",
    "        past_prices = data_batch[6]\n",
    "        predicting_dates = data_batch[7]\n",
    "        for ii, date1 in enumerate(prediction_dates):\n",
    "            out1 = out[ii]\n",
    "            out_orig = out1/10*past_prices[ii] + past_prices[ii]\n",
    "            rets[date1] = {'predicting_date': predicting_dates[ii],\n",
    "                          'pred': out_orig,\n",
    "                           'past': past_prices[ii],\n",
    "                           'tgt': data_batch[4][ii]}\n",
    "            \n",
    "preds = preds.reshape(-1)\n",
    "tgts = tgts.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "lyric-sullivan",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(rets.keys()).index('2021-01-04')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "temporal-giant",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 45\n",
      "10 25\n",
      "71 39 0.5492957746478874\n"
     ]
    }
   ],
   "source": [
    "limit = 999\n",
    "sum1 = np.sum((preds[limit:] > 0) & (tgts[limit:] > 0))\n",
    "sum2 = np.sum((preds[limit:] < 0) & (tgts[limit:] < 0))\n",
    "print(sum1, np.sum(tgts[limit:]>0))\n",
    "print(sum2, np.sum(tgts[limit:]<0))\n",
    "print(len(preds[limit:]), sum1+sum2, 1.0*(sum1+sum2)/len(preds[limit:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "secure-airfare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 481\n",
      "389 586\n",
      "1070 649 0.6065420560747663\n"
     ]
    }
   ],
   "source": [
    "sum1 = np.sum((preds > 0) & (tgts > 0))\n",
    "sum2 = np.sum((preds < 0) & (tgts < 0))\n",
    "print(sum1, np.sum(tgts>0))\n",
    "print(sum2, np.sum(tgts<0))\n",
    "print(len(preds), sum1+sum2, 1.0*(sum1+sum2)/len(preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-pharmacology",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## all includes 3*3=9 diff, predict 4 days avg, prediction_period=10\n",
    "\n",
    "lstm_layer=2  \n",
    "epoch 30, dropout=0, dim_hidden=100, mlp (dim_hidden, dim_hidden, 1) --> 55.5%  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, 1) --> 55.3%  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden*2, 1) --> 53.4%  \n",
    "\n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden//2, 1) --> 53.4%  \n",
    "epoch 20, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden//2, 1) --> 53.9%  \n",
    "\n",
    "epoch 20, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 48.8%  \n",
    "epoch 30, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 51.4%  \n",
    "epoch 40, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 53.7%  \n",
    "epoch 50, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 49.1%  \n",
    "epoch 60, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 48.7%  \n",
    "epoch 70, dropout=0, dim_hidden=200, mlp (dim_hidden, dim_hidden, dim_hidden, 1) --> 50.3%  \n",
    "\n",
    "\n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.2%  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.0%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 56.9%, 54.3%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.9%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.3%  \n",
    "epoch 60, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 48.6%, 49.6  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.2  \n",
    "epoch 40, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.8  \n",
    "epoch 50, dropout=0, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.8  \n",
    "\n",
    "epoch 30, dropout=0, dim_hidden=500, mlp (dim_hidden, dim_hidden, 1) --> 55.8  \n",
    "epoch 40, dropout=0, dim_hidden=500, mlp (dim_hidden, dim_hidden, 1) --> 53.8  \n",
    "\n",
    "lstm_layer=1\n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.3%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.7%  \n",
    "\n",
    "lstm_layer=3\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.7%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.4%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.8%  \n",
    "\n",
    "## includes 3*8=24 diff\n",
    "layer=2\n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.5%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.9%  \n",
    "\n",
    "## includes 3*8=24 diff, 1day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.8%  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-economics",
   "metadata": {},
   "source": [
    "Below prediction days are (-2,-1,0)\n",
    "## includes 3*8=24 diff, 3day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 53.7%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.5%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=5\n",
    "layer=2\n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.8%  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 62.6%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.5%  \n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 58.3%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=15\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3%  \n",
    "\n",
    "## includes 3*8=24 diff, 3day predicting, 3day prediction, prediction_period=20\n",
    "layer=2\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.7%  \n",
    "epoch 40, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.1%  \n",
    "epoch 50, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.1%  \n",
    "epoch 60, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 51.4%  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-separate",
   "metadata": {},
   "source": [
    "##  includes 3*8=24 diff, last price, 3day prediction, layer=2\n",
    "epoch 5, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 51.7%  \n",
    "epoch 10, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 56.4%, 54.8, 54.6  \n",
    "epoch 10, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 52.7  \n",
    "epoch 20, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.5%, 53.9 \n",
    "epoch 20, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.3\n",
    "epoch 30, dropout=0, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 49.3% \n",
    "epoch 30, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 55.2% \n",
    "epoch 40, dropout=0.35, dim_hidden=300, mlp (dim_hidden, dim_hidden, 1) --> 54.5% \n",
    "\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.5  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.3  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9  \n",
    "\n",
    "## Bidirectional\n",
    "epoch 10, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.9 \n",
    "epoch 20, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.2 \n",
    "epoch 30, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.5 \n",
    "epoch 40, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.5 \n",
    "epoch 50, dropout=0., dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 52.8 \n",
    "\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 56.7  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 54.1 \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.1 \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 56.4 \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 55.0 \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden*2, dim_hidden*2, 1) --> 54.6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-inside",
   "metadata": {},
   "source": [
    "## last to last period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.0\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.2\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.1\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-buying",
   "metadata": {},
   "source": [
    "## last to last period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.6\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.1\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.3\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.5\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.1\n",
    "\n",
    "## last to last period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.3, 51.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.2, 52.8\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.8, 54.5\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.2, 52.1\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.5, 50.7\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.3, 52.2\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.1, 48.4\n",
    "\n",
    "## last to last period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.7\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.1\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 48.6\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 47.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-expansion",
   "metadata": {},
   "source": [
    "## last to pred3 period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 47.0  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.1  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.0  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.4  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4  \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.3  \n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.5  \n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.2  \n",
    "epoch 90, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-living",
   "metadata": {},
   "source": [
    "## last to pred3 period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.2\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.6\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.4\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.5\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.1\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.7\n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-clause",
   "metadata": {},
   "source": [
    "## last to pred3 period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.9\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.0\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.0\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.6\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-reminder",
   "metadata": {},
   "source": [
    "## last to pred3 period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.4\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.8\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.4\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.3\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 51.5\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-applicant",
   "metadata": {},
   "source": [
    "## pred3 to last period 5\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.0  \n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 60.1  \n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.9  \n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 60.5  \n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 61.0  \n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.5  \n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.3  \n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.6  \n",
    "\n",
    "## pred3 to last period 10\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.0\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.1\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 59.8\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.6\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 58.7\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.9\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 57.5\n",
    "epoch 80, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.1\n",
    "\n",
    "## pred3 to last period 15\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.9\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.9\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.6\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.0\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.9\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 53.3\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 50.5\n",
    "\n",
    "## pred3 to last    period 20\n",
    "epoch 10, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 52.6\n",
    "epoch 20, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 30, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 56.4\n",
    "epoch 40, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.8\n",
    "epoch 50, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 54.1\n",
    "epoch 60, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 55.2\n",
    "epoch 70, dropout=0.35, dim_hidden=400, mlp (dim_hidden, dim_hidden, 1) --> 49.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "heated-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021 = {key: val for key, val in rets.items() if key>'2021-01-01'}\n",
    "rows = list(rets_2021['2021-01-04'].keys())\n",
    "cols = list(rets_2021.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "corrected-weekend",
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021b = {}\n",
    "for col in cols:\n",
    "    for row in rows:\n",
    "        rets_2021b.setdefault(row, {})\n",
    "        val = rets_2021[col][row]\n",
    "        if type(val) is not str:\n",
    "            rets_2021b[row][col] = rets_2021[col][row].item()        \n",
    "        else:\n",
    "            rets_2021b[row][col] = rets_2021[col][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "signal-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(rets_2021b).to_csv('./rets.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caroline-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021-01-04': {'predicting_date': '2020-12-24',\n",
       "  'pred': tensor(0.9607),\n",
       "  'past': tensor(0.9403),\n",
       "  'tgt': tensor(0.9550)},\n",
       " '2021-01-05': {'predicting_date': '2020-12-28',\n",
       "  'pred': tensor(0.9542),\n",
       "  'past': tensor(0.9453),\n",
       "  'tgt': tensor(0.9370)},\n",
       " '2021-01-06': {'predicting_date': '2020-12-29',\n",
       "  'pred': tensor(0.9781),\n",
       "  'past': tensor(0.9630),\n",
       "  'tgt': tensor(0.9600)},\n",
       " '2021-01-07': {'predicting_date': '2020-12-30',\n",
       "  'pred': tensor(0.9640),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-08': {'predicting_date': '2020-12-31',\n",
       "  'pred': tensor(0.9614),\n",
       "  'past': tensor(0.9733),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-11': {'predicting_date': '2021-01-04',\n",
       "  'pred': tensor(0.9480),\n",
       "  'past': tensor(0.9650),\n",
       "  'tgt': tensor(0.9650)},\n",
       " '2021-01-12': {'predicting_date': '2021-01-05',\n",
       "  'pred': tensor(0.9365),\n",
       "  'past': tensor(0.9540),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-13': {'predicting_date': '2021-01-06',\n",
       "  'pred': tensor(0.9564),\n",
       "  'past': tensor(0.9507),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-14': {'predicting_date': '2021-01-07',\n",
       "  'pred': tensor(0.9678),\n",
       "  'past': tensor(0.9557),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-15': {'predicting_date': '2021-01-08',\n",
       "  'pred': tensor(0.9769),\n",
       "  'past': tensor(0.9690),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-18': {'predicting_date': '2021-01-11',\n",
       "  'pred': tensor(0.9591),\n",
       "  'past': tensor(0.9707),\n",
       "  'tgt': tensor(0.9670)},\n",
       " '2021-01-19': {'predicting_date': '2021-01-12',\n",
       "  'pred': tensor(0.9671),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-20': {'predicting_date': '2021-01-13',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9723),\n",
       "  'tgt': tensor(0.9750)},\n",
       " '2021-01-21': {'predicting_date': '2021-01-14',\n",
       "  'pred': tensor(0.9775),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9700)},\n",
       " '2021-01-22': {'predicting_date': '2021-01-15',\n",
       "  'pred': tensor(0.9616),\n",
       "  'past': tensor(0.9773),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-01-25': {'predicting_date': '2021-01-18',\n",
       "  'pred': tensor(0.9577),\n",
       "  'past': tensor(0.9747),\n",
       "  'tgt': tensor(1.0050)},\n",
       " '2021-01-26': {'predicting_date': '2021-01-19',\n",
       "  'pred': tensor(0.9661),\n",
       "  'past': tensor(0.9713),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-01-27': {'predicting_date': '2021-01-20',\n",
       "  'pred': tensor(0.9637),\n",
       "  'past': tensor(0.9730),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-01-28': {'predicting_date': '2021-01-21',\n",
       "  'pred': tensor(0.9625),\n",
       "  'past': tensor(0.9740),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-01-29': {'predicting_date': '2021-01-22',\n",
       "  'pred': tensor(0.9944),\n",
       "  'past': tensor(0.9783),\n",
       "  'tgt': tensor(0.9720)},\n",
       " '2021-02-01': {'predicting_date': '2021-01-25',\n",
       "  'pred': tensor(1.0058),\n",
       "  'past': tensor(0.9883),\n",
       "  'tgt': tensor(0.9870)},\n",
       " '2021-02-02': {'predicting_date': '2021-01-26',\n",
       "  'pred': tensor(1.0003),\n",
       "  'past': tensor(0.9990),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-03': {'predicting_date': '2021-01-27',\n",
       "  'pred': tensor(0.9802),\n",
       "  'past': tensor(0.9980),\n",
       "  'tgt': tensor(0.9770)},\n",
       " '2021-02-04': {'predicting_date': '2021-01-28',\n",
       "  'pred': tensor(0.9724),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-05': {'predicting_date': '2021-01-29',\n",
       "  'pred': tensor(0.9692),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-08': {'predicting_date': '2021-02-01',\n",
       "  'pred': tensor(0.9851),\n",
       "  'past': tensor(0.9787),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-09': {'predicting_date': '2021-02-02',\n",
       "  'pred': tensor(0.9823),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9900)},\n",
       " '2021-02-10': {'predicting_date': '2021-02-03',\n",
       "  'pred': tensor(0.9800),\n",
       "  'past': tensor(0.9813),\n",
       "  'tgt': tensor(0.9920)},\n",
       " '2021-02-15': {'predicting_date': '2021-02-04',\n",
       "  'pred': tensor(0.9856),\n",
       "  'past': tensor(0.9790),\n",
       "  'tgt': tensor(0.9940)},\n",
       " '2021-02-16': {'predicting_date': '2021-02-05',\n",
       "  'pred': tensor(0.9774),\n",
       "  'past': tensor(0.9797),\n",
       "  'tgt': tensor(0.9800)},\n",
       " '2021-02-17': {'predicting_date': '2021-02-08',\n",
       "  'pred': tensor(1.0049),\n",
       "  'past': tensor(0.9857),\n",
       "  'tgt': tensor(0.9820)},\n",
       " '2021-02-18': {'predicting_date': '2021-02-09',\n",
       "  'pred': tensor(0.9871),\n",
       "  'past': tensor(0.9890),\n",
       "  'tgt': tensor(0.9850)},\n",
       " '2021-02-19': {'predicting_date': '2021-02-10',\n",
       "  'pred': tensor(0.9882),\n",
       "  'past': tensor(0.9923),\n",
       "  'tgt': tensor(0.9950)},\n",
       " '2021-02-22': {'predicting_date': '2021-02-15',\n",
       "  'pred': tensor(0.9979),\n",
       "  'past': tensor(0.9920),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-02-23': {'predicting_date': '2021-02-16',\n",
       "  'pred': tensor(0.9768),\n",
       "  'past': tensor(0.9887),\n",
       "  'tgt': tensor(1.0150)},\n",
       " '2021-02-24': {'predicting_date': '2021-02-17',\n",
       "  'pred': tensor(0.9893),\n",
       "  'past': tensor(0.9853),\n",
       "  'tgt': tensor(1.0020)},\n",
       " '2021-02-25': {'predicting_date': '2021-02-18',\n",
       "  'pred': tensor(0.9911),\n",
       "  'past': tensor(0.9823),\n",
       "  'tgt': tensor(0.9960)},\n",
       " '2021-02-26': {'predicting_date': '2021-02-19',\n",
       "  'pred': tensor(1.0203),\n",
       "  'past': tensor(0.9873),\n",
       "  'tgt': tensor(1.0200)},\n",
       " '2021-03-02': {'predicting_date': '2021-02-22',\n",
       "  'pred': tensor(1.0568),\n",
       "  'past': tensor(1.),\n",
       "  'tgt': tensor(1.0220)},\n",
       " '2021-03-03': {'predicting_date': '2021-02-23',\n",
       "  'pred': tensor(1.0410),\n",
       "  'past': tensor(1.0100),\n",
       "  'tgt': tensor(1.0170)},\n",
       " '2021-03-04': {'predicting_date': '2021-02-24',\n",
       "  'pred': tensor(1.0213),\n",
       "  'past': tensor(1.0123),\n",
       "  'tgt': tensor(1.0350)},\n",
       " '2021-03-05': {'predicting_date': '2021-02-25',\n",
       "  'pred': tensor(0.9976),\n",
       "  'past': tensor(1.0043),\n",
       "  'tgt': tensor(1.0670)},\n",
       " '2021-03-08': {'predicting_date': '2021-02-26',\n",
       "  'pred': tensor(1.0095),\n",
       "  'past': tensor(1.0060),\n",
       "  'tgt': tensor(1.1500)},\n",
       " '2021-03-09': {'predicting_date': '2021-03-02',\n",
       "  'pred': tensor(1.0161),\n",
       "  'past': tensor(1.0127),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-10': {'predicting_date': '2021-03-03',\n",
       "  'pred': tensor(1.0121),\n",
       "  'past': tensor(1.0197),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-11': {'predicting_date': '2021-03-04',\n",
       "  'pred': tensor(1.0291),\n",
       "  'past': tensor(1.0247),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-12': {'predicting_date': '2021-03-05',\n",
       "  'pred': tensor(1.0829),\n",
       "  'past': tensor(1.0397),\n",
       "  'tgt': tensor(1.2300)},\n",
       " '2021-03-15': {'predicting_date': '2021-03-08',\n",
       "  'pred': tensor(1.1399),\n",
       "  'past': tensor(1.0840),\n",
       "  'tgt': tensor(1.2200)},\n",
       " '2021-03-16': {'predicting_date': '2021-03-09',\n",
       "  'pred': tensor(1.1995),\n",
       "  'past': tensor(1.1457),\n",
       "  'tgt': tensor(1.1750)},\n",
       " '2021-03-17': {'predicting_date': '2021-03-10',\n",
       "  'pred': tensor(1.2035),\n",
       "  'past': tensor(1.1817),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-03-18': {'predicting_date': '2021-03-11',\n",
       "  'pred': tensor(1.1807),\n",
       "  'past': tensor(1.1900),\n",
       "  'tgt': tensor(1.1370)},\n",
       " '2021-03-19': {'predicting_date': '2021-03-12',\n",
       "  'pred': tensor(1.1893),\n",
       "  'past': tensor(1.1933),\n",
       "  'tgt': tensor(1.1420)},\n",
       " '2021-03-22': {'predicting_date': '2021-03-15',\n",
       "  'pred': tensor(1.2049),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1300)},\n",
       " '2021-03-23': {'predicting_date': '2021-03-16',\n",
       "  'pred': tensor(1.1990),\n",
       "  'past': tensor(1.2083),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-03-24': {'predicting_date': '2021-03-17',\n",
       "  'pred': tensor(1.1872),\n",
       "  'past': tensor(1.1907),\n",
       "  'tgt': tensor(1.1200)},\n",
       " '2021-03-25': {'predicting_date': '2021-03-18',\n",
       "  'pred': tensor(1.1615),\n",
       "  'past': tensor(1.1630),\n",
       "  'tgt': tensor(1.0950)},\n",
       " '2021-03-26': {'predicting_date': '2021-03-19',\n",
       "  'pred': tensor(1.1602),\n",
       "  'past': tensor(1.1520),\n",
       "  'tgt': tensor(1.1250)},\n",
       " '2021-03-29': {'predicting_date': '2021-03-22',\n",
       "  'pred': tensor(1.1531),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1150)},\n",
       " '2021-03-30': {'predicting_date': '2021-03-23',\n",
       "  'pred': tensor(1.1640),\n",
       "  'past': tensor(1.1397),\n",
       "  'tgt': tensor(1.1510)},\n",
       " '2021-03-31': {'predicting_date': '2021-03-24',\n",
       "  'pred': tensor(1.1460),\n",
       "  'past': tensor(1.1323),\n",
       "  'tgt': tensor(1.1270)},\n",
       " '2021-04-01': {'predicting_date': '2021-03-25',\n",
       "  'pred': tensor(1.1230),\n",
       "  'past': tensor(1.1207),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-02': {'predicting_date': '2021-03-26',\n",
       "  'pred': tensor(1.1178),\n",
       "  'past': tensor(1.1133),\n",
       "  'tgt': tensor(1.1470)},\n",
       " '2021-04-05': {'predicting_date': '2021-03-29',\n",
       "  'pred': tensor(1.1063),\n",
       "  'past': tensor(1.1117),\n",
       "  'tgt': tensor(1.2020)},\n",
       " '2021-04-06': {'predicting_date': '2021-03-30',\n",
       "  'pred': tensor(1.1355),\n",
       "  'past': tensor(1.1303),\n",
       "  'tgt': tensor(1.1850)},\n",
       " '2021-04-07': {'predicting_date': '2021-03-31',\n",
       "  'pred': tensor(1.1263),\n",
       "  'past': tensor(1.1310),\n",
       "  'tgt': tensor(1.1770)},\n",
       " '2021-04-08': {'predicting_date': '2021-04-01',\n",
       "  'pred': tensor(1.1260),\n",
       "  'past': tensor(1.1377),\n",
       "  'tgt': tensor(1.1600)},\n",
       " '2021-04-09': {'predicting_date': '2021-04-02',\n",
       "  'pred': tensor(1.1282),\n",
       "  'past': tensor(1.1363),\n",
       "  'tgt': tensor(1.1670)},\n",
       " '2021-04-12': {'predicting_date': '2021-04-05',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1613),\n",
       "  'tgt': tensor(1.1350)},\n",
       " '2021-04-13': {'predicting_date': '2021-04-06',\n",
       "  'pred': tensor(1.1763),\n",
       "  'past': tensor(1.1780),\n",
       "  'tgt': tensor(1.1320)},\n",
       " '2021-04-14': {'predicting_date': '2021-04-07',\n",
       "  'pred': tensor(1.1718),\n",
       "  'past': tensor(1.1880),\n",
       "  'tgt': tensor(1.1000)},\n",
       " '2021-04-15': {'predicting_date': '2021-04-08',\n",
       "  'pred': tensor(1.1563),\n",
       "  'past': tensor(1.1740),\n",
       "  'tgt': tensor(1.1400)}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rets_2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-lambda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python My Env",
   "language": "python",
   "name": "python-my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
