{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply attention.\n",
    "MOdified from dual attention rnn from https://chandlerzuo.github.io/blog/2017/11/darnn\n",
    "\n",
    "These are seq2seq model which predicts final status sequentially.  \n",
    "This is not good because we get errors from each prediction step.  \n",
    "\n",
    "Another thing to note is  \n",
    "In case prediction_period is larger than 1,\n",
    "we do not need to infer all information.  \n",
    "We have information before predicting dates.  \n",
    "So y market data before predicting date can be used in inference.  \n",
    "\n",
    "Results are bad for distant prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy as cp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "try: # ip: 153\n",
    "    lib_s = '/home/adminuser/public/libs/pytorch_examples/basic_networks'\n",
    "    sys.path.append(lib_s)\n",
    "    import generate_model as nets\n",
    "except: # ip : 103\n",
    "    lib_s = '/home/bwlee/work/codes/examples/basic_networks'\n",
    "    sys.path.append(lib_s)\n",
    "    import generate_model as nets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make input data\n",
    "interest.csv has only closed price.   \n",
    "On the other hand, zipline needs OLHCV(?) format for each index  \n",
    "make dummy columns and put 'close' value to them  \n",
    "make csv file for each index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_s0 = 'interest.csv'\n",
    "df0 = pd.read_csv(f_s0, index_col='date')\n",
    "df0 = df0.iloc[::-1] # 과거에서 현재로 정렬순서 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized input, norm val, tgt_date, tgt_val, tgt_diff_ratio, \n",
    "class Environ():\n",
    "    def __init__(self):\n",
    "        self.indices = ['trea3', 'trea5', 'trea10']\n",
    "        #self.tgt_index = 'trea10'\n",
    "        self.tgt_index = 'trea3'\n",
    "        # input window size\n",
    "        self.input_size = 15\n",
    "        # prediction is made after algo.pred_period from the present\n",
    "        self.pred_period = 5\n",
    "\n",
    "env = Environ() # setting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff0 = df0\n",
    "dff = dff0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_periods = [int(env.input_size/4*x) for x in [1, 2, 3]]\n",
    "for diff_period in diff_periods:\n",
    "    diff = dff0.diff(periods=diff_period)\n",
    "    ii = diff_period\n",
    "    columns = ['d%d_diff_trea3'%(ii), 'd%d_diff_trea5'%(ii), 'd%d_diff_trea10'%(ii)]\n",
    "    diff.columns = columns\n",
    "    dff = dff.merge(diff, how='inner', on='date')\n",
    "dff['diff35'] = dff['trea3'] - dff['trea5']\n",
    "dff['diff310'] = dff['trea3'] - dff['trea10']\n",
    "dff['diff510'] = dff['trea5'] - dff['trea10']\n",
    "\n",
    "dff = dff.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data without time confusion\n",
    "Seperate two actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_data(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read past data and push them to mkt_data\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        if ii_days+1 < env.input_size:\n",
    "            continue\n",
    "        elif ii_days+1 == env.input_size:\n",
    "            env.start_date = data.iloc[ii_days].name\n",
    "        \n",
    "        predicting_date = data.iloc[ii_days].name\n",
    "        # historical data from today to env.input_size behind days\n",
    "        history = data.iloc[ii_days+1-env.input_size:ii_days+1]\n",
    "        \n",
    "        ## normalize\n",
    "        \"\"\"\n",
    "        #  : look back 1 year\n",
    "        lookback_step = 250\n",
    "        lookback_step = env.input_size\n",
    "        if ii_days+1 < lookback_step:\n",
    "            cumul = data.iloc[:ii_days+1]\n",
    "        else:\n",
    "            cumul = data.iloc[ii_days+1-lookback_step:ii_days+1]\n",
    "        \n",
    "        mean_val = cumul.mean() \n",
    "        range_val = 2*cumul.std()\n",
    "        \"\"\"\n",
    "        \n",
    "        mean_val = history.mean() \n",
    "        range_val = history.max() - history.min()\n",
    "\n",
    "        norm_history = (history-mean_val)/range_val\n",
    "        mkt_data[predicting_date] = [norm_history, mean_val, range_val, \n",
    "                                     None, None, None, None, None]\n",
    "        \n",
    "            \n",
    "def push_tgt(env, data, mkt_data):\n",
    "    \"\"\"\n",
    "    read future data and push them to mkt_data\n",
    "    future_prediction_date is a future date when model want to predict\n",
    "    past_predicting_date is a past date when model made a prediction with given information\n",
    "    :param data: dataframe info from csv file\n",
    "    :param mkt_data: dataframe to be used for learning\n",
    "    \"\"\"\n",
    "    for ii_days in range(len(data)):\n",
    "        ii_predicting = ii_days - env.pred_period\n",
    "        predicting_date = data.iloc[ii_predicting].name\n",
    "        if ii_predicting < 0 or predicting_date < env.start_date:\n",
    "            continue\n",
    "        \n",
    "        past_history = mkt_data[predicting_date][0]\n",
    "        mean_val = mkt_data[predicting_date][1]\n",
    "        range_val = mkt_data[predicting_date][2]\n",
    "        \n",
    "        past_price = (past_history.iloc[-1]*range_val+mean_val)[env.tgt_index] # 49.6\n",
    "        # wrong\n",
    "        #past_priceb = (past_history.iloc[-env.pred_period:]*range_val+mean_val)[env.tgt_index] \n",
    "        #print('past_priceb', past_priceb)\n",
    "        #past_price = (past_history.iloc[-5]*range_val+mean_val)[env.tgt_index] # 56.3\n",
    "        \n",
    "        #mkt_data[predicting_date][0] = past_history[:-4]\n",
    "        \n",
    "        #past_price = (past_history.iloc[-3]*range_val+mean_val)[env.tgt_index] # 53.0, # 53.0\n",
    "        #past_price = (past_history.iloc[-13]*range_val+mean_val)[env.tgt_index] # 67.4\n",
    "        #past_price = (past_history*range_val+mean_val).mean()[env.tgt_index] # 58.4\n",
    "        #past_price = (past_history*range_val+mean_val)[:-3].mean()[env.tgt_index] # 63.2\n",
    "        #past_price = (past_history*range_val+mean_val)[:-5].mean()[env.tgt_index] # 63.7\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10]*range_val+mean_val)[env.tgt_index] # 66.4\n",
    "\n",
    "        #past_price = (past_history.iloc[-5:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        #past_price = (past_history.iloc[-3:].mean()*range_val+mean_val)[env.tgt_index] # 48.1\n",
    "        \n",
    "        #past_price = (past_history.iloc[-10:].mean()*range_val+mean_val)[env.tgt_index] # 55.9\n",
    "        #past_price = (past_history.iloc[-10:-5].mean()*range_val+mean_val)[env.tgt_index] # 60.4\n",
    "        \n",
    "        prediction_date = data.iloc[ii_days].name\n",
    "        tgt_price = data.iloc[ii_days][env.tgt_index]\n",
    "        # tgt should have one more time step than input, because is has initial value\n",
    "        \n",
    "        #tgt_price = (data.iloc[ii_days-1:ii_days+2].mean())[env.tgt_index]\n",
    "        \n",
    "        # ignore dates and subtract to get target history\n",
    "        tgt_ratio = (tgt_price - past_price)/past_price*10\n",
    "        \n",
    "        mkt_data[predicting_date][3] = prediction_date\n",
    "        mkt_data[predicting_date][4] = np.single(tgt_price)\n",
    "        mkt_data[predicting_date][5] = np.single(tgt_ratio)\n",
    "        \n",
    "        mkt_data[predicting_date][6] = np.single(past_price)\n",
    "        mkt_data[predicting_date][7] = predicting_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push past data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkt_data = {} # object to store data for learning\n",
    "push_data(env, dff, mkt_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check data are pushed correctly\n",
    "* no dates should be ahead of key date\n",
    "* prediction info are not pushed yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000-11-11', '2000-11-13', '2000-11-14']\n"
     ]
    }
   ],
   "source": [
    "print(list(mkt_data.keys())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[               trea3     trea5    trea10  d3_diff_trea3  d3_diff_trea5  \\\n",
       " date                                                                     \n",
       " 2000-11-07  0.681934  0.687143  0.711795       0.130667       0.124551   \n",
       " 2000-11-08  0.567430  0.558571  0.588718       0.030667       0.016766   \n",
       " 2000-11-09  0.124682  0.080000  0.111795      -0.382667      -0.414371   \n",
       " 2000-11-10 -0.226463 -0.212857 -0.149744      -0.689333      -0.665868   \n",
       " 2000-11-11 -0.226463 -0.212857 -0.149744      -0.589333      -0.558084   \n",
       " 2000-11-13 -0.318066 -0.305714 -0.288205      -0.282667      -0.234731   \n",
       " 2000-11-14 -0.287532 -0.312857 -0.288205       0.050667       0.004790   \n",
       " 2000-11-15 -0.150127 -0.191429 -0.172821       0.170667       0.106587   \n",
       " 2000-11-16 -0.081425 -0.120000 -0.134359       0.310667       0.244311   \n",
       " 2000-11-17 -0.104326 -0.141429 -0.157436       0.264000       0.232335   \n",
       " 2000-11-18 -0.104326 -0.141429 -0.157436       0.144000       0.130539   \n",
       " 2000-11-20 -0.073791 -0.070000 -0.095897       0.110667       0.130539   \n",
       " 2000-11-21  0.117048  0.151429  0.104103       0.297333       0.334132   \n",
       " 2000-11-22  0.017812  0.072857  0.011795       0.210667       0.268263   \n",
       " 2000-11-23  0.063613  0.158571  0.065641       0.224000       0.280240   \n",
       " \n",
       "             d3_diff_trea10  d7_diff_trea3  d7_diff_trea5  d7_diff_trea10  \\\n",
       " date                                                                       \n",
       " 2000-11-07        0.145662       0.288768       0.263054        0.262983   \n",
       " 2000-11-08        0.036073       0.163768       0.139901        0.152486   \n",
       " 2000-11-09       -0.422831      -0.118841      -0.155665       -0.156906   \n",
       " 2000-11-10       -0.655708      -0.385145      -0.357635       -0.322652   \n",
       " 2000-11-11       -0.546119      -0.406884      -0.382266       -0.344751   \n",
       " 2000-11-13       -0.244749      -0.472101      -0.446305       -0.444199   \n",
       " 2000-11-14       -0.011872      -0.472101      -0.475862       -0.471823   \n",
       " 2000-11-15        0.090868      -0.374275      -0.397044       -0.388950   \n",
       " 2000-11-16        0.248402      -0.243841      -0.259113       -0.272928   \n",
       " 2000-11-17        0.227854       0.055072       0.056158        0.053039   \n",
       " 2000-11-18        0.125114       0.305072       0.258128        0.240884   \n",
       " 2000-11-20        0.145662       0.326812       0.307389        0.285083   \n",
       " 2000-11-21        0.344292       0.527899       0.524138        0.528177   \n",
       " 2000-11-22        0.262100       0.435507       0.474877        0.461878   \n",
       " 2000-11-23        0.255251       0.370290       0.450246        0.417680   \n",
       " \n",
       "             d11_diff_trea3  d11_diff_trea5  d11_diff_trea10    diff35  \\\n",
       " date                                                                    \n",
       " 2000-11-07        0.314228        0.336185         0.300629 -0.326984   \n",
       " 2000-11-08        0.265447        0.272014         0.306918 -0.184127   \n",
       " 2000-11-09        0.070325        0.015330         0.030189  0.244444   \n",
       " 2000-11-10       -0.210163       -0.203922        -0.183648  0.006349   \n",
       " 2000-11-11       -0.197967       -0.198574        -0.183648  0.006349   \n",
       " 2000-11-13       -0.319919       -0.305526        -0.322013  0.053968   \n",
       " 2000-11-14       -0.258943       -0.273440        -0.284277  0.292063   \n",
       " 2000-11-15       -0.167480       -0.182531        -0.164780  0.339683   \n",
       " 2000-11-16       -0.136992       -0.155793        -0.158491  0.292063   \n",
       " 2000-11-17       -0.155285       -0.171836        -0.177358  0.292063   \n",
       " 2000-11-18       -0.179675       -0.198574        -0.208805  0.292063   \n",
       " 2000-11-20       -0.155285       -0.150446        -0.158491  0.006349   \n",
       " 2000-11-21        0.088618        0.111586         0.105660 -0.279365   \n",
       " 2000-11-22        0.363008        0.411052         0.420126 -0.374603   \n",
       " 2000-11-23        0.680081        0.694474         0.677987 -0.660317   \n",
       " \n",
       "              diff310   diff510  \n",
       " date                            \n",
       " 2000-11-07 -0.188235  0.152778  \n",
       " 2000-11-08 -0.129412  0.069444  \n",
       " 2000-11-09  0.105882 -0.138889  \n",
       " 2000-11-10 -0.600000 -0.430556  \n",
       " 2000-11-11 -0.600000 -0.430556  \n",
       " 2000-11-13 -0.247059 -0.222222  \n",
       " 2000-11-14 -0.011765 -0.263889  \n",
       " 2000-11-15  0.164706 -0.180556  \n",
       " 2000-11-16  0.400000  0.027778  \n",
       " 2000-11-17  0.400000  0.027778  \n",
       " 2000-11-18  0.400000  0.027778  \n",
       " 2000-11-20  0.164706  0.111111  \n",
       " 2000-11-21  0.105882  0.319444  \n",
       " 2000-11-22  0.047059  0.361111  \n",
       " 2000-11-23 -0.011765  0.569444  ,\n",
       " trea3              7.343333\n",
       " trea5              7.524000\n",
       " trea10             7.792333\n",
       " d3_diff_trea3     -0.078000\n",
       " d3_diff_trea5     -0.074000\n",
       " d3_diff_trea10    -0.081333\n",
       " d7_diff_trea3     -0.200667\n",
       " d7_diff_trea5     -0.212000\n",
       " d7_diff_trea10    -0.223000\n",
       " d11_diff_trea3    -0.367667\n",
       " d11_diff_trea5    -0.389333\n",
       " d11_diff_trea10   -0.399000\n",
       " diff35            -0.180667\n",
       " diff310           -0.449000\n",
       " diff510           -0.268333\n",
       " dtype: float64,\n",
       " trea3              0.655\n",
       " trea5              0.700\n",
       " trea10             0.650\n",
       " d3_diff_trea3      0.750\n",
       " d3_diff_trea5      0.835\n",
       " d3_diff_trea10     0.730\n",
       " d7_diff_trea3      0.920\n",
       " d7_diff_trea5      1.015\n",
       " d7_diff_trea10     0.905\n",
       " d11_diff_trea3     0.820\n",
       " d11_diff_trea5     0.935\n",
       " d11_diff_trea10    0.795\n",
       " diff35             0.105\n",
       " diff310            0.085\n",
       " diff510            0.120\n",
       " dtype: float64,\n",
       " '2000-11-29',\n",
       " 7.24,\n",
       " -0.19634394,\n",
       " 7.385,\n",
       " '2000-11-23']"
      ]
     },
     "execution_count": 676,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkt_data['2000-11-23']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2000-11-23'"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.start_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Push future data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "push_tgt(env, dff, mkt_data)\n",
    "mkt_data = {key: val for key, val in mkt_data.items() if (val[-1] is not None)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CHECK tgt push is correctly made\n",
    "end value of '2000-11-23' * norm == prediction value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-10-26  0.300000  0.272099  0.293963\n",
      "2000-10-27  0.105970  0.131358  0.152231\n",
      "2000-10-28  0.105970  0.131358  0.152231\n",
      "2000-10-30  0.091045  0.123951  0.152231\n",
      "2000-10-31  0.150746  0.175802  0.183727\n",
      "2000-11-01  0.105970  0.123951  0.136483\n",
      "2000-11-02  0.128358  0.123951  0.104987\n",
      "2000-11-03  0.158209  0.160988  0.136483\n",
      "2000-11-04  0.158209  0.160988  0.136483\n",
      "2000-11-06  0.188060  0.198025  0.175853\n",
      "2000-11-07  0.188060  0.205432  0.175853\n",
      "2000-11-08  0.076119  0.072099  0.049869\n",
      "2000-11-09 -0.356716 -0.424198 -0.438320\n",
      "2000-11-10 -0.700000 -0.727901 -0.706037\n",
      "2000-11-11 -0.700000 -0.727901 -0.706037, trea3     7.664000\n",
      "trea5     7.866333\n",
      "trea10    8.143333\n",
      "dtype: float64, trea3     0.670\n",
      "trea5     0.675\n",
      "trea10    0.635\n",
      "dtype: float64, '2000-12-05', 7.07, -1.0108074]\n",
      "[               trea3     trea5    trea10\n",
      "date                                    \n",
      "2000-11-07  0.681934  0.687143  0.711795\n",
      "2000-11-08  0.567430  0.558571  0.588718\n",
      "2000-11-09  0.124682  0.080000  0.111795\n",
      "2000-11-10 -0.226463 -0.212857 -0.149744\n",
      "2000-11-11 -0.226463 -0.212857 -0.149744\n",
      "2000-11-13 -0.318066 -0.305714 -0.288205\n",
      "2000-11-14 -0.287532 -0.312857 -0.288205\n",
      "2000-11-15 -0.150127 -0.191429 -0.172821\n",
      "2000-11-16 -0.081425 -0.120000 -0.134359\n",
      "2000-11-17 -0.104326 -0.141429 -0.157436\n",
      "2000-11-18 -0.104326 -0.141429 -0.157436\n",
      "2000-11-20 -0.073791 -0.070000 -0.095897\n",
      "2000-11-21  0.117048  0.151429  0.104103\n",
      "2000-11-22  0.017812  0.072857  0.011795\n",
      "2000-11-23  0.063613  0.158571  0.065641, trea3     7.343333\n",
      "trea5     7.524000\n",
      "trea10    7.792333\n",
      "dtype: float64, trea3     0.655\n",
      "trea5     0.700\n",
      "trea10    0.650\n",
      "dtype: float64, '2000-12-16', 7.11, -0.9541985]\n"
     ]
    }
   ],
   "source": [
    "print(mkt_data['2000-11-11'])\n",
    "print(mkt_data['2000-11-23'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Index_data(Dataset):\n",
    "    def __init__(self, in_data0, dates, addi=None):\n",
    "        \"\"\"\n",
    "        :param in_data: dict of (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        :param addi: (optional) additional non-sequential elements that can be added to in_data\n",
    "        \"\"\"\n",
    "        in_data = cp.deepcopy(in_data0)\n",
    "        if addi is not None:\n",
    "            assert list(in_data.keys()) == list(addi.keys())\n",
    "            \n",
    "        # change dataframe to float numpy array\n",
    "        for key, val in in_data.items():\n",
    "            # transform in_data into Tensor\n",
    "            if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "                in_data[key][0] = torch.tensor(in_data[key][0].to_numpy(np.single))\n",
    "                in_data[key][1] = torch.tensor(in_data[key][1].to_numpy(np.single))\n",
    "                in_data[key][2] = torch.tensor(in_data[key][2].to_numpy(np.single))\n",
    "                #in_data[key][4] = torch.tensor(in_data[key][4])\n",
    "                #in_data[key][5] = torch.tensor(in_data[key][5]) # isn't this necessary? not in rnn and cnn model\n",
    "            \n",
    "            if addi is not None:\n",
    "                in_data[key].append(addi[key])\n",
    "            else:\n",
    "                # None is not accept in Dataloader, so give null value\n",
    "                in_data[key].append('NULL')\n",
    "                \n",
    "        self.in_data = in_data\n",
    "        self.i2dates = {}\n",
    "        for ii, date1 in enumerate(dates):\n",
    "             self.i2dates[ii] = date1\n",
    "        \n",
    "    def __getitem__(self, ii_date):\n",
    "        \"\"\"\n",
    "        :return: (df_past_normed, norm_val, prediction_date, tgt_price, tgt_ratio)\n",
    "        \"\"\"\n",
    "        predicting_date = self.i2dates[ii_date]\n",
    "        return self.in_data[predicting_date]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.in_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "#erase data that do not have prediction target\n",
    "traintest_dates = sorted(list(mkt_data.keys()))\n",
    "n_train = len(traintest_dates)*8//10\n",
    "n_test = len(traintest_dates) - n_train\n",
    "train_dates = traintest_dates[:n_train]\n",
    "test_dates = traintest_dates[n_train:]\n",
    "\n",
    "train_data = {}\n",
    "test_data = {}\n",
    "for key, val in mkt_data.items():\n",
    "    if key in train_dates:\n",
    "        train_data[key] = val\n",
    "    elif key in test_dates:\n",
    "        test_data[key] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## this is an example, I already add differences\n",
    "def get_addi(in_data):\n",
    "    ret = {}\n",
    "    for key, val in in_data.items():\n",
    "        if not 'Tensor' in str(type(in_data[key][0])) :\n",
    "            history_val = torch.tensor(val[0][['trea3','trea5', 'trea10']].to_numpy(np.single))\n",
    "        else:\n",
    "            history_val = val[0][:,:3] # only use three elements : ['trea3','trea5', 'trea10']\n",
    "        addi1 = history_val[-1] - history_val[-5]\n",
    "        addi2 = history_val[-1] - history_val[-7]\n",
    "        addi3 = history_val[-1] - history_val[-10]\n",
    "        addi4 = history_val[-1] - history_val[-3]\n",
    "        addi5 = history_val[-3] - history_val[-7]\n",
    "        addi6 = history_val[-3] - history_val[-10]\n",
    "        addi7 = history_val[-5] - history_val[-7]\n",
    "        addi8 = history_val[-5] - history_val[-10]\n",
    "        addi = torch.cat((addi1, addi2, addi3, addi4, addi5, addi6, addi7, addi8), dim=-1)    \n",
    "        ret[key] = addi\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\"\"\" # no additional\n",
    "train_dset = Index_data(train_data, train_dates)\n",
    "test_dset = Index_data(test_data, test_dates)\n",
    "\"\"\"\n",
    "train_dset = Index_data(train_data, train_dates, get_addi(train_data))\n",
    "test_dset = Index_data(test_data, test_dates, get_addi(test_data))\n",
    "train_loader = DataLoader(train_dset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1068\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.3948, -0.4402, -0.4924],\n",
       "         [-0.4127, -0.4342, -0.4758],\n",
       "         [-0.4307, -0.4704, -0.5015],\n",
       "         [-0.5139, -0.5590, -0.6024],\n",
       "         [-0.3228, -0.2934, -0.2630],\n",
       "         [-0.2105, -0.1324, -0.0520],\n",
       "         [ 0.1041,  0.1996,  0.2508],\n",
       "         [ 0.0592,  0.1091,  0.1407],\n",
       "         [ 0.2951,  0.3022,  0.3058],\n",
       "         [ 0.1491,  0.1392,  0.1315],\n",
       "         [ 0.2884,  0.2720,  0.2893],\n",
       "         [ 0.2727,  0.2700,  0.2783],\n",
       "         [ 0.3064,  0.3062,  0.3150],\n",
       "         [ 0.3243,  0.2901,  0.2783],\n",
       "         [ 0.4861,  0.4410,  0.3976]]),\n",
       " tensor([1.6137, 1.7458, 1.9783]),\n",
       " tensor([0.4450, 0.4970, 0.5450]),\n",
       " '2016-12-15',\n",
       " tensor([1.8300, 1.8230, 1.7480, 1.7100, 1.7220, 1.7250, 1.7600, 1.7200, 1.7370,\n",
       "         1.7310, 1.7090, 1.7350, 1.7160, 1.6620, 1.6450, 1.6950]),\n",
       " tensor([ 0.0000, -0.0383, -0.4481, -0.6557, -0.5902, -0.5738, -0.3825, -0.6011,\n",
       "         -0.5082, -0.5410, -0.6612, -0.5191, -0.6230, -0.9180, -1.0109, -0.7377]),\n",
       " 1.83,\n",
       " '2016-11-24',\n",
       " tensor([ 0.1978,  0.1690,  0.1083,  0.1910,  0.1388,  0.0917,  0.6966,  0.5734,\n",
       "          0.4495,  0.1798,  0.1348,  0.0826,  0.0112,  0.0040,  0.0092,  0.5169,\n",
       "          0.4386,  0.3670, -0.0067, -0.0302, -0.0165,  0.4989,  0.4044,  0.3413])]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, T):\n",
    "        # input size: number of underlying factors (81)\n",
    "        # T: number of time steps (10)\n",
    "        # hidden_size: dimension of the hidden state\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.T = T\n",
    "\n",
    "        self.lstm_layer = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 1)\n",
    "        self.attn_linear = nn.Linear(in_features = 2 * hidden_size + T - 1, out_features = 1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        # input_data: batch_size * T - 1 * input_size        \n",
    "        input_weighted = Variable(input_data.data.new(input_data.size(0), self.T - 1, self.input_size).zero_())\n",
    "        input_encoded = Variable(input_data.data.new(input_data.size(0), self.T - 1, self.hidden_size).zero_())\n",
    "        # hidden, cell: initial states with dimention hidden_size\n",
    "        hidden = self.init_hidden(input_data) # 1 * batch_size * hidden_size\n",
    "        cell = self.init_hidden(input_data)\n",
    "        # hidden.requires_grad = False\n",
    "        # cell.requires_grad = False\n",
    "        for t in range(self.T - 1):\n",
    "            # Eqn. 8: concatenate the hidden states with each predictor\n",
    "            x = torch.cat((hidden.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
    "                           cell.repeat(self.input_size, 1, 1).permute(1, 0, 2),\n",
    "                           input_data.permute(0, 2, 1)), dim = 2) # batch_size * input_size * (2*hidden_size + T - 1)\n",
    "            # Eqn. 9: Get attention weights\n",
    "            x = self.attn_linear(x.view(-1, self.hidden_size * 2 + self.T - 1)) # (batch_size * input_size) * 1\n",
    "            attn_weights = F.softmax(x.view(-1, self.input_size)) # batch_size * input_size, attn weights with values sum up to 1.\n",
    "            # Eqn. 10: LSTM\n",
    "            weighted_input = torch.mul(attn_weights, input_data[:, t, :]) # batch_size * input_size\n",
    "            # Fix the warning about non-contiguous memory\n",
    "            # see https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282\n",
    "            self.lstm_layer.flatten_parameters()\n",
    "            output, lstm_states = self.lstm_layer(weighted_input.unsqueeze(0), (hidden, cell))\n",
    "            hidden = lstm_states[0]\n",
    "            cell = lstm_states[1]\n",
    "            # Save output\n",
    "            input_weighted[:, t, :] = weighted_input\n",
    "            input_encoded[:, t, :] = hidden\n",
    "        return input_weighted, input_encoded, output\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        # No matter whether CUDA is used, the returned variable will have the same type as x.\n",
    "        return Variable(x.data.new(1, x.size(0), self.hidden_size).zero_()) # dimension 0 is the batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Predictor_rnn(nets.Net):\n",
    "    def __init__(self, downnet=None, dim_hiddens=[200, 200], \n",
    "                 loss=None, optimizer=None, device='cuda:2',\n",
    "                 dim_input=100, bidirectional=True, time_step=15, lr=0.001):\n",
    "        \"\"\"\n",
    "        net is consists of [embed, rnn, downnet]\n",
    "        :param downnet: define downstream job\n",
    "        \"\"\"\n",
    "        super(Predictor_rnn, self).__init__(loss=loss, device=device)\n",
    "        \n",
    "        encoder_dim_hidden, rnn_dim_hidden = dim_hiddens\n",
    "        self.encoder = Encoder(dim_input, encoder_dim_hidden, time_step)\n",
    "        self.encoder2 = Encoder(encoder_dim_hidden, encoder_dim_hidden, \n",
    "                                time_step)\n",
    "        # this is optional, add another layer\n",
    "        self.rnn = nn.LSTM(input_size=dim_input, hidden_size=rnn_dim_hidden,\n",
    "                          num_layers=2, batch_first=True, bidirectional=bidirectional)\n",
    "            \n",
    "        if downnet is None:\n",
    "            self.downnet = nets.get_MLP([rnn_dim_hidden, rnn_dim_hidden*2, 1], \n",
    "                                        dropout=0.35, end=True)\n",
    "        else:\n",
    "            self.downnet = downnet\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "        self.nets = [self.encoder, self.encoder2, self.rnn, self.downnet]\n",
    "        parms = None\n",
    "        for net1 in self.nets:\n",
    "            net1 = net1.to(device)\n",
    "            if parms is None:\n",
    "                parms = list(net1.parameters())\n",
    "            else:\n",
    "                parms += list(net1.parameters())\n",
    "        self.optimizer = optimizer(parms, lr=lr)\n",
    "            \n",
    "    def set_train(self):\n",
    "        for net1 in self.nets:\n",
    "            net1.train()\n",
    "        \n",
    "    def set_eval(self):\n",
    "        for net1 in self.nets:\n",
    "            net1.eval()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        # check is there a way to initialize lstm parameters?\n",
    "        for ii, layer in enumerate(self.downnet):\n",
    "            if 'Linear' in str(layer):\n",
    "                #torch.nn.init.xavier_uniform_(self.downnet[ii].weight)\n",
    "                torch.nn.init.xavier_normal_(self.downnet[ii].weight)\n",
    "\n",
    "    def forward(self, x, addi=('NULL')):\n",
    "        input_weighted, input_encoded, output = self.encoder(x)\n",
    "        # output, # [dim_seq=1, dim_batch, dim_hidden] \n",
    "        output = output.squeeze(0)\n",
    "        \"\"\" test\n",
    "        #out, hidden = self.rnn(x)\n",
    "        out, hidden = self.rnn(input_weighted)\n",
    "        out = out[:,-1] # choose last output\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        out = output\n",
    "        \"\"\"\n",
    "        #\"\"\"\n",
    "        input_weighted, input_encoded, output = self.encoder2(input_encoded)\n",
    "        output = output.squeeze(0)\n",
    "        out = output\n",
    "        #\"\"\"\n",
    "        \n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            out = torch.cat((out, addi),dim=1)\n",
    "        out = self.downnet(out)\n",
    "        return out.view(-1)\n",
    "\n",
    "    def run_eval(self, data):\n",
    "        self.set_eval()\n",
    "        loss = 0\n",
    "        outs = None\n",
    "        tgts = None\n",
    "        with torch.no_grad():\n",
    "            for i_batch, data_batch in enumerate(data):\n",
    "                data_in = data_batch[0][:,1:]\n",
    "                mean_val = data_batch[1]\n",
    "                range_val = data_batch[2]\n",
    "                pred_date = data_batch[3]\n",
    "                tgts_in = data_batch[5]\n",
    "                tgt = tgts_in\n",
    "                addi = data_batch[-1]\n",
    "                if 'Tensor' in str(type(addi)):\n",
    "                    addi = addi.to(self.device)\n",
    "                \n",
    "                data_in = data_in.to(self.device)\n",
    "                tgt = tgt.to(self.device)\n",
    "                \n",
    "                out = self.forward(data_in, addi)\n",
    "                    \n",
    "                #print('loss', loss, self.loss)\n",
    "                loss += self.loss(out, tgt).cpu().numpy()\n",
    "                out = out.cpu().numpy()\n",
    "                #print('loss===', loss)\n",
    "                tgt = tgt.cpu().numpy()\n",
    "                #print('tgt', tgt.shape)\n",
    "                if outs is None:\n",
    "                    outs = out\n",
    "                    tgts = tgt\n",
    "                else:\n",
    "                    outs = np.concatenate((outs, out), axis=0)\n",
    "                    tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "        loss /= 1.0*(i_batch+1)\n",
    "        print('evaluate', 'loss', loss, 'accuracy : define function')\n",
    "        return outs, tgts, loss\n",
    "    \n",
    "    def run_batch(self, i_batch, data_batch):\n",
    "        self.optimizer.zero_grad()\n",
    "        data_in = data_batch[0][:,1:]\n",
    "        mean_val = data_batch[1]\n",
    "        range_val = data_batch[2]\n",
    "        pred_date = data_batch[3]\n",
    "        tgts_in = data_batch[5]\n",
    "        tgt = tgts_in\n",
    "        addi = data_batch[-1]\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            addi = addi.to(self.device)\n",
    "            \n",
    "        data_in = data_in.to(self.device)\n",
    "        tgt = tgt.to(self.device)\n",
    "        \n",
    "        out = self.forward(data_in, addi)\n",
    "        \n",
    "        loss = self.loss(out, tgt)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_input = train_dset[0][0].shape[-1]\n",
    "time_step = train_dset[0][0].shape[0]\n",
    "dim_hidden = 100\n",
    "\"\"\"\n",
    "# no addition\n",
    "dim_hiddenb = dim_hidden\n",
    "downnet = nets.get_MLP([dim_hiddenb, 2*dim_hidden, 1], dropout=0.0, end=True)\n",
    "\"\"\"\n",
    "#downnet = nets.get_MLP([dim_hidden+3*3, dim_hidden, 1], \n",
    "# Add difference variables\n",
    "#\"\"\"\n",
    "dim_hiddenb = dim_hidden+3*8\n",
    "downnet = nets.get_MLP([dim_hiddenb, dim_hidden, 1], \n",
    "                        dropout=0.0, end=True)\n",
    "#\"\"\"\n",
    "#bidownnet = nets.get_MLP([dim_hidden*2+3*8, dim_hidden*2, 1], \n",
    "#                        dropout=0.0, end=True)\n",
    "loss = nn.MSELoss() # which combines logsoftmax and nll loss\n",
    "optimizer = optim.Adam\n",
    "device = torch.device(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor_rnn(loss=loss, optimizer=optimizer, \n",
    "                          device=device, dim_input=dim_input, \n",
    "                          dim_hiddens=[dim_hidden, dim_hidden], \n",
    "                          #downnet=None, bidirectional=False, \n",
    "                          downnet=downnet, bidirectional=False, lr=0.001,\n",
    "                          time_step=env.input_size)\n",
    "                          #dim_hidden=dim_hidden, downnet=bidownnet, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 loss 0.09726834519585567\n",
      "evaluate loss 0.1382761851829641 accuracy : define function\n",
      "epoch 1 loss 0.09272779596608076\n",
      "evaluate loss 0.13492741194718025 accuracy : define function\n",
      "epoch 2 loss 0.0924056424245016\n",
      "evaluate loss 0.13625193584491224 accuracy : define function\n",
      "epoch 3 loss 0.09129118974973906\n",
      "evaluate loss 0.13498407470829346 accuracy : define function\n",
      "epoch 4 loss 0.09170094082382188\n",
      "evaluate loss 0.13491127453744411 accuracy : define function\n",
      "epoch 5 loss 0.09073380080621633\n",
      "evaluate loss 0.1373136026017806 accuracy : define function\n",
      "epoch 6 loss 0.09055503274300206\n",
      "evaluate loss 0.13598577875424833 accuracy : define function\n",
      "epoch 7 loss 0.09004736196861338\n",
      "evaluate loss 0.13570796742158778 accuracy : define function\n",
      "epoch 8 loss 0.08934091854451308\n",
      "evaluate loss 0.13613879702547016 accuracy : define function\n",
      "epoch 9 loss 0.08990806869384069\n",
      "evaluate loss 0.1346574513351216 accuracy : define function\n",
      "epoch 10 loss 0.09009381097310515\n",
      "evaluate loss 0.13438879829995773 accuracy : define function\n",
      "epoch 11 loss 0.08904045503308523\n",
      "evaluate loss 0.1356101140160771 accuracy : define function\n",
      "epoch 12 loss 0.08879177683769766\n",
      "evaluate loss 0.1349680332795662 accuracy : define function\n",
      "epoch 13 loss 0.08935235829940483\n",
      "evaluate loss 0.13639252895818038 accuracy : define function\n",
      "epoch 14 loss 0.08877952296787234\n",
      "evaluate loss 0.13567387268823736 accuracy : define function\n",
      "epoch 15 loss 0.08755843302429613\n",
      "evaluate loss 0.13588826599366524 accuracy : define function\n",
      "epoch 16 loss 0.08781942497216054\n",
      "evaluate loss 0.13478177643435843 accuracy : define function\n",
      "epoch 17 loss 0.08697068218642207\n",
      "evaluate loss 0.13550828879370408 accuracy : define function\n",
      "epoch 18 loss 0.08670476779564103\n",
      "evaluate loss 0.13590524718165398 accuracy : define function\n",
      "epoch 19 loss 0.0863201597066068\n",
      "evaluate loss 0.1343125584809219 accuracy : define function\n",
      "epoch 20 loss 0.08677311958884125\n",
      "evaluate loss 0.13618540917249286 accuracy : define function\n",
      "epoch 21 loss 0.08662625580136456\n",
      "evaluate loss 0.1361971736392554 accuracy : define function\n",
      "epoch 22 loss 0.08597739207655636\n",
      "evaluate loss 0.1355826149092001 accuracy : define function\n",
      "epoch 23 loss 0.08566395310101224\n",
      "evaluate loss 0.13847233628963723 accuracy : define function\n",
      "epoch 24 loss 0.08574764063554023\n",
      "evaluate loss 0.13722757688340018 accuracy : define function\n",
      "epoch 25 loss 0.08532482082608031\n",
      "evaluate loss 0.1390002908952096 accuracy : define function\n",
      "epoch 26 loss 0.08565658069591024\n",
      "evaluate loss 0.13632668445215507 accuracy : define function\n",
      "epoch 27 loss 0.08445645268283673\n",
      "evaluate loss 0.13663400468580864 accuracy : define function\n",
      "epoch 28 loss 0.08508329671710285\n",
      "evaluate loss 0.13615919605774038 accuracy : define function\n",
      "epoch 29 loss 0.08420073635764976\n",
      "evaluate loss 0.1415891112650142 accuracy : define function\n"
     ]
    }
   ],
   "source": [
    "predictor.run_train(n_epoch=30, data=train_loader, test_data=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "rets = {}\n",
    "tgts = None\n",
    "preds = None\n",
    "predictor.set_eval()\n",
    "with torch.no_grad():\n",
    "    for data_batch in test_loader:\n",
    "        data_in = data_batch[0][:,1:]\n",
    "        tgts_in = data_batch[5]\n",
    "        tgt = tgts_in\n",
    "        \n",
    "        data_in = data_in.to(device)\n",
    "        addi = data_batch[-1]\n",
    "        if 'Tensor' in str(type(addi)):\n",
    "            addi = addi.to(device)\n",
    "\n",
    "        tgt = tgt.numpy()\n",
    "        out = predictor.forward(data_in, addi).cpu().numpy()\n",
    "        if preds is None:\n",
    "            preds = out\n",
    "            tgts = tgt\n",
    "        else:\n",
    "            preds = np.concatenate((preds, out), axis=0)\n",
    "            tgts = np.concatenate((tgts, tgt), axis=0)\n",
    "            \n",
    "        prediction_dates = data_batch[3]\n",
    "        past_prices = data_batch[6]\n",
    "        predicting_dates = data_batch[7]\n",
    "        for ii, date1 in enumerate(prediction_dates):\n",
    "            out1 = out[ii]\n",
    "            #out_orig = out1/10*past_prices[ii] + past_prices[ii]\n",
    "            out_orig = out1*past_prices[ii]\n",
    "            rets[date1] = {'predicting_date': predicting_dates[ii],\n",
    "                          'pred': out_orig,\n",
    "                           'past': past_prices[ii],\n",
    "                           'tgt': data_batch[4][ii]}\n",
    "            \n",
    "preds = preds.reshape(-1)\n",
    "tgts = tgts.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 44\n",
      "7 24\n",
      "71 43 0.6056338028169014\n"
     ]
    }
   ],
   "source": [
    "limit = list(rets.keys()).index('2021-01-04')\n",
    "sum1 = np.sum((preds[limit:] > 0) & (tgts[limit:] > 0))\n",
    "sum2 = np.sum((preds[limit:] < 0) & (tgts[limit:] < 0))\n",
    "print(sum1, np.sum(tgts[limit:]>0))\n",
    "print(sum2, np.sum(tgts[limit:]<0))\n",
    "print(len(preds[limit:]), sum1+sum2, 1.0*(sum1+sum2)/len(preds[limit:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 482\n",
      "154 568\n",
      "1068 504 0.47191011235955055\n"
     ]
    }
   ],
   "source": [
    "sum1 = np.sum((preds > 0) & (tgts > 0))\n",
    "sum2 = np.sum((preds < 0) & (tgts < 0))\n",
    "print(sum1, np.sum(tgts>0))\n",
    "print(sum2, np.sum(tgts<0))\n",
    "print(len(preds), sum1+sum2, 1.0*(sum1+sum2)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021 = {key: val for key, val in rets.items() if key>'2021-01-01'}\n",
    "rows = list(rets_2021['2021-01-04'].keys())\n",
    "cols = list(rets_2021.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "rets_2021b = {}\n",
    "for col in cols:\n",
    "    for row in rows:\n",
    "        rets_2021b.setdefault(row, {})\n",
    "        val = rets_2021[col][row]\n",
    "        if type(val) is not str:\n",
    "            rets_2021b[row][col] = rets_2021[col][row].item()        \n",
    "        else:\n",
    "            rets_2021b[row][col] = rets_2021[col][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
